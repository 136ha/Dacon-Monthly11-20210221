{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 증강을 통해 정확도를 높일 것인데, 전체를 증가시키면 아무 의미 없음으로 26번을 제외하고 증가시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "\n",
    "path = './data/'\n",
    "train = pd.read_csv(path + 'train_features.csv')\n",
    "train_label = pd.read_csv(path + 'train_labels.csv')\n",
    "test = pd.read_csv(path + 'test_features.csv')\n",
    "submission = pd.read_csv(path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 26번만 비정상적으로 많음\n",
    "train_label['label'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26    1518\n",
       "8       97\n",
       "28      55\n",
       "60      48\n",
       "18      47\n",
       "      ... \n",
       "53      13\n",
       "52      12\n",
       "13      12\n",
       "12      12\n",
       "0       12\n",
       "Name: label, Length: 61, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정확한 수치를 확인해 보았다. 26번이 적폐 수준... 데이터의 반을 차지한다.\n",
    "train_label['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26번을 제외한 id 리스트\n",
    "feature = list(train_label[train_label['label'] != 26]['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1875000/1875000 [06:14<00:00, 5005.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# train 데이터에서 26번을 삭제시킨다.\n",
    "temp = []\n",
    "for n in tqdm(range(train.shape[0])):\n",
    "    if train['id'][n] in feature:\n",
    "        temp.append(train.iloc[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>acc_y</th>\n",
       "      <th>acc_z</th>\n",
       "      <th>gy_x</th>\n",
       "      <th>gy_y</th>\n",
       "      <th>gy_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.206087</td>\n",
       "      <td>-0.179371</td>\n",
       "      <td>-0.148447</td>\n",
       "      <td>-0.591608</td>\n",
       "      <td>-30.549010</td>\n",
       "      <td>-31.676112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.287696</td>\n",
       "      <td>-0.198974</td>\n",
       "      <td>-0.182444</td>\n",
       "      <td>0.303100</td>\n",
       "      <td>-39.139103</td>\n",
       "      <td>-24.927216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.304609</td>\n",
       "      <td>-0.195114</td>\n",
       "      <td>-0.253382</td>\n",
       "      <td>-3.617278</td>\n",
       "      <td>-44.122565</td>\n",
       "      <td>-25.019629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.293095</td>\n",
       "      <td>-0.230366</td>\n",
       "      <td>-0.215210</td>\n",
       "      <td>2.712986</td>\n",
       "      <td>-53.597843</td>\n",
       "      <td>-27.454013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.300887</td>\n",
       "      <td>-0.187757</td>\n",
       "      <td>-0.222523</td>\n",
       "      <td>4.286707</td>\n",
       "      <td>-57.906561</td>\n",
       "      <td>-27.961234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964195</th>\n",
       "      <td>3124</td>\n",
       "      <td>595</td>\n",
       "      <td>-0.712530</td>\n",
       "      <td>-0.658357</td>\n",
       "      <td>0.293707</td>\n",
       "      <td>-29.367857</td>\n",
       "      <td>-104.013664</td>\n",
       "      <td>-76.290437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964196</th>\n",
       "      <td>3124</td>\n",
       "      <td>596</td>\n",
       "      <td>-0.683037</td>\n",
       "      <td>-0.658466</td>\n",
       "      <td>0.329223</td>\n",
       "      <td>-30.149089</td>\n",
       "      <td>-101.796809</td>\n",
       "      <td>-76.625087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964197</th>\n",
       "      <td>3124</td>\n",
       "      <td>597</td>\n",
       "      <td>-0.664730</td>\n",
       "      <td>-0.666625</td>\n",
       "      <td>0.364114</td>\n",
       "      <td>-27.873095</td>\n",
       "      <td>-98.776072</td>\n",
       "      <td>-79.365125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964198</th>\n",
       "      <td>3124</td>\n",
       "      <td>598</td>\n",
       "      <td>-0.630534</td>\n",
       "      <td>-0.682565</td>\n",
       "      <td>0.373696</td>\n",
       "      <td>-23.636550</td>\n",
       "      <td>-99.139495</td>\n",
       "      <td>-80.259478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964199</th>\n",
       "      <td>3124</td>\n",
       "      <td>599</td>\n",
       "      <td>-0.578351</td>\n",
       "      <td>-0.700235</td>\n",
       "      <td>0.384390</td>\n",
       "      <td>-17.917626</td>\n",
       "      <td>-100.181873</td>\n",
       "      <td>-80.676229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>964200 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  time     acc_x     acc_y     acc_z       gy_x        gy_y  \\\n",
       "0          0     0  1.206087 -0.179371 -0.148447  -0.591608  -30.549010   \n",
       "1          0     1  1.287696 -0.198974 -0.182444   0.303100  -39.139103   \n",
       "2          0     2  1.304609 -0.195114 -0.253382  -3.617278  -44.122565   \n",
       "3          0     3  1.293095 -0.230366 -0.215210   2.712986  -53.597843   \n",
       "4          0     4  1.300887 -0.187757 -0.222523   4.286707  -57.906561   \n",
       "...      ...   ...       ...       ...       ...        ...         ...   \n",
       "964195  3124   595 -0.712530 -0.658357  0.293707 -29.367857 -104.013664   \n",
       "964196  3124   596 -0.683037 -0.658466  0.329223 -30.149089 -101.796809   \n",
       "964197  3124   597 -0.664730 -0.666625  0.364114 -27.873095  -98.776072   \n",
       "964198  3124   598 -0.630534 -0.682565  0.373696 -23.636550  -99.139495   \n",
       "964199  3124   599 -0.578351 -0.700235  0.384390 -17.917626 -100.181873   \n",
       "\n",
       "             gy_z  \n",
       "0      -31.676112  \n",
       "1      -24.927216  \n",
       "2      -25.019629  \n",
       "3      -27.454013  \n",
       "4      -27.961234  \n",
       "...           ...  \n",
       "964195 -76.290437  \n",
       "964196 -76.625087  \n",
       "964197 -79.365125  \n",
       "964198 -80.259478  \n",
       "964199 -80.676229  \n",
       "\n",
       "[964200 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 26번을 삭제시킨 데이터프레임\n",
    "without = pd.DataFrame(data=np.array(temp), columns=train.columns)\n",
    "without = without.astype({'id':int, 'time':int})\n",
    "without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1607/1607 [00:02<00:00, 714.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://dacon.io/competitions/official/235689/codeshare/2347?page=1&dtype=recent&ptype=pub\n",
    "# 데이터 증강을 통해 과접합을 줄여보자 - DACON\n",
    "# 증강할 데이터 정리\n",
    "\n",
    "without_train = []\n",
    "\n",
    "for uid in tqdm(without['id'].unique()):\n",
    "    temp = np.array(without[without['id'] == uid].iloc[:,2:], np.float32).T\n",
    "    without_train.append(temp)\n",
    "\n",
    "without_train = np.array(without_train, np.float32)\n",
    "without_train = without_train[:,:,:,np.newaxis].reshape(-1,600,6)\n",
    "\n",
    "without_label = train_label[train_label['label'] != 26]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 3125/3125 [00:05<00:00, 606.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 782/782 [00:00<00:00, 1718.29it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://dacon.io/competitions/official/235689/codeshare/2347?page=1&dtype=recent&ptype=pub\n",
    "# 데이터 증강을 통해 과접합을 줄여보자 - DACON\n",
    "# 원본 데이터 정리\n",
    "import tensorflow as tf\n",
    "\n",
    "x_train = []\n",
    "\n",
    "for uid in tqdm(train['id'].unique()):\n",
    "    temp = np.array(train[train['id'] == uid].iloc[:,2:], np.float32).T\n",
    "    x_train.append(temp)\n",
    "\n",
    "x_train = np.array(x_train, np.float32)\n",
    "x_train = x_train[:,:,:,np.newaxis].reshape(-1,600,6)\n",
    "\n",
    "y_train = train_label['label']\n",
    "y_categorical = tf.keras.utils.to_categorical(y_train)\n",
    "\n",
    "x_test = []\n",
    "\n",
    "for uid in tqdm(test['id'].unique()):\n",
    "    temp = np.array(test[test['id'] == uid].iloc[:,2:], np.float32).T\n",
    "    x_test.append(temp)\n",
    "\n",
    "x_test = np.array(x_test, np.float32)\n",
    "x_test = x_test[:,:,:,np.newaxis].reshape(-1,600,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 325.10it/s]\n"
     ]
    }
   ],
   "source": [
    "def aug(data, shift):\n",
    "    shift_data = np.roll(data, shift, axis=2)\n",
    "    return shift_data\n",
    "\n",
    "# 데이터 증강\n",
    "shift_data = []\n",
    "shift_label = []\n",
    "for n in tqdm(range(20)):\n",
    "    shifted = aug(without_train, n*30)\n",
    "    shift_data.append(shifted)\n",
    "    shift_label.append(without_label)\n",
    "\n",
    "shift_data = np.array(shift_data).reshape(-1,600,6)\n",
    "shift_label = np.array(shift_label).reshape(-1,1)\n",
    "shift_categorical = tf.keras.utils.to_categorical(shift_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35265, 600, 6)\n",
      "(35265, 61)\n"
     ]
    }
   ],
   "source": [
    "# 원본 데이터와 증강 데이터 합치기\n",
    "concat_train = np.concatenate((x_train, shift_data), axis=0)\n",
    "concat_label = np.concatenate((y_categorical, shift_categorical), axis=0)\n",
    "print(concat_train.shape)\n",
    "print(concat_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Activation, Convolution2D, MaxPooling2D, BatchNormalization, Flatten, Dense, Dropout, Conv2D,MaxPool2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pc\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 740s 26ms/sample - loss: 3.3602 - accuracy: 0.1472 - val_loss: 2.8865 - val_accuracy: 0.2148\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 439us/sample - loss: 2.6386 - accuracy: 0.2794 - val_loss: 2.4490 - val_accuracy: 0.3197\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 441us/sample - loss: 2.3011 - accuracy: 0.3613 - val_loss: 2.1857 - val_accuracy: 0.3915\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 13s 444us/sample - loss: 2.0494 - accuracy: 0.4233 - val_loss: 1.9674 - val_accuracy: 0.4475\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 440us/sample - loss: 1.8480 - accuracy: 0.4769 - val_loss: 1.7835 - val_accuracy: 0.5038\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 439us/sample - loss: 1.6706 - accuracy: 0.5309 - val_loss: 1.6348 - val_accuracy: 0.5334\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 1.5010 - accuracy: 0.5761 - val_loss: 1.4521 - val_accuracy: 0.5918\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 439us/sample - loss: 1.3586 - accuracy: 0.6198 - val_loss: 1.3331 - val_accuracy: 0.6117\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 440us/sample - loss: 1.2236 - accuracy: 0.6585 - val_loss: 1.2396 - val_accuracy: 0.6531\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 440us/sample - loss: 1.0974 - accuracy: 0.7002 - val_loss: 1.1147 - val_accuracy: 0.6908\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 442us/sample - loss: 0.9871 - accuracy: 0.7312 - val_loss: 0.9815 - val_accuracy: 0.7317\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 13s 444us/sample - loss: 0.8859 - accuracy: 0.7623 - val_loss: 0.8980 - val_accuracy: 0.7465\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 13s 443us/sample - loss: 0.7958 - accuracy: 0.7901 - val_loss: 0.7795 - val_accuracy: 0.7853\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 13s 444us/sample - loss: 0.7088 - accuracy: 0.8157 - val_loss: 0.7410 - val_accuracy: 0.7957\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 13s 449us/sample - loss: 0.6435 - accuracy: 0.8376 - val_loss: 0.6891 - val_accuracy: 0.8150\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.5821 - accuracy: 0.8548 - val_loss: 0.6823 - val_accuracy: 0.8290\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.5324 - accuracy: 0.8687 - val_loss: 0.5250 - val_accuracy: 0.8825\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 0.4763 - accuracy: 0.8850 - val_loss: 0.5618 - val_accuracy: 0.8525\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 13s 449us/sample - loss: 0.4526 - accuracy: 0.8913 - val_loss: 0.4891 - val_accuracy: 0.8791\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 0.4079 - accuracy: 0.9041 - val_loss: 0.4045 - val_accuracy: 0.9064\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 13s 450us/sample - loss: 0.3612 - accuracy: 0.9158 - val_loss: 0.3831 - val_accuracy: 0.9138\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 13s 450us/sample - loss: 0.3387 - accuracy: 0.9212 - val_loss: 0.3406 - val_accuracy: 0.9277\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 0.3104 - accuracy: 0.9300 - val_loss: 0.3252 - val_accuracy: 0.9278\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 0.2800 - accuracy: 0.9346 - val_loss: 0.3133 - val_accuracy: 0.9270\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 0.2685 - accuracy: 0.9383 - val_loss: 0.2718 - val_accuracy: 0.9437\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 0.2481 - accuracy: 0.9430 - val_loss: 0.2826 - val_accuracy: 0.9412\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.2426 - accuracy: 0.9452 - val_loss: 0.2827 - val_accuracy: 0.9403\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 13s 444us/sample - loss: 0.2192 - accuracy: 0.9504 - val_loss: 0.2643 - val_accuracy: 0.9318\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 0.2114 - accuracy: 0.9529 - val_loss: 0.5611 - val_accuracy: 0.8636\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 13s 444us/sample - loss: 0.1952 - accuracy: 0.9552 - val_loss: 0.2940 - val_accuracy: 0.9297\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.1947 - accuracy: 0.9555 - val_loss: 0.2352 - val_accuracy: 0.9465\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 0.1734 - accuracy: 0.9601 - val_loss: 0.2296 - val_accuracy: 0.9420\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.1708 - accuracy: 0.9616 - val_loss: 0.1792 - val_accuracy: 0.9647\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 13s 450us/sample - loss: 0.1619 - accuracy: 0.9630 - val_loss: 0.1580 - val_accuracy: 0.9692\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 0.1556 - accuracy: 0.9646 - val_loss: 0.1597 - val_accuracy: 0.9668\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 443us/sample - loss: 0.1582 - accuracy: 0.9635 - val_loss: 0.1839 - val_accuracy: 0.9565\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 13s 444us/sample - loss: 0.1435 - accuracy: 0.9673 - val_loss: 0.2146 - val_accuracy: 0.9474\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 13s 449us/sample - loss: 0.1335 - accuracy: 0.9706 - val_loss: 0.1911 - val_accuracy: 0.9525\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 439us/sample - loss: 0.1329 - accuracy: 0.9691 - val_loss: 0.1700 - val_accuracy: 0.9626\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 431us/sample - loss: 0.1229 - accuracy: 0.9727 - val_loss: 0.2023 - val_accuracy: 0.9590\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.1200 - accuracy: 0.9723 - val_loss: 0.1723 - val_accuracy: 0.9596\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.1147 - accuracy: 0.9732 - val_loss: 0.1408 - val_accuracy: 0.9715\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.1112 - accuracy: 0.9759 - val_loss: 0.1819 - val_accuracy: 0.9556\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.1108 - accuracy: 0.9752 - val_loss: 0.1410 - val_accuracy: 0.9663\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1064 - accuracy: 0.9758 - val_loss: 0.1608 - val_accuracy: 0.9623\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1074 - accuracy: 0.9758 - val_loss: 0.1522 - val_accuracy: 0.9687\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 13s 449us/sample - loss: 0.0982 - accuracy: 0.9778 - val_loss: 0.1649 - val_accuracy: 0.9596\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 440us/sample - loss: 0.1094 - accuracy: 0.9761 - val_loss: 0.1426 - val_accuracy: 0.9668\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0934 - accuracy: 0.9778 - val_loss: 0.1253 - val_accuracy: 0.9726\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 440us/sample - loss: 0.0891 - accuracy: 0.9794 - val_loss: 0.1444 - val_accuracy: 0.9655\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 440us/sample - loss: 0.0837 - accuracy: 0.9796 - val_loss: 0.1225 - val_accuracy: 0.9718\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 421us/sample - loss: 0.0846 - accuracy: 0.9802 - val_loss: 0.1625 - val_accuracy: 0.9599\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 423us/sample - loss: 0.0811 - accuracy: 0.9811 - val_loss: 0.1777 - val_accuracy: 0.9606\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.0854 - accuracy: 0.9792 - val_loss: 0.1426 - val_accuracy: 0.9682\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.0835 - accuracy: 0.9797 - val_loss: 0.1454 - val_accuracy: 0.9697\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 13s 462us/sample - loss: 0.0767 - accuracy: 0.9819 - val_loss: 0.1132 - val_accuracy: 0.9758\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 13s 462us/sample - loss: 0.0863 - accuracy: 0.9814 - val_loss: 0.1104 - val_accuracy: 0.9762\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 13s 468us/sample - loss: 0.0747 - accuracy: 0.9821 - val_loss: 0.2517 - val_accuracy: 0.9414\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 13s 452us/sample - loss: 0.0737 - accuracy: 0.9829 - val_loss: 0.1080 - val_accuracy: 0.9777\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 13s 454us/sample - loss: 0.0796 - accuracy: 0.9824 - val_loss: 0.1293 - val_accuracy: 0.9687\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 442us/sample - loss: 0.0744 - accuracy: 0.9826 - val_loss: 0.1206 - val_accuracy: 0.9739\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 13s 462us/sample - loss: 0.0668 - accuracy: 0.9842 - val_loss: 0.1267 - val_accuracy: 0.9699\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 13s 455us/sample - loss: 0.0731 - accuracy: 0.9833 - val_loss: 0.1167 - val_accuracy: 0.9767\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 13s 458us/sample - loss: 0.0773 - accuracy: 0.9822 - val_loss: 0.1633 - val_accuracy: 0.9660\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 13s 468us/sample - loss: 0.0775 - accuracy: 0.9824 - val_loss: 0.1093 - val_accuracy: 0.9780\n",
      "Epoch 66/100\n",
      "28212/28212 [==============================] - 13s 451us/sample - loss: 0.0602 - accuracy: 0.9860 - val_loss: 0.1052 - val_accuracy: 0.9780\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 13s 454us/sample - loss: 0.0677 - accuracy: 0.9839 - val_loss: 0.1075 - val_accuracy: 0.9735\n",
      "Epoch 68/100\n",
      "28212/28212 [==============================] - 13s 455us/sample - loss: 0.0738 - accuracy: 0.9833 - val_loss: 0.1173 - val_accuracy: 0.9784\n",
      "Epoch 69/100\n",
      "28212/28212 [==============================] - 13s 458us/sample - loss: 0.0656 - accuracy: 0.9845 - val_loss: 0.1339 - val_accuracy: 0.9748\n",
      "Epoch 70/100\n",
      "28212/28212 [==============================] - 13s 451us/sample - loss: 0.0607 - accuracy: 0.9861 - val_loss: 0.1608 - val_accuracy: 0.9607\n",
      "Epoch 71/100\n",
      "28212/28212 [==============================] - 13s 450us/sample - loss: 0.0657 - accuracy: 0.9854 - val_loss: 0.1069 - val_accuracy: 0.9793\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 14s 496us/sample - loss: 3.3645 - accuracy: 0.1491 - val_loss: 2.9126 - val_accuracy: 0.2288\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 13s 452us/sample - loss: 2.6560 - accuracy: 0.2701 - val_loss: 2.4957 - val_accuracy: 0.3175\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 13s 452us/sample - loss: 2.2973 - accuracy: 0.3492 - val_loss: 2.1813 - val_accuracy: 0.3854\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 13s 449us/sample - loss: 2.0559 - accuracy: 0.4031 - val_loss: 1.9832 - val_accuracy: 0.4235\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 1.8408 - accuracy: 0.4648 - val_loss: 1.8147 - val_accuracy: 0.4614\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 1.6450 - accuracy: 0.5191 - val_loss: 1.6052 - val_accuracy: 0.5315\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 13s 445us/sample - loss: 1.4857 - accuracy: 0.5665 - val_loss: 1.5508 - val_accuracy: 0.5656\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 1.3493 - accuracy: 0.6062 - val_loss: 1.3061 - val_accuracy: 0.6131\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.2288 - accuracy: 0.6481 - val_loss: 1.2206 - val_accuracy: 0.6474\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.1050 - accuracy: 0.6851 - val_loss: 1.0895 - val_accuracy: 0.6894\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.0000 - accuracy: 0.7182 - val_loss: 0.9956 - val_accuracy: 0.7200\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.9216 - accuracy: 0.7439 - val_loss: 0.8799 - val_accuracy: 0.7673\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.8357 - accuracy: 0.7712 - val_loss: 0.8232 - val_accuracy: 0.7641\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.7835 - accuracy: 0.7871 - val_loss: 0.7187 - val_accuracy: 0.8162\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.6896 - accuracy: 0.8145 - val_loss: 0.8822 - val_accuracy: 0.7618\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.6500 - accuracy: 0.8249 - val_loss: 0.6622 - val_accuracy: 0.8239\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.5992 - accuracy: 0.8412 - val_loss: 0.5857 - val_accuracy: 0.8456\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.5606 - accuracy: 0.8523 - val_loss: 0.5758 - val_accuracy: 0.8527\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.5357 - accuracy: 0.8578 - val_loss: 0.5105 - val_accuracy: 0.8677\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.4825 - accuracy: 0.8731 - val_loss: 0.4767 - val_accuracy: 0.8778\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.4524 - accuracy: 0.8844 - val_loss: 0.5162 - val_accuracy: 0.8698\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.4279 - accuracy: 0.8899 - val_loss: 0.4292 - val_accuracy: 0.8833\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.3980 - accuracy: 0.8977 - val_loss: 0.3806 - val_accuracy: 0.9025\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.3847 - accuracy: 0.9040 - val_loss: 0.3864 - val_accuracy: 0.9110\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 13s 450us/sample - loss: 0.3627 - accuracy: 0.9066 - val_loss: 0.3320 - val_accuracy: 0.9212\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.3395 - accuracy: 0.9130 - val_loss: 0.3642 - val_accuracy: 0.9108\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.3300 - accuracy: 0.9187 - val_loss: 0.6384 - val_accuracy: 0.8357\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.3082 - accuracy: 0.9209 - val_loss: 0.3359 - val_accuracy: 0.9090\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.2894 - accuracy: 0.9277 - val_loss: 0.3317 - val_accuracy: 0.9080\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.2948 - accuracy: 0.9254 - val_loss: 0.3591 - val_accuracy: 0.8975\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.2751 - accuracy: 0.9319 - val_loss: 0.3312 - val_accuracy: 0.9145\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.2663 - accuracy: 0.9339 - val_loss: 0.2633 - val_accuracy: 0.9284\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2402 - accuracy: 0.9411 - val_loss: 0.2211 - val_accuracy: 0.9488\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2627 - accuracy: 0.9356 - val_loss: 0.3663 - val_accuracy: 0.9025\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.2245 - accuracy: 0.9434 - val_loss: 0.4200 - val_accuracy: 0.8934\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2356 - accuracy: 0.9412 - val_loss: 0.2285 - val_accuracy: 0.9465\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.2056 - accuracy: 0.9493 - val_loss: 0.2772 - val_accuracy: 0.9314\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.2140 - accuracy: 0.9467 - val_loss: 0.3433 - val_accuracy: 0.9000\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1950 - accuracy: 0.9525 - val_loss: 0.2746 - val_accuracy: 0.9416\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1840 - accuracy: 0.9557 - val_loss: 0.2291 - val_accuracy: 0.9406\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1739 - accuracy: 0.9570 - val_loss: 0.1637 - val_accuracy: 0.9648\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1798 - accuracy: 0.9566 - val_loss: 0.1588 - val_accuracy: 0.9631\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1704 - accuracy: 0.9596 - val_loss: 0.2073 - val_accuracy: 0.9526\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1582 - accuracy: 0.9611 - val_loss: 0.1621 - val_accuracy: 0.9667\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1564 - accuracy: 0.9624 - val_loss: 0.1844 - val_accuracy: 0.9528\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1591 - accuracy: 0.9628 - val_loss: 0.1575 - val_accuracy: 0.9620\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1412 - accuracy: 0.9675 - val_loss: 0.1609 - val_accuracy: 0.9650\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1474 - accuracy: 0.9652 - val_loss: 0.6868 - val_accuracy: 0.8306\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1362 - accuracy: 0.9675 - val_loss: 0.1476 - val_accuracy: 0.9677\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1326 - accuracy: 0.9678 - val_loss: 0.1649 - val_accuracy: 0.9624\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1272 - accuracy: 0.9707 - val_loss: 0.2093 - val_accuracy: 0.9460\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1296 - accuracy: 0.9679 - val_loss: 0.1463 - val_accuracy: 0.9636\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 0.1293 - accuracy: 0.9695 - val_loss: 0.3342 - val_accuracy: 0.9128\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1261 - accuracy: 0.9709 - val_loss: 0.1365 - val_accuracy: 0.9694\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1194 - accuracy: 0.9717 - val_loss: 0.1481 - val_accuracy: 0.9684\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1238 - accuracy: 0.9719 - val_loss: 0.2443 - val_accuracy: 0.9515\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1172 - accuracy: 0.9726 - val_loss: 0.1899 - val_accuracy: 0.9494\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1138 - accuracy: 0.9735 - val_loss: 0.1241 - val_accuracy: 0.9709\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 431us/sample - loss: 0.1080 - accuracy: 0.9746 - val_loss: 0.1236 - val_accuracy: 0.9745\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.1073 - accuracy: 0.9745 - val_loss: 0.2349 - val_accuracy: 0.9390\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1337 - accuracy: 0.9679 - val_loss: 0.1446 - val_accuracy: 0.9687\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.1130 - accuracy: 0.9731 - val_loss: 0.1163 - val_accuracy: 0.9758\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1026 - accuracy: 0.9760 - val_loss: 0.3426 - val_accuracy: 0.9182\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0960 - accuracy: 0.9778 - val_loss: 0.1257 - val_accuracy: 0.9743\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0965 - accuracy: 0.9769 - val_loss: 0.4705 - val_accuracy: 0.9077\n",
      "Epoch 66/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1002 - accuracy: 0.9762 - val_loss: 0.1065 - val_accuracy: 0.9784\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1050 - accuracy: 0.9741 - val_loss: 0.1469 - val_accuracy: 0.9624\n",
      "Epoch 68/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0850 - accuracy: 0.9794 - val_loss: 0.1404 - val_accuracy: 0.9695\n",
      "Epoch 69/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1007 - accuracy: 0.9763 - val_loss: 0.1181 - val_accuracy: 0.9699\n",
      "Epoch 70/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0899 - accuracy: 0.9797 - val_loss: 0.3038 - val_accuracy: 0.9270\n",
      "Epoch 71/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0827 - accuracy: 0.9806 - val_loss: 0.1134 - val_accuracy: 0.9772\n",
      "Epoch 72/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0881 - accuracy: 0.9793 - val_loss: 0.1117 - val_accuracy: 0.9738\n",
      "Epoch 73/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0961 - accuracy: 0.9778 - val_loss: 0.1650 - val_accuracy: 0.9619\n",
      "Epoch 74/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0873 - accuracy: 0.9802 - val_loss: 0.1435 - val_accuracy: 0.9651\n",
      "Epoch 75/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0832 - accuracy: 0.9802 - val_loss: 0.1252 - val_accuracy: 0.9739\n",
      "Epoch 76/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1025 - accuracy: 0.9763 - val_loss: 0.2098 - val_accuracy: 0.9538\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 14s 484us/sample - loss: 3.3961 - accuracy: 0.1368 - val_loss: 2.9550 - val_accuracy: 0.2222\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 2.6771 - accuracy: 0.2738 - val_loss: 2.4471 - val_accuracy: 0.3136\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 2.2710 - accuracy: 0.3617 - val_loss: 2.1403 - val_accuracy: 0.3672\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 1.9934 - accuracy: 0.4258 - val_loss: 1.9032 - val_accuracy: 0.4402\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 1.7737 - accuracy: 0.4904 - val_loss: 1.6793 - val_accuracy: 0.5262\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.5799 - accuracy: 0.5476 - val_loss: 1.5497 - val_accuracy: 0.5508\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.4347 - accuracy: 0.5913 - val_loss: 1.3568 - val_accuracy: 0.6318\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 1.2635 - accuracy: 0.6405 - val_loss: 1.2362 - val_accuracy: 0.6533\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.1446 - accuracy: 0.6787 - val_loss: 1.1616 - val_accuracy: 0.6633\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.0152 - accuracy: 0.7186 - val_loss: 1.1884 - val_accuracy: 0.6667\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.9112 - accuracy: 0.7530 - val_loss: 0.8831 - val_accuracy: 0.7604\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.8101 - accuracy: 0.7826 - val_loss: 0.7320 - val_accuracy: 0.8026\n",
      "Epoch 13/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.7380 - accuracy: 0.8045 - val_loss: 0.7100 - val_accuracy: 0.8144\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.6653 - accuracy: 0.8274 - val_loss: 0.8457 - val_accuracy: 0.7690\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.6002 - accuracy: 0.8462 - val_loss: 0.5521 - val_accuracy: 0.8622\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.5303 - accuracy: 0.8648 - val_loss: 0.5012 - val_accuracy: 0.8761\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.4746 - accuracy: 0.8824 - val_loss: 0.4894 - val_accuracy: 0.8744\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.4516 - accuracy: 0.8878 - val_loss: 0.4109 - val_accuracy: 0.8995\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.4063 - accuracy: 0.8988 - val_loss: 0.3656 - val_accuracy: 0.9142\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.3570 - accuracy: 0.9137 - val_loss: 0.3455 - val_accuracy: 0.9196\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.3305 - accuracy: 0.9203 - val_loss: 0.3585 - val_accuracy: 0.9101\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.3224 - accuracy: 0.9218 - val_loss: 0.3343 - val_accuracy: 0.9206\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.2813 - accuracy: 0.9338 - val_loss: 0.2971 - val_accuracy: 0.9253\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2629 - accuracy: 0.9371 - val_loss: 0.2431 - val_accuracy: 0.9490\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2523 - accuracy: 0.9382 - val_loss: 0.3479 - val_accuracy: 0.9202\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2245 - accuracy: 0.9468 - val_loss: 0.2817 - val_accuracy: 0.9297\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2173 - accuracy: 0.9489 - val_loss: 0.3392 - val_accuracy: 0.9090\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2123 - accuracy: 0.9493 - val_loss: 0.2480 - val_accuracy: 0.9356\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1972 - accuracy: 0.9539 - val_loss: 0.1897 - val_accuracy: 0.9526\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1819 - accuracy: 0.9575 - val_loss: 0.1794 - val_accuracy: 0.9607\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1627 - accuracy: 0.9614 - val_loss: 0.6999 - val_accuracy: 0.8433\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1612 - accuracy: 0.9621 - val_loss: 0.1810 - val_accuracy: 0.9566\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1590 - accuracy: 0.9623 - val_loss: 0.1692 - val_accuracy: 0.9597\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1475 - accuracy: 0.9659 - val_loss: 0.1690 - val_accuracy: 0.9678\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1302 - accuracy: 0.9701 - val_loss: 0.1953 - val_accuracy: 0.9551\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1385 - accuracy: 0.9685 - val_loss: 0.2297 - val_accuracy: 0.9501\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.1409 - accuracy: 0.9666 - val_loss: 0.1353 - val_accuracy: 0.9725\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.1267 - accuracy: 0.9700 - val_loss: 0.1398 - val_accuracy: 0.9702\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1152 - accuracy: 0.9730 - val_loss: 0.1268 - val_accuracy: 0.9716\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1040 - accuracy: 0.9751 - val_loss: 0.2057 - val_accuracy: 0.9422\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 441us/sample - loss: 0.1025 - accuracy: 0.9758 - val_loss: 0.1117 - val_accuracy: 0.9735\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.1348 - accuracy: 0.9675 - val_loss: 0.1461 - val_accuracy: 0.9626\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1014 - accuracy: 0.9760 - val_loss: 0.1162 - val_accuracy: 0.9745\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1145 - accuracy: 0.9747 - val_loss: 0.1239 - val_accuracy: 0.9699\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0984 - accuracy: 0.9759 - val_loss: 0.1210 - val_accuracy: 0.9715\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.0937 - accuracy: 0.9780 - val_loss: 0.1127 - val_accuracy: 0.9750\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.0930 - accuracy: 0.9779 - val_loss: 0.1459 - val_accuracy: 0.9641\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0910 - accuracy: 0.9778 - val_loss: 0.1556 - val_accuracy: 0.9592\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0831 - accuracy: 0.9798 - val_loss: 0.1543 - val_accuracy: 0.9600\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0820 - accuracy: 0.9821 - val_loss: 0.1058 - val_accuracy: 0.9758\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0892 - accuracy: 0.9783 - val_loss: 0.1218 - val_accuracy: 0.9694\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0896 - accuracy: 0.9798 - val_loss: 0.0963 - val_accuracy: 0.9787\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0802 - accuracy: 0.9802 - val_loss: 0.1052 - val_accuracy: 0.9721\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.0827 - accuracy: 0.9805 - val_loss: 0.1116 - val_accuracy: 0.9731\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0825 - accuracy: 0.9804 - val_loss: 0.1438 - val_accuracy: 0.9658\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0783 - accuracy: 0.9814 - val_loss: 0.1490 - val_accuracy: 0.9640\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0837 - accuracy: 0.9808 - val_loss: 0.2047 - val_accuracy: 0.9468\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0706 - accuracy: 0.9836 - val_loss: 0.2576 - val_accuracy: 0.9405\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0805 - accuracy: 0.9804 - val_loss: 0.1170 - val_accuracy: 0.9663\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0748 - accuracy: 0.9814 - val_loss: 0.1021 - val_accuracy: 0.9772\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0691 - accuracy: 0.9828 - val_loss: 0.0896 - val_accuracy: 0.9802\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.0590 - accuracy: 0.9852 - val_loss: 0.0955 - val_accuracy: 0.9775\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.0731 - accuracy: 0.9825 - val_loss: 0.1742 - val_accuracy: 0.9641\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.0616 - accuracy: 0.9848 - val_loss: 0.1109 - val_accuracy: 0.9758\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0657 - accuracy: 0.9851 - val_loss: 0.1136 - val_accuracy: 0.9732\n",
      "Epoch 66/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0722 - accuracy: 0.9835 - val_loss: 0.1158 - val_accuracy: 0.9759\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0776 - accuracy: 0.9809 - val_loss: 0.0962 - val_accuracy: 0.9797\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 477us/sample - loss: 3.3682 - accuracy: 0.1395 - val_loss: 2.9101 - val_accuracy: 0.2062\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 2.6574 - accuracy: 0.2596 - val_loss: 2.4514 - val_accuracy: 0.3192\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 2.2959 - accuracy: 0.3514 - val_loss: 2.1737 - val_accuracy: 0.3762\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 2.0394 - accuracy: 0.4171 - val_loss: 1.9461 - val_accuracy: 0.4425\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.8267 - accuracy: 0.4726 - val_loss: 1.7677 - val_accuracy: 0.4882\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.6430 - accuracy: 0.5249 - val_loss: 1.5733 - val_accuracy: 0.5440\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 1.4738 - accuracy: 0.5741 - val_loss: 1.4396 - val_accuracy: 0.5823\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.3382 - accuracy: 0.6196 - val_loss: 1.3069 - val_accuracy: 0.6275\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.2006 - accuracy: 0.6631 - val_loss: 1.1967 - val_accuracy: 0.6474\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 1.0844 - accuracy: 0.7024 - val_loss: 1.0701 - val_accuracy: 0.7152\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.9850 - accuracy: 0.7345 - val_loss: 0.9719 - val_accuracy: 0.7429\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.8899 - accuracy: 0.7644 - val_loss: 0.9345 - val_accuracy: 0.7459\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.8012 - accuracy: 0.7915 - val_loss: 0.8093 - val_accuracy: 0.8045\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.7239 - accuracy: 0.8142 - val_loss: 0.7919 - val_accuracy: 0.7896\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.6534 - accuracy: 0.8323 - val_loss: 0.6730 - val_accuracy: 0.8317\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.5976 - accuracy: 0.8514 - val_loss: 0.5936 - val_accuracy: 0.8656\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.5425 - accuracy: 0.8638 - val_loss: 0.6758 - val_accuracy: 0.8238\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.4893 - accuracy: 0.8782 - val_loss: 0.4783 - val_accuracy: 0.8837\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.4514 - accuracy: 0.8881 - val_loss: 0.5130 - val_accuracy: 0.8691\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.4222 - accuracy: 0.8936 - val_loss: 0.4408 - val_accuracy: 0.8998\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.4050 - accuracy: 0.8996 - val_loss: 0.4652 - val_accuracy: 0.8825\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.3701 - accuracy: 0.9074 - val_loss: 0.4146 - val_accuracy: 0.8978\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.3516 - accuracy: 0.9133 - val_loss: 0.5659 - val_accuracy: 0.8596\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3195 - accuracy: 0.9211 - val_loss: 0.3448 - val_accuracy: 0.9226\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2936 - accuracy: 0.9262 - val_loss: 0.3953 - val_accuracy: 0.8962\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2879 - accuracy: 0.9257 - val_loss: 0.3854 - val_accuracy: 0.8959\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2559 - accuracy: 0.9351 - val_loss: 0.3109 - val_accuracy: 0.9290\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2571 - accuracy: 0.9338 - val_loss: 0.2788 - val_accuracy: 0.9363\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2316 - accuracy: 0.9409 - val_loss: 0.2791 - val_accuracy: 0.9307\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2213 - accuracy: 0.9433 - val_loss: 0.3390 - val_accuracy: 0.9110\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2105 - accuracy: 0.9473 - val_loss: 0.2339 - val_accuracy: 0.9429\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.2071 - accuracy: 0.9473 - val_loss: 0.2533 - val_accuracy: 0.9376\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 13s 450us/sample - loss: 0.1886 - accuracy: 0.9529 - val_loss: 0.2131 - val_accuracy: 0.9585\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.1949 - accuracy: 0.9506 - val_loss: 0.2584 - val_accuracy: 0.9359\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1751 - accuracy: 0.9565 - val_loss: 0.3425 - val_accuracy: 0.9135\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1754 - accuracy: 0.9552 - val_loss: 0.2106 - val_accuracy: 0.9507\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1655 - accuracy: 0.9588 - val_loss: 0.2146 - val_accuracy: 0.9467\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1622 - accuracy: 0.9589 - val_loss: 0.1899 - val_accuracy: 0.9534\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1725 - accuracy: 0.9553 - val_loss: 0.1893 - val_accuracy: 0.9599\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1481 - accuracy: 0.9629 - val_loss: 0.1791 - val_accuracy: 0.9604\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.1504 - accuracy: 0.9626 - val_loss: 0.2052 - val_accuracy: 0.9575\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.1389 - accuracy: 0.9644 - val_loss: 0.1686 - val_accuracy: 0.9638\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1396 - accuracy: 0.9655 - val_loss: 0.2673 - val_accuracy: 0.9378\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.1388 - accuracy: 0.9644 - val_loss: 0.2153 - val_accuracy: 0.9549\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.1309 - accuracy: 0.9670 - val_loss: 0.3707 - val_accuracy: 0.9029\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.1212 - accuracy: 0.9699 - val_loss: 0.1774 - val_accuracy: 0.9619\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.1214 - accuracy: 0.9695 - val_loss: 0.2682 - val_accuracy: 0.9336\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1189 - accuracy: 0.9701 - val_loss: 0.1944 - val_accuracy: 0.9587\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.1129 - accuracy: 0.9710 - val_loss: 0.2860 - val_accuracy: 0.9268\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.1094 - accuracy: 0.9728 - val_loss: 0.1353 - val_accuracy: 0.9749\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.1229 - accuracy: 0.9693 - val_loss: 0.1637 - val_accuracy: 0.9627\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1008 - accuracy: 0.9747 - val_loss: 0.2116 - val_accuracy: 0.9471\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0979 - accuracy: 0.9760 - val_loss: 0.1542 - val_accuracy: 0.9630\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0982 - accuracy: 0.9755 - val_loss: 0.1444 - val_accuracy: 0.9687\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0952 - accuracy: 0.9759 - val_loss: 0.1605 - val_accuracy: 0.9647\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0972 - accuracy: 0.9765 - val_loss: 0.1530 - val_accuracy: 0.9685\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0924 - accuracy: 0.9767 - val_loss: 0.1539 - val_accuracy: 0.9638\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0947 - accuracy: 0.9769 - val_loss: 0.2059 - val_accuracy: 0.9505\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0894 - accuracy: 0.9780 - val_loss: 0.1490 - val_accuracy: 0.9671\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.0966 - accuracy: 0.9760 - val_loss: 0.1520 - val_accuracy: 0.9670\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.0825 - accuracy: 0.9792 - val_loss: 0.1441 - val_accuracy: 0.9692\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0827 - accuracy: 0.9803 - val_loss: 0.1566 - val_accuracy: 0.9641\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0854 - accuracy: 0.9784 - val_loss: 0.1417 - val_accuracy: 0.9718\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0833 - accuracy: 0.9791 - val_loss: 0.1350 - val_accuracy: 0.9716\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0755 - accuracy: 0.9815 - val_loss: 0.2391 - val_accuracy: 0.9490\n",
      "Epoch 66/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0857 - accuracy: 0.9796 - val_loss: 0.1467 - val_accuracy: 0.9699\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0833 - accuracy: 0.9804 - val_loss: 0.1372 - val_accuracy: 0.9728\n",
      "Epoch 68/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0773 - accuracy: 0.9809 - val_loss: 0.1257 - val_accuracy: 0.9735\n",
      "Epoch 69/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0721 - accuracy: 0.9831 - val_loss: 0.1683 - val_accuracy: 0.9614\n",
      "Epoch 70/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0773 - accuracy: 0.9816 - val_loss: 0.1879 - val_accuracy: 0.9580\n",
      "Epoch 71/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0690 - accuracy: 0.9829 - val_loss: 0.1243 - val_accuracy: 0.9729\n",
      "Epoch 72/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 0.0738 - accuracy: 0.9816 - val_loss: 0.1360 - val_accuracy: 0.9704\n",
      "Epoch 73/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.0715 - accuracy: 0.9831 - val_loss: 0.1614 - val_accuracy: 0.9636\n",
      "Epoch 74/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0637 - accuracy: 0.9838 - val_loss: 0.2312 - val_accuracy: 0.9382\n",
      "Epoch 75/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.0694 - accuracy: 0.9828 - val_loss: 0.2112 - val_accuracy: 0.9573\n",
      "Epoch 76/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 0.0804 - accuracy: 0.9810 - val_loss: 0.1300 - val_accuracy: 0.9742\n",
      "Epoch 77/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0664 - accuracy: 0.9843 - val_loss: 0.1902 - val_accuracy: 0.9606\n",
      "Epoch 78/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0554 - accuracy: 0.9869 - val_loss: 0.1542 - val_accuracy: 0.9722\n",
      "Epoch 79/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0767 - accuracy: 0.9807 - val_loss: 0.1451 - val_accuracy: 0.9709\n",
      "Epoch 80/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0739 - accuracy: 0.9824 - val_loss: 0.1261 - val_accuracy: 0.9753\n",
      "Epoch 81/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0737 - accuracy: 0.9834 - val_loss: 0.1702 - val_accuracy: 0.9621\n",
      "Epoch 82/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.0647 - accuracy: 0.9832 - val_loss: 0.1475 - val_accuracy: 0.9668\n",
      "Epoch 83/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0604 - accuracy: 0.9853 - val_loss: 0.1648 - val_accuracy: 0.9604\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 470us/sample - loss: 3.3637 - accuracy: 0.1362 - val_loss: 2.9269 - val_accuracy: 0.2195\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 2.6398 - accuracy: 0.2689 - val_loss: 2.4736 - val_accuracy: 0.3094\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 2.2779 - accuracy: 0.3580 - val_loss: 2.1722 - val_accuracy: 0.3709\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 2.0024 - accuracy: 0.4229 - val_loss: 2.0060 - val_accuracy: 0.4149\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 1.7697 - accuracy: 0.4881 - val_loss: 1.7433 - val_accuracy: 0.4883\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 1.5677 - accuracy: 0.5482 - val_loss: 1.5642 - val_accuracy: 0.5408\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 434us/sample - loss: 1.3855 - accuracy: 0.6028 - val_loss: 1.3563 - val_accuracy: 0.5944\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 1.2217 - accuracy: 0.6487 - val_loss: 1.3230 - val_accuracy: 0.6183\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 1.0799 - accuracy: 0.6965 - val_loss: 1.0907 - val_accuracy: 0.6957\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.9575 - accuracy: 0.7347 - val_loss: 0.9679 - val_accuracy: 0.7295\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.8472 - accuracy: 0.7742 - val_loss: 0.8922 - val_accuracy: 0.7619\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.7588 - accuracy: 0.8022 - val_loss: 0.7366 - val_accuracy: 0.8120\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 0.6801 - accuracy: 0.8256 - val_loss: 0.7898 - val_accuracy: 0.7826\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.6070 - accuracy: 0.8480 - val_loss: 0.6059 - val_accuracy: 0.8564\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 436us/sample - loss: 0.5454 - accuracy: 0.8643 - val_loss: 0.5539 - val_accuracy: 0.8625\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.4933 - accuracy: 0.8798 - val_loss: 0.5334 - val_accuracy: 0.8620\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.4535 - accuracy: 0.8910 - val_loss: 0.5500 - val_accuracy: 0.8506\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.4171 - accuracy: 0.8994 - val_loss: 0.4253 - val_accuracy: 0.9000\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3767 - accuracy: 0.9101 - val_loss: 0.3443 - val_accuracy: 0.9295\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3468 - accuracy: 0.9170 - val_loss: 0.3802 - val_accuracy: 0.9135\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3227 - accuracy: 0.9232 - val_loss: 0.3560 - val_accuracy: 0.9125\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2992 - accuracy: 0.9289 - val_loss: 0.2823 - val_accuracy: 0.9386\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2807 - accuracy: 0.9333 - val_loss: 0.3711 - val_accuracy: 0.9051\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2627 - accuracy: 0.9377 - val_loss: 0.2831 - val_accuracy: 0.9294\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2493 - accuracy: 0.9398 - val_loss: 0.2530 - val_accuracy: 0.9457\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2310 - accuracy: 0.9447 - val_loss: 0.2568 - val_accuracy: 0.9376\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2146 - accuracy: 0.9482 - val_loss: 0.2393 - val_accuracy: 0.9464\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2079 - accuracy: 0.9490 - val_loss: 0.2385 - val_accuracy: 0.9434\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1929 - accuracy: 0.9539 - val_loss: 0.2268 - val_accuracy: 0.9450\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1932 - accuracy: 0.9543 - val_loss: 0.1978 - val_accuracy: 0.9573\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 423us/sample - loss: 0.1819 - accuracy: 0.9550 - val_loss: 0.2054 - val_accuracy: 0.9451\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1727 - accuracy: 0.9588 - val_loss: 0.1852 - val_accuracy: 0.9604\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1664 - accuracy: 0.9589 - val_loss: 0.2165 - val_accuracy: 0.9482\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1680 - accuracy: 0.9578 - val_loss: 0.2289 - val_accuracy: 0.9461\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1544 - accuracy: 0.9618 - val_loss: 0.1679 - val_accuracy: 0.9614\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1390 - accuracy: 0.9666 - val_loss: 0.1816 - val_accuracy: 0.9548\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1381 - accuracy: 0.9668 - val_loss: 0.3252 - val_accuracy: 0.9056\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1485 - accuracy: 0.9657 - val_loss: 0.1881 - val_accuracy: 0.9518\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1308 - accuracy: 0.9680 - val_loss: 0.1819 - val_accuracy: 0.9549\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1258 - accuracy: 0.9692 - val_loss: 0.2164 - val_accuracy: 0.9514\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1261 - accuracy: 0.9694 - val_loss: 0.1399 - val_accuracy: 0.9697\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1210 - accuracy: 0.9698 - val_loss: 0.1504 - val_accuracy: 0.9619\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1147 - accuracy: 0.9724 - val_loss: 0.1996 - val_accuracy: 0.9501\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1127 - accuracy: 0.9712 - val_loss: 0.1720 - val_accuracy: 0.9582\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.1042 - accuracy: 0.9742 - val_loss: 0.1296 - val_accuracy: 0.9732\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0995 - accuracy: 0.9759 - val_loss: 0.1891 - val_accuracy: 0.9507\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0990 - accuracy: 0.9752 - val_loss: 0.1616 - val_accuracy: 0.9569\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1014 - accuracy: 0.9764 - val_loss: 0.2002 - val_accuracy: 0.9572\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0920 - accuracy: 0.9778 - val_loss: 0.1328 - val_accuracy: 0.9694\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0931 - accuracy: 0.9770 - val_loss: 0.1369 - val_accuracy: 0.9689\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1092 - accuracy: 0.9727 - val_loss: 0.1373 - val_accuracy: 0.9701\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0962 - accuracy: 0.9763 - val_loss: 0.1145 - val_accuracy: 0.9767\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0956 - accuracy: 0.9772 - val_loss: 0.1297 - val_accuracy: 0.9729\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0852 - accuracy: 0.9795 - val_loss: 0.2034 - val_accuracy: 0.9481\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0836 - accuracy: 0.9792 - val_loss: 0.1378 - val_accuracy: 0.9663\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1109 - accuracy: 0.9729 - val_loss: 0.1328 - val_accuracy: 0.9707\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.0824 - accuracy: 0.9787 - val_loss: 0.2033 - val_accuracy: 0.9439\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0861 - accuracy: 0.9793 - val_loss: 0.1116 - val_accuracy: 0.9732\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0724 - accuracy: 0.9816 - val_loss: 0.1269 - val_accuracy: 0.9729\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0740 - accuracy: 0.9818 - val_loss: 0.2210 - val_accuracy: 0.9373\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0709 - accuracy: 0.9826 - val_loss: 0.1434 - val_accuracy: 0.9660\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0726 - accuracy: 0.9822 - val_loss: 0.1637 - val_accuracy: 0.9630\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0739 - accuracy: 0.9810 - val_loss: 0.1183 - val_accuracy: 0.9736\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0680 - accuracy: 0.9817 - val_loss: 0.1064 - val_accuracy: 0.9776\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0708 - accuracy: 0.9822 - val_loss: 0.1261 - val_accuracy: 0.9755\n",
      "Epoch 66/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0708 - accuracy: 0.9825 - val_loss: 0.1479 - val_accuracy: 0.9661\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0700 - accuracy: 0.9825 - val_loss: 0.1247 - val_accuracy: 0.9731\n",
      "Epoch 68/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0652 - accuracy: 0.9839 - val_loss: 0.1083 - val_accuracy: 0.9750\n",
      "Epoch 69/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0658 - accuracy: 0.9828 - val_loss: 0.1336 - val_accuracy: 0.9714\n",
      "Epoch 70/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0622 - accuracy: 0.9851 - val_loss: 0.1082 - val_accuracy: 0.9773\n",
      "Epoch 71/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0622 - accuracy: 0.9838 - val_loss: 0.1136 - val_accuracy: 0.9750\n",
      "Epoch 72/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0687 - accuracy: 0.9829 - val_loss: 0.2123 - val_accuracy: 0.9434\n",
      "Epoch 73/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.0676 - accuracy: 0.9831 - val_loss: 0.1258 - val_accuracy: 0.9714\n",
      "Epoch 74/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0683 - accuracy: 0.9830 - val_loss: 0.1409 - val_accuracy: 0.9674\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.0676 - accuracy: 0.9840 - val_loss: 0.2689 - val_accuracy: 0.9305\n",
      "Epoch 76/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.0582 - accuracy: 0.9845 - val_loss: 0.1427 - val_accuracy: 0.9702\n",
      "Epoch 77/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.0584 - accuracy: 0.9852 - val_loss: 0.2124 - val_accuracy: 0.9390\n",
      "Epoch 78/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0596 - accuracy: 0.9852 - val_loss: 0.1278 - val_accuracy: 0.9731\n",
      "Epoch 79/100\n",
      "28212/28212 [==============================] - 12s 423us/sample - loss: 0.0616 - accuracy: 0.9842 - val_loss: 0.1099 - val_accuracy: 0.9766\n",
      "Epoch 80/100\n",
      "28212/28212 [==============================] - 12s 423us/sample - loss: 0.0617 - accuracy: 0.9850 - val_loss: 0.1870 - val_accuracy: 0.9487\n",
      "Epoch 81/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0530 - accuracy: 0.9866 - val_loss: 0.1502 - val_accuracy: 0.9599\n",
      "Epoch 82/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0602 - accuracy: 0.9855 - val_loss: 0.1178 - val_accuracy: 0.9746\n",
      "Epoch 83/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0688 - accuracy: 0.9839 - val_loss: 0.4507 - val_accuracy: 0.8981\n",
      "Epoch 84/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0647 - accuracy: 0.9845 - val_loss: 0.1066 - val_accuracy: 0.9779\n",
      "Epoch 85/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0526 - accuracy: 0.9867 - val_loss: 0.0999 - val_accuracy: 0.9784\n",
      "Epoch 86/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0578 - accuracy: 0.9853 - val_loss: 0.1445 - val_accuracy: 0.9664\n",
      "Epoch 87/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0541 - accuracy: 0.9863 - val_loss: 0.1446 - val_accuracy: 0.9677\n",
      "Epoch 88/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0586 - accuracy: 0.9865 - val_loss: 0.1180 - val_accuracy: 0.9755\n",
      "Epoch 89/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0631 - accuracy: 0.9848 - val_loss: 0.2187 - val_accuracy: 0.9477\n",
      "Epoch 90/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0531 - accuracy: 0.9864 - val_loss: 0.1069 - val_accuracy: 0.9782\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\pc\\.conda\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass shuffle=True, random_state=42 as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 464us/sample - loss: 3.4210 - accuracy: 0.1330 - val_loss: 3.0253 - val_accuracy: 0.1896\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 2.7462 - accuracy: 0.2489 - val_loss: 2.5668 - val_accuracy: 0.2780\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 2.3612 - accuracy: 0.3295 - val_loss: 2.2299 - val_accuracy: 0.3468\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 2.0840 - accuracy: 0.3959 - val_loss: 2.0511 - val_accuracy: 0.4132\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 1.8536 - accuracy: 0.4579 - val_loss: 1.7509 - val_accuracy: 0.4767\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.6514 - accuracy: 0.5160 - val_loss: 1.5950 - val_accuracy: 0.5446\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.4719 - accuracy: 0.5760 - val_loss: 1.4432 - val_accuracy: 0.5965\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.3051 - accuracy: 0.6324 - val_loss: 1.3214 - val_accuracy: 0.6270\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.1683 - accuracy: 0.6815 - val_loss: 1.1938 - val_accuracy: 0.6706\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.0447 - accuracy: 0.7176 - val_loss: 0.9697 - val_accuracy: 0.7509\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.9322 - accuracy: 0.7547 - val_loss: 0.9453 - val_accuracy: 0.7421\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.8419 - accuracy: 0.7786 - val_loss: 0.8214 - val_accuracy: 0.7853\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.7588 - accuracy: 0.8088 - val_loss: 0.7205 - val_accuracy: 0.8228\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.6866 - accuracy: 0.8261 - val_loss: 0.6664 - val_accuracy: 0.8334\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.6263 - accuracy: 0.8442 - val_loss: 0.6168 - val_accuracy: 0.8470\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.5692 - accuracy: 0.8601 - val_loss: 0.6110 - val_accuracy: 0.8585\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.5291 - accuracy: 0.8705 - val_loss: 0.4862 - val_accuracy: 0.8918\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.4763 - accuracy: 0.8842 - val_loss: 0.4863 - val_accuracy: 0.8813\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.4317 - accuracy: 0.8967 - val_loss: 0.4066 - val_accuracy: 0.9083\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.4149 - accuracy: 0.8997 - val_loss: 0.3821 - val_accuracy: 0.9139\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3839 - accuracy: 0.9077 - val_loss: 0.5025 - val_accuracy: 0.8765\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3629 - accuracy: 0.9106 - val_loss: 0.4162 - val_accuracy: 0.8949\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3271 - accuracy: 0.9229 - val_loss: 0.4378 - val_accuracy: 0.8941\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.3111 - accuracy: 0.9248 - val_loss: 0.7397 - val_accuracy: 0.8130\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2901 - accuracy: 0.9299 - val_loss: 0.2751 - val_accuracy: 0.9380\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.2849 - accuracy: 0.9314 - val_loss: 0.2909 - val_accuracy: 0.9335\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.2661 - accuracy: 0.9355 - val_loss: 0.3111 - val_accuracy: 0.9224\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 438us/sample - loss: 0.2454 - accuracy: 0.9397 - val_loss: 0.2761 - val_accuracy: 0.9372\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.2456 - accuracy: 0.9416 - val_loss: 0.2608 - val_accuracy: 0.9456\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2252 - accuracy: 0.9457 - val_loss: 0.2494 - val_accuracy: 0.9454\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2126 - accuracy: 0.9486 - val_loss: 0.2363 - val_accuracy: 0.9491\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2025 - accuracy: 0.9522 - val_loss: 0.2465 - val_accuracy: 0.9402\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1980 - accuracy: 0.9528 - val_loss: 0.1895 - val_accuracy: 0.9607\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1847 - accuracy: 0.9561 - val_loss: 0.1990 - val_accuracy: 0.9539\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1750 - accuracy: 0.9571 - val_loss: 0.1988 - val_accuracy: 0.9573\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1721 - accuracy: 0.9596 - val_loss: 0.1698 - val_accuracy: 0.9613\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1660 - accuracy: 0.9608 - val_loss: 0.1906 - val_accuracy: 0.9565\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1531 - accuracy: 0.9627 - val_loss: 0.1672 - val_accuracy: 0.9630\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1489 - accuracy: 0.9644 - val_loss: 0.2805 - val_accuracy: 0.9287\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1467 - accuracy: 0.9660 - val_loss: 0.1846 - val_accuracy: 0.9592\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1396 - accuracy: 0.9673 - val_loss: 0.2016 - val_accuracy: 0.9542\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.1327 - accuracy: 0.9691 - val_loss: 0.1612 - val_accuracy: 0.9621\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1340 - accuracy: 0.9689 - val_loss: 0.1940 - val_accuracy: 0.9585\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.1313 - accuracy: 0.9705 - val_loss: 0.1570 - val_accuracy: 0.9641\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1261 - accuracy: 0.9700 - val_loss: 0.1704 - val_accuracy: 0.9614\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1230 - accuracy: 0.9703 - val_loss: 0.1325 - val_accuracy: 0.9731\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1122 - accuracy: 0.9745 - val_loss: 0.1295 - val_accuracy: 0.9702\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1238 - accuracy: 0.9701 - val_loss: 0.1993 - val_accuracy: 0.9498\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1133 - accuracy: 0.9747 - val_loss: 0.2300 - val_accuracy: 0.9417\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1126 - accuracy: 0.9736 - val_loss: 0.2977 - val_accuracy: 0.9460\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1011 - accuracy: 0.9761 - val_loss: 0.1268 - val_accuracy: 0.9715\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1031 - accuracy: 0.9749 - val_loss: 0.1222 - val_accuracy: 0.9738\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0946 - accuracy: 0.9788 - val_loss: 0.2168 - val_accuracy: 0.9519\n",
      "Epoch 54/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0948 - accuracy: 0.9777 - val_loss: 0.8632 - val_accuracy: 0.8369\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0974 - accuracy: 0.9769 - val_loss: 0.1521 - val_accuracy: 0.9653\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0994 - accuracy: 0.9767 - val_loss: 0.1148 - val_accuracy: 0.9748\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.0981 - accuracy: 0.9784 - val_loss: 0.1939 - val_accuracy: 0.9602\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.0911 - accuracy: 0.9784 - val_loss: 0.1593 - val_accuracy: 0.9646\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0952 - accuracy: 0.9777 - val_loss: 0.1168 - val_accuracy: 0.9763\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0923 - accuracy: 0.9787 - val_loss: 0.1341 - val_accuracy: 0.9702\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0851 - accuracy: 0.9792 - val_loss: 0.1667 - val_accuracy: 0.9616\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0817 - accuracy: 0.9798 - val_loss: 0.1547 - val_accuracy: 0.9655\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0796 - accuracy: 0.9809 - val_loss: 0.2526 - val_accuracy: 0.9387\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0859 - accuracy: 0.9803 - val_loss: 0.1195 - val_accuracy: 0.9742\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0851 - accuracy: 0.9794 - val_loss: 0.2177 - val_accuracy: 0.9474\n",
      "Epoch 66/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0877 - accuracy: 0.9791 - val_loss: 0.1443 - val_accuracy: 0.9721\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0776 - accuracy: 0.9811 - val_loss: 0.1031 - val_accuracy: 0.9772\n",
      "Epoch 68/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0784 - accuracy: 0.9812 - val_loss: 0.1095 - val_accuracy: 0.9777\n",
      "Epoch 69/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0721 - accuracy: 0.9836 - val_loss: 0.1902 - val_accuracy: 0.9522\n",
      "Epoch 70/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0849 - accuracy: 0.9804 - val_loss: 0.1549 - val_accuracy: 0.9708\n",
      "Epoch 71/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0686 - accuracy: 0.9838 - val_loss: 0.1876 - val_accuracy: 0.9599\n",
      "Epoch 72/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0706 - accuracy: 0.9839 - val_loss: 0.2045 - val_accuracy: 0.9535\n",
      "Epoch 73/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0715 - accuracy: 0.9833 - val_loss: 0.1093 - val_accuracy: 0.9789\n",
      "Epoch 74/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0685 - accuracy: 0.9847 - val_loss: 0.1129 - val_accuracy: 0.9765\n",
      "Epoch 75/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0795 - accuracy: 0.9805 - val_loss: 0.1204 - val_accuracy: 0.9743\n",
      "Epoch 76/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0581 - accuracy: 0.9863 - val_loss: 0.1075 - val_accuracy: 0.9763\n",
      "Epoch 77/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0569 - accuracy: 0.9860 - val_loss: 0.1003 - val_accuracy: 0.9789\n",
      "Epoch 78/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0663 - accuracy: 0.9838 - val_loss: 0.1298 - val_accuracy: 0.9748\n",
      "Epoch 79/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0716 - accuracy: 0.9832 - val_loss: 0.1221 - val_accuracy: 0.9749\n",
      "Epoch 80/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0722 - accuracy: 0.9834 - val_loss: 0.1958 - val_accuracy: 0.9644\n",
      "Epoch 81/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0633 - accuracy: 0.9846 - val_loss: 0.1312 - val_accuracy: 0.9746\n",
      "Epoch 82/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0600 - accuracy: 0.9845 - val_loss: 0.2297 - val_accuracy: 0.9436\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 464us/sample - loss: 3.4220 - accuracy: 0.1332 - val_loss: 3.0349 - val_accuracy: 0.1984\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 2.7692 - accuracy: 0.2479 - val_loss: 2.6102 - val_accuracy: 0.2831\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 2.4169 - accuracy: 0.3260 - val_loss: 2.2921 - val_accuracy: 0.3553\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 2.1275 - accuracy: 0.3969 - val_loss: 2.0433 - val_accuracy: 0.4197\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.9024 - accuracy: 0.4568 - val_loss: 1.8650 - val_accuracy: 0.4578\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 1.7117 - accuracy: 0.5077 - val_loss: 1.7233 - val_accuracy: 0.5137\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.5453 - accuracy: 0.5603 - val_loss: 1.5437 - val_accuracy: 0.5593\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.3870 - accuracy: 0.6072 - val_loss: 1.3644 - val_accuracy: 0.6061\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.2479 - accuracy: 0.6481 - val_loss: 1.2481 - val_accuracy: 0.6454\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 1.1255 - accuracy: 0.6864 - val_loss: 1.1331 - val_accuracy: 0.6855\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 1.0090 - accuracy: 0.7230 - val_loss: 1.0163 - val_accuracy: 0.7173\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.9028 - accuracy: 0.7544 - val_loss: 0.8919 - val_accuracy: 0.7580\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.8172 - accuracy: 0.7828 - val_loss: 0.8892 - val_accuracy: 0.7594\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.7360 - accuracy: 0.8080 - val_loss: 0.7316 - val_accuracy: 0.8157\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.6744 - accuracy: 0.8240 - val_loss: 0.6802 - val_accuracy: 0.8289\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.6100 - accuracy: 0.8452 - val_loss: 0.6102 - val_accuracy: 0.8447\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.5526 - accuracy: 0.8610 - val_loss: 0.5950 - val_accuracy: 0.8496\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.5122 - accuracy: 0.8719 - val_loss: 0.5006 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.4655 - accuracy: 0.8827 - val_loss: 0.5429 - val_accuracy: 0.8490\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.4276 - accuracy: 0.8934 - val_loss: 0.4874 - val_accuracy: 0.8748\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 430us/sample - loss: 0.4042 - accuracy: 0.9001 - val_loss: 0.4413 - val_accuracy: 0.8820\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3792 - accuracy: 0.9062 - val_loss: 0.3549 - val_accuracy: 0.9161\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3497 - accuracy: 0.9146 - val_loss: 0.3899 - val_accuracy: 0.9036\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3257 - accuracy: 0.9194 - val_loss: 0.3627 - val_accuracy: 0.9110\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.3057 - accuracy: 0.9264 - val_loss: 0.3194 - val_accuracy: 0.9261\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2909 - accuracy: 0.9298 - val_loss: 0.3057 - val_accuracy: 0.9270\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2609 - accuracy: 0.9360 - val_loss: 0.2868 - val_accuracy: 0.9361\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2504 - accuracy: 0.9395 - val_loss: 0.2715 - val_accuracy: 0.9312\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2376 - accuracy: 0.9425 - val_loss: 0.2356 - val_accuracy: 0.9465\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2283 - accuracy: 0.9428 - val_loss: 0.2383 - val_accuracy: 0.9473\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2115 - accuracy: 0.9491 - val_loss: 0.2472 - val_accuracy: 0.9450\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1981 - accuracy: 0.9519 - val_loss: 0.2870 - val_accuracy: 0.9268\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1999 - accuracy: 0.9523 - val_loss: 0.2319 - val_accuracy: 0.9474\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1835 - accuracy: 0.9560 - val_loss: 0.3175 - val_accuracy: 0.9341\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1733 - accuracy: 0.9592 - val_loss: 0.2251 - val_accuracy: 0.9439\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1590 - accuracy: 0.9624 - val_loss: 0.1896 - val_accuracy: 0.9568\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1839 - accuracy: 0.9560 - val_loss: 0.2401 - val_accuracy: 0.9366\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1547 - accuracy: 0.9627 - val_loss: 0.1897 - val_accuracy: 0.9597\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1466 - accuracy: 0.9655 - val_loss: 0.1674 - val_accuracy: 0.9678\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1352 - accuracy: 0.9679 - val_loss: 0.2115 - val_accuracy: 0.9541\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1348 - accuracy: 0.9684 - val_loss: 0.1833 - val_accuracy: 0.9590\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1336 - accuracy: 0.9693 - val_loss: 0.1932 - val_accuracy: 0.9577\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1280 - accuracy: 0.9705 - val_loss: 0.2810 - val_accuracy: 0.9335\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1172 - accuracy: 0.9721 - val_loss: 0.1636 - val_accuracy: 0.9651\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1223 - accuracy: 0.9713 - val_loss: 0.1494 - val_accuracy: 0.9654\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1742 - accuracy: 0.9629 - val_loss: 0.2086 - val_accuracy: 0.9531\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1447 - accuracy: 0.9657 - val_loss: 0.7397 - val_accuracy: 0.8323\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1175 - accuracy: 0.9722 - val_loss: 0.1459 - val_accuracy: 0.9680\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1200 - accuracy: 0.9711 - val_loss: 0.1778 - val_accuracy: 0.9559\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 464us/sample - loss: 3.3903 - accuracy: 0.1404 - val_loss: 2.9142 - val_accuracy: 0.2162\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 2.7022 - accuracy: 0.2602 - val_loss: 2.5119 - val_accuracy: 0.3021\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 2.3647 - accuracy: 0.3380 - val_loss: 2.2639 - val_accuracy: 0.3594\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 2.1089 - accuracy: 0.4044 - val_loss: 2.0188 - val_accuracy: 0.4193\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.8824 - accuracy: 0.4598 - val_loss: 1.8545 - val_accuracy: 0.4621\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.6722 - accuracy: 0.5189 - val_loss: 1.6561 - val_accuracy: 0.5158\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.4864 - accuracy: 0.5732 - val_loss: 1.5175 - val_accuracy: 0.5511\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.3295 - accuracy: 0.6187 - val_loss: 1.3318 - val_accuracy: 0.6214\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.1891 - accuracy: 0.6615 - val_loss: 1.1954 - val_accuracy: 0.6684\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.0820 - accuracy: 0.6961 - val_loss: 1.0240 - val_accuracy: 0.7142\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.9692 - accuracy: 0.7304 - val_loss: 0.9630 - val_accuracy: 0.7218\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.8724 - accuracy: 0.7595 - val_loss: 0.8201 - val_accuracy: 0.7768\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.7880 - accuracy: 0.7829 - val_loss: 0.9437 - val_accuracy: 0.7483\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.7113 - accuracy: 0.8075 - val_loss: 0.8830 - val_accuracy: 0.7598\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.6532 - accuracy: 0.8295 - val_loss: 0.6582 - val_accuracy: 0.8197\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.5951 - accuracy: 0.8448 - val_loss: 0.6225 - val_accuracy: 0.8345\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.5517 - accuracy: 0.8578 - val_loss: 0.5451 - val_accuracy: 0.8567\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.5026 - accuracy: 0.8692 - val_loss: 0.5166 - val_accuracy: 0.8625\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.4641 - accuracy: 0.8821 - val_loss: 0.4662 - val_accuracy: 0.8786\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.4360 - accuracy: 0.8865 - val_loss: 0.4785 - val_accuracy: 0.8731\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.4043 - accuracy: 0.8995 - val_loss: 0.4406 - val_accuracy: 0.8893\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3732 - accuracy: 0.9055 - val_loss: 0.3481 - val_accuracy: 0.9206\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3530 - accuracy: 0.9124 - val_loss: 0.3319 - val_accuracy: 0.9264\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.3136 - accuracy: 0.9236 - val_loss: 0.3801 - val_accuracy: 0.9005\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.3108 - accuracy: 0.9241 - val_loss: 0.3941 - val_accuracy: 0.9030\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2919 - accuracy: 0.9295 - val_loss: 0.3019 - val_accuracy: 0.9318\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.2790 - accuracy: 0.9341 - val_loss: 0.4929 - val_accuracy: 0.8965\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2741 - accuracy: 0.9330 - val_loss: 0.3248 - val_accuracy: 0.9064\n",
      "Epoch 29/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.2540 - accuracy: 0.9389 - val_loss: 0.2460 - val_accuracy: 0.9470\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2320 - accuracy: 0.9458 - val_loss: 0.2350 - val_accuracy: 0.9593\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2359 - accuracy: 0.9419 - val_loss: 0.2462 - val_accuracy: 0.9495\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.2166 - accuracy: 0.9473 - val_loss: 0.2579 - val_accuracy: 0.9344\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.2077 - accuracy: 0.9509 - val_loss: 0.2262 - val_accuracy: 0.9480\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1908 - accuracy: 0.9551 - val_loss: 0.2088 - val_accuracy: 0.9593\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1915 - accuracy: 0.9534 - val_loss: 0.3544 - val_accuracy: 0.9073\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1838 - accuracy: 0.9556 - val_loss: 0.1796 - val_accuracy: 0.9617\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1894 - accuracy: 0.9541 - val_loss: 0.2268 - val_accuracy: 0.9498\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1681 - accuracy: 0.9603 - val_loss: 0.1896 - val_accuracy: 0.9577\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1638 - accuracy: 0.9623 - val_loss: 0.1839 - val_accuracy: 0.9619\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1545 - accuracy: 0.9621 - val_loss: 0.1706 - val_accuracy: 0.9660\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1513 - accuracy: 0.9647 - val_loss: 0.2064 - val_accuracy: 0.9484\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1435 - accuracy: 0.9670 - val_loss: 0.2057 - val_accuracy: 0.9586\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1297 - accuracy: 0.9692 - val_loss: 0.2080 - val_accuracy: 0.9528\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1421 - accuracy: 0.9661 - val_loss: 0.1766 - val_accuracy: 0.9580\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1479 - accuracy: 0.9664 - val_loss: 0.1247 - val_accuracy: 0.9728\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1297 - accuracy: 0.9692 - val_loss: 0.2363 - val_accuracy: 0.9433\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1154 - accuracy: 0.9737 - val_loss: 0.1825 - val_accuracy: 0.9535\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1232 - accuracy: 0.9722 - val_loss: 0.1353 - val_accuracy: 0.9709\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1328 - accuracy: 0.9704 - val_loss: 0.2104 - val_accuracy: 0.9589\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1153 - accuracy: 0.9733 - val_loss: 0.1365 - val_accuracy: 0.9736\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1106 - accuracy: 0.9742 - val_loss: 0.1519 - val_accuracy: 0.9678\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1021 - accuracy: 0.9752 - val_loss: 0.1760 - val_accuracy: 0.9586\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1103 - accuracy: 0.9741 - val_loss: 0.1195 - val_accuracy: 0.9719\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1063 - accuracy: 0.9746 - val_loss: 0.1842 - val_accuracy: 0.9603\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1087 - accuracy: 0.9728 - val_loss: 0.1603 - val_accuracy: 0.9665\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0947 - accuracy: 0.9771 - val_loss: 0.2032 - val_accuracy: 0.9532\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1165 - accuracy: 0.9730 - val_loss: 0.4623 - val_accuracy: 0.8898\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1138 - accuracy: 0.9730 - val_loss: 0.1376 - val_accuracy: 0.9729\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0973 - accuracy: 0.9759 - val_loss: 0.1110 - val_accuracy: 0.9752\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0878 - accuracy: 0.9795 - val_loss: 0.1186 - val_accuracy: 0.9738\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0936 - accuracy: 0.9768 - val_loss: 0.1100 - val_accuracy: 0.9789\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0922 - accuracy: 0.9778 - val_loss: 0.1280 - val_accuracy: 0.9724\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0892 - accuracy: 0.9781 - val_loss: 0.1207 - val_accuracy: 0.9756\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0959 - accuracy: 0.9776 - val_loss: 0.1319 - val_accuracy: 0.9745\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.0912 - accuracy: 0.9785 - val_loss: 0.1295 - val_accuracy: 0.9729\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 464us/sample - loss: 3.3882 - accuracy: 0.1381 - val_loss: 2.9774 - val_accuracy: 0.2113\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 2.7305 - accuracy: 0.2563 - val_loss: 2.5225 - val_accuracy: 0.3009\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 2.3735 - accuracy: 0.3367 - val_loss: 2.2587 - val_accuracy: 0.3597\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 2.1064 - accuracy: 0.4057 - val_loss: 2.0103 - val_accuracy: 0.4246\n",
      "Epoch 5/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.8804 - accuracy: 0.4643 - val_loss: 1.8081 - val_accuracy: 0.4726\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 1.7124 - accuracy: 0.5124 - val_loss: 1.6617 - val_accuracy: 0.5205\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.5462 - accuracy: 0.5634 - val_loss: 1.5944 - val_accuracy: 0.5459\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 1.3924 - accuracy: 0.6057 - val_loss: 1.3951 - val_accuracy: 0.6067\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 1.2565 - accuracy: 0.6473 - val_loss: 1.2473 - val_accuracy: 0.6423\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.1420 - accuracy: 0.6801 - val_loss: 1.1072 - val_accuracy: 0.6918\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.0313 - accuracy: 0.7163 - val_loss: 1.0851 - val_accuracy: 0.6770\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.9348 - accuracy: 0.7421 - val_loss: 0.9974 - val_accuracy: 0.7154\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.8522 - accuracy: 0.7679 - val_loss: 0.8484 - val_accuracy: 0.7705\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.7751 - accuracy: 0.7907 - val_loss: 0.7695 - val_accuracy: 0.7984\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.7103 - accuracy: 0.8090 - val_loss: 0.7758 - val_accuracy: 0.7913\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.6714 - accuracy: 0.8211 - val_loss: 0.7392 - val_accuracy: 0.7991\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.6077 - accuracy: 0.8393 - val_loss: 0.6829 - val_accuracy: 0.8063\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.5640 - accuracy: 0.8517 - val_loss: 0.6127 - val_accuracy: 0.8399\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.5129 - accuracy: 0.8667 - val_loss: 0.5300 - val_accuracy: 0.8618\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.4849 - accuracy: 0.8732 - val_loss: 0.4870 - val_accuracy: 0.8747\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.4386 - accuracy: 0.8859 - val_loss: 0.4759 - val_accuracy: 0.8687\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.4181 - accuracy: 0.8936 - val_loss: 0.4412 - val_accuracy: 0.8908\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.3842 - accuracy: 0.9033 - val_loss: 0.3653 - val_accuracy: 0.9172\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.3528 - accuracy: 0.9123 - val_loss: 0.4823 - val_accuracy: 0.8741\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3277 - accuracy: 0.9203 - val_loss: 0.3393 - val_accuracy: 0.9199\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3266 - accuracy: 0.9197 - val_loss: 0.3412 - val_accuracy: 0.9210\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2892 - accuracy: 0.9295 - val_loss: 0.3192 - val_accuracy: 0.9275\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2806 - accuracy: 0.9321 - val_loss: 0.2680 - val_accuracy: 0.9368\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2594 - accuracy: 0.9359 - val_loss: 0.2641 - val_accuracy: 0.9424\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2467 - accuracy: 0.9422 - val_loss: 0.2927 - val_accuracy: 0.9277\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.2296 - accuracy: 0.9460 - val_loss: 0.2807 - val_accuracy: 0.9321\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2255 - accuracy: 0.9467 - val_loss: 0.3167 - val_accuracy: 0.9250\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2050 - accuracy: 0.9507 - val_loss: 0.2040 - val_accuracy: 0.9542\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1961 - accuracy: 0.9544 - val_loss: 0.2093 - val_accuracy: 0.9518\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1888 - accuracy: 0.9549 - val_loss: 0.1985 - val_accuracy: 0.9572\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1819 - accuracy: 0.9564 - val_loss: 0.1880 - val_accuracy: 0.9589\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1745 - accuracy: 0.9594 - val_loss: 0.2152 - val_accuracy: 0.9501\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1693 - accuracy: 0.9599 - val_loss: 0.2263 - val_accuracy: 0.9458\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1585 - accuracy: 0.9619 - val_loss: 0.2463 - val_accuracy: 0.9387\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1602 - accuracy: 0.9610 - val_loss: 0.1859 - val_accuracy: 0.9542\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1582 - accuracy: 0.9619 - val_loss: 0.1728 - val_accuracy: 0.9617\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1607 - accuracy: 0.9619 - val_loss: 0.2285 - val_accuracy: 0.9402\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1375 - accuracy: 0.9667 - val_loss: 0.1893 - val_accuracy: 0.9538\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1452 - accuracy: 0.9655 - val_loss: 0.1663 - val_accuracy: 0.9626\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1308 - accuracy: 0.9686 - val_loss: 0.2975 - val_accuracy: 0.9339\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1387 - accuracy: 0.9663 - val_loss: 0.1896 - val_accuracy: 0.9575\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1269 - accuracy: 0.9709 - val_loss: 0.3060 - val_accuracy: 0.9171\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1210 - accuracy: 0.9694 - val_loss: 0.2009 - val_accuracy: 0.9498\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.1322 - accuracy: 0.9678 - val_loss: 0.1737 - val_accuracy: 0.9609\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1161 - accuracy: 0.9720 - val_loss: 0.1783 - val_accuracy: 0.9617\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1180 - accuracy: 0.9713 - val_loss: 0.1979 - val_accuracy: 0.9524\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.1209 - accuracy: 0.9721 - val_loss: 0.2132 - val_accuracy: 0.9494\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1374 - accuracy: 0.9682 - val_loss: 0.1891 - val_accuracy: 0.9604\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.1040 - accuracy: 0.9736 - val_loss: 0.1719 - val_accuracy: 0.9590\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1101 - accuracy: 0.9737 - val_loss: 0.1276 - val_accuracy: 0.9733\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0951 - accuracy: 0.9778 - val_loss: 0.2623 - val_accuracy: 0.9356\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1239 - accuracy: 0.9716 - val_loss: 0.2462 - val_accuracy: 0.9543\n",
      "Epoch 58/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1053 - accuracy: 0.9755 - val_loss: 0.1419 - val_accuracy: 0.9688\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0943 - accuracy: 0.9771 - val_loss: 0.1373 - val_accuracy: 0.9665\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0882 - accuracy: 0.9786 - val_loss: 0.2306 - val_accuracy: 0.9528\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.0922 - accuracy: 0.9783 - val_loss: 0.1595 - val_accuracy: 0.9641\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.0992 - accuracy: 0.9760 - val_loss: 0.1116 - val_accuracy: 0.9772\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0883 - accuracy: 0.9778 - val_loss: 0.3881 - val_accuracy: 0.9135\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0937 - accuracy: 0.9771 - val_loss: 0.1556 - val_accuracy: 0.9709\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2105 - accuracy: 0.9582 - val_loss: 0.1408 - val_accuracy: 0.9721\n",
      "------------------\n",
      "Train on 28212 samples, validate on 7053 samples\n",
      "Epoch 1/100\n",
      "28212/28212 [==============================] - 13s 463us/sample - loss: 3.4304 - accuracy: 0.1327 - val_loss: 3.0382 - val_accuracy: 0.1965\n",
      "Epoch 2/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 2.7412 - accuracy: 0.2603 - val_loss: 2.5535 - val_accuracy: 0.3040\n",
      "Epoch 3/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 2.3660 - accuracy: 0.3367 - val_loss: 2.2896 - val_accuracy: 0.3460\n",
      "Epoch 4/100\n",
      "28212/28212 [==============================] - 12s 432us/sample - loss: 2.1210 - accuracy: 0.3982 - val_loss: 2.0610 - val_accuracy: 0.4194\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 435us/sample - loss: 1.9166 - accuracy: 0.4560 - val_loss: 1.8530 - val_accuracy: 0.4835\n",
      "Epoch 6/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 1.7240 - accuracy: 0.5140 - val_loss: 1.6901 - val_accuracy: 0.5188\n",
      "Epoch 7/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 1.5606 - accuracy: 0.5598 - val_loss: 1.5772 - val_accuracy: 0.5633\n",
      "Epoch 8/100\n",
      "28212/28212 [==============================] - 12s 437us/sample - loss: 1.4215 - accuracy: 0.5997 - val_loss: 1.4510 - val_accuracy: 0.5963\n",
      "Epoch 9/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.2878 - accuracy: 0.6427 - val_loss: 1.2493 - val_accuracy: 0.6546\n",
      "Epoch 10/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.1617 - accuracy: 0.6811 - val_loss: 1.1958 - val_accuracy: 0.6701\n",
      "Epoch 11/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 1.0429 - accuracy: 0.7151 - val_loss: 1.0656 - val_accuracy: 0.7066\n",
      "Epoch 12/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.9525 - accuracy: 0.7417 - val_loss: 0.9729 - val_accuracy: 0.7245\n",
      "Epoch 13/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.8592 - accuracy: 0.7685 - val_loss: 0.8865 - val_accuracy: 0.7661\n",
      "Epoch 14/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.7686 - accuracy: 0.7947 - val_loss: 0.7862 - val_accuracy: 0.7946\n",
      "Epoch 15/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.7101 - accuracy: 0.8140 - val_loss: 0.7155 - val_accuracy: 0.8188\n",
      "Epoch 16/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.6444 - accuracy: 0.8310 - val_loss: 0.6906 - val_accuracy: 0.8223\n",
      "Epoch 17/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.5899 - accuracy: 0.8479 - val_loss: 0.6575 - val_accuracy: 0.8398\n",
      "Epoch 18/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.5402 - accuracy: 0.8617 - val_loss: 0.5262 - val_accuracy: 0.8751\n",
      "Epoch 19/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.4959 - accuracy: 0.8747 - val_loss: 0.5408 - val_accuracy: 0.8585\n",
      "Epoch 20/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.4602 - accuracy: 0.8840 - val_loss: 0.4361 - val_accuracy: 0.9033\n",
      "Epoch 21/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.4194 - accuracy: 0.8965 - val_loss: 0.4743 - val_accuracy: 0.8847\n",
      "Epoch 22/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3969 - accuracy: 0.9019 - val_loss: 0.4114 - val_accuracy: 0.9019\n",
      "Epoch 23/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.3654 - accuracy: 0.9115 - val_loss: 0.4521 - val_accuracy: 0.8829\n",
      "Epoch 24/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.3507 - accuracy: 0.9160 - val_loss: 0.3920 - val_accuracy: 0.9108\n",
      "Epoch 25/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.3196 - accuracy: 0.9241 - val_loss: 0.3958 - val_accuracy: 0.9023\n",
      "Epoch 26/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.3056 - accuracy: 0.9269 - val_loss: 0.3225 - val_accuracy: 0.9261\n",
      "Epoch 27/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.2800 - accuracy: 0.9336 - val_loss: 0.2971 - val_accuracy: 0.9288\n",
      "Epoch 28/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2650 - accuracy: 0.9394 - val_loss: 0.3027 - val_accuracy: 0.9249\n",
      "Epoch 29/100\n",
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.2466 - accuracy: 0.9430 - val_loss: 0.3116 - val_accuracy: 0.9236\n",
      "Epoch 30/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2322 - accuracy: 0.9477 - val_loss: 0.2556 - val_accuracy: 0.9451\n",
      "Epoch 31/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2228 - accuracy: 0.9485 - val_loss: 0.2479 - val_accuracy: 0.9387\n",
      "Epoch 32/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2111 - accuracy: 0.9535 - val_loss: 0.2707 - val_accuracy: 0.9328\n",
      "Epoch 33/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2127 - accuracy: 0.9514 - val_loss: 0.2680 - val_accuracy: 0.9478\n",
      "Epoch 34/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.2360 - accuracy: 0.9464 - val_loss: 0.2669 - val_accuracy: 0.9407\n",
      "Epoch 35/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1947 - accuracy: 0.9556 - val_loss: 0.2462 - val_accuracy: 0.9509\n",
      "Epoch 36/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1805 - accuracy: 0.9589 - val_loss: 0.2555 - val_accuracy: 0.9441\n",
      "Epoch 37/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1697 - accuracy: 0.9605 - val_loss: 0.1839 - val_accuracy: 0.9633\n",
      "Epoch 38/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1662 - accuracy: 0.9612 - val_loss: 0.1818 - val_accuracy: 0.9627\n",
      "Epoch 39/100\n",
      "28212/28212 [==============================] - 12s 429us/sample - loss: 0.1652 - accuracy: 0.9617 - val_loss: 0.2561 - val_accuracy: 0.9405\n",
      "Epoch 40/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1551 - accuracy: 0.9641 - val_loss: 0.2062 - val_accuracy: 0.9560\n",
      "Epoch 41/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1514 - accuracy: 0.9656 - val_loss: 0.1870 - val_accuracy: 0.9596\n",
      "Epoch 42/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1394 - accuracy: 0.9665 - val_loss: 0.1882 - val_accuracy: 0.9586\n",
      "Epoch 43/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1301 - accuracy: 0.9703 - val_loss: 0.2502 - val_accuracy: 0.9399\n",
      "Epoch 44/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1464 - accuracy: 0.9653 - val_loss: 0.1868 - val_accuracy: 0.9610\n",
      "Epoch 45/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1232 - accuracy: 0.9705 - val_loss: 0.1576 - val_accuracy: 0.9687\n",
      "Epoch 46/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1206 - accuracy: 0.9722 - val_loss: 0.1483 - val_accuracy: 0.9733\n",
      "Epoch 47/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1124 - accuracy: 0.9739 - val_loss: 0.2296 - val_accuracy: 0.9407\n",
      "Epoch 48/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1231 - accuracy: 0.9723 - val_loss: 0.1516 - val_accuracy: 0.9692\n",
      "Epoch 49/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1200 - accuracy: 0.9723 - val_loss: 0.1861 - val_accuracy: 0.9579\n",
      "Epoch 50/100\n",
      "28212/28212 [==============================] - 12s 426us/sample - loss: 0.1087 - accuracy: 0.9738 - val_loss: 0.2329 - val_accuracy: 0.9485\n",
      "Epoch 51/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1107 - accuracy: 0.9739 - val_loss: 0.1668 - val_accuracy: 0.9621\n",
      "Epoch 52/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1026 - accuracy: 0.9768 - val_loss: 0.1777 - val_accuracy: 0.9634\n",
      "Epoch 53/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.1028 - accuracy: 0.9753 - val_loss: 0.3538 - val_accuracy: 0.9103\n",
      "Epoch 54/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.2677 - accuracy: 0.9442 - val_loss: 0.1937 - val_accuracy: 0.9602\n",
      "Epoch 55/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1198 - accuracy: 0.9715 - val_loss: 0.1737 - val_accuracy: 0.9646\n",
      "Epoch 56/100\n",
      "28212/28212 [==============================] - 12s 427us/sample - loss: 0.1214 - accuracy: 0.9699 - val_loss: 0.2145 - val_accuracy: 0.9612\n",
      "Epoch 57/100\n",
      "28212/28212 [==============================] - 12s 425us/sample - loss: 0.0960 - accuracy: 0.9775 - val_loss: 0.1480 - val_accuracy: 0.9694\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28212/28212 [==============================] - 12s 424us/sample - loss: 0.0995 - accuracy: 0.9768 - val_loss: 0.1410 - val_accuracy: 0.9722\n",
      "Epoch 59/100\n",
      "28212/28212 [==============================] - 12s 428us/sample - loss: 0.0901 - accuracy: 0.9789 - val_loss: 0.1463 - val_accuracy: 0.9718\n",
      "Epoch 60/100\n",
      "28212/28212 [==============================] - 12s 433us/sample - loss: 0.0819 - accuracy: 0.9798 - val_loss: 0.1535 - val_accuracy: 0.9670\n",
      "Epoch 61/100\n",
      "28212/28212 [==============================] - 12s 441us/sample - loss: 0.0857 - accuracy: 0.9800 - val_loss: 0.1546 - val_accuracy: 0.9726\n",
      "Epoch 62/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.1028 - accuracy: 0.9758 - val_loss: 0.2046 - val_accuracy: 0.9515\n",
      "Epoch 63/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 0.0779 - accuracy: 0.9816 - val_loss: 0.1493 - val_accuracy: 0.9668\n",
      "Epoch 64/100\n",
      "28212/28212 [==============================] - 13s 455us/sample - loss: 0.0726 - accuracy: 0.9825 - val_loss: 0.1416 - val_accuracy: 0.9707\n",
      "Epoch 65/100\n",
      "28212/28212 [==============================] - 13s 448us/sample - loss: 0.0728 - accuracy: 0.9821 - val_loss: 0.1272 - val_accuracy: 0.9748\n",
      "Epoch 66/100\n",
      "28212/28212 [==============================] - 13s 447us/sample - loss: 0.0761 - accuracy: 0.9833 - val_loss: 0.1464 - val_accuracy: 0.9685\n",
      "Epoch 67/100\n",
      "28212/28212 [==============================] - 13s 446us/sample - loss: 0.0760 - accuracy: 0.9827 - val_loss: 0.1617 - val_accuracy: 0.9664\n",
      "Epoch 68/100\n",
      "28212/28212 [==============================] - 13s 443us/sample - loss: 0.0746 - accuracy: 0.9819 - val_loss: 0.2029 - val_accuracy: 0.9630\n",
      "Epoch 69/100\n",
      "28212/28212 [==============================] - 12s 435us/sample - loss: 0.0778 - accuracy: 0.9816 - val_loss: 0.1460 - val_accuracy: 0.9689\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델 1번: lstm\n",
    "\n",
    "def build_cnn(split_num, train, target, test, rnd):\n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((train.shape[0], 61)), np.zeros((test.shape[0], 61))\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(filepath='./model/lstm.h5',save_weights_only=True,monitor='loss',mode='min',save_best_only=True)\n",
    "    \n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, val_idx in mskf.split(train, target):\n",
    "\n",
    "        # split train, validation set\n",
    "        X = train[train_idx]\n",
    "        y = target[train_idx]\n",
    "        valid_x = train[val_idx]\n",
    "        valid_y = target[val_idx]\n",
    "\n",
    "        #가벼운 모델 생성\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(32, input_shape=(600,6)))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dense(61, activation='softmax'))\n",
    "\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.fit(X,y,\n",
    "                  epochs=100,\n",
    "                  batch_size=64,\n",
    "                  validation_data=[valid_x,valid_y],\n",
    "                  callbacks=[es,mc]\n",
    "                 )\n",
    "        \n",
    "        # save feat\n",
    "        model.load_weights('./model/lstm.h5')\n",
    "        train_pred[val_idx] = model.predict(valid_x)\n",
    "        test_pred += model.predict(test)/split_num\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred, test_pred\n",
    "\n",
    "cnn_train1, cnn_test1 = build_cnn(5, concat_train, concat_label, x_test, 1)\n",
    "cnn_train2, cnn_test2 = build_cnn(5, concat_train, concat_label, x_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submssion = pd.read_csv(path + 'sample_submission.csv')\n",
    "sample_submssion.iloc[:,1:] = cnn_test1\n",
    "sample_submssion.to_csv(\"lstm.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ML 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_train = np.array([np.max(concat_train, axis=1),np.min(concat_train, axis=1),np.mean(concat_train, axis=1)]).reshape(-1, 18)\n",
    "ml_test = np.array([np.max(x_test, axis=1),np.min(x_test, axis=1),np.mean(x_test, axis=1)]).reshape(-1, 18)\n",
    "ml_label = np.concatenate((np.array(y_train).reshape(-1,1), shift_label), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35265, 18), (782, 18))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_train.shape, ml_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "# import catboost as cb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:3.99914\tvalid-mlogloss:4.02074\n",
      "[100]\ttrain-mlogloss:0.92431\tvalid-mlogloss:1.28733\n",
      "[200]\ttrain-mlogloss:0.48641\tvalid-mlogloss:0.92702\n",
      "[300]\ttrain-mlogloss:0.32236\tvalid-mlogloss:0.86104\n",
      "[375]\ttrain-mlogloss:0.26589\tvalid-mlogloss:0.86279\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:4.00358\tvalid-mlogloss:4.02379\n",
      "[100]\ttrain-mlogloss:0.92590\tvalid-mlogloss:1.29619\n",
      "[200]\ttrain-mlogloss:0.48396\tvalid-mlogloss:0.93957\n",
      "[300]\ttrain-mlogloss:0.32275\tvalid-mlogloss:0.87711\n",
      "[382]\ttrain-mlogloss:0.26316\tvalid-mlogloss:0.88144\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.99327\tvalid-mlogloss:4.01504\n",
      "[100]\ttrain-mlogloss:0.92744\tvalid-mlogloss:1.31394\n",
      "[200]\ttrain-mlogloss:0.48634\tvalid-mlogloss:0.95626\n",
      "[300]\ttrain-mlogloss:0.32201\tvalid-mlogloss:0.89343\n",
      "[367]\ttrain-mlogloss:0.26995\tvalid-mlogloss:0.89692\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:4.00005\tvalid-mlogloss:4.02098\n",
      "[100]\ttrain-mlogloss:0.91342\tvalid-mlogloss:1.27217\n",
      "[200]\ttrain-mlogloss:0.48506\tvalid-mlogloss:0.91946\n",
      "[300]\ttrain-mlogloss:0.32203\tvalid-mlogloss:0.85436\n",
      "[372]\ttrain-mlogloss:0.26815\tvalid-mlogloss:0.85719\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.99932\tvalid-mlogloss:4.02117\n",
      "[100]\ttrain-mlogloss:0.93096\tvalid-mlogloss:1.25811\n",
      "[200]\ttrain-mlogloss:0.49084\tvalid-mlogloss:0.88236\n",
      "[300]\ttrain-mlogloss:0.32586\tvalid-mlogloss:0.80619\n",
      "[400]\ttrain-mlogloss:0.25731\tvalid-mlogloss:0.80600\n",
      "[402]\ttrain-mlogloss:0.25647\tvalid-mlogloss:0.80619\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:4.00089\tvalid-mlogloss:4.02180\n",
      "[100]\ttrain-mlogloss:0.91406\tvalid-mlogloss:1.27855\n",
      "[200]\ttrain-mlogloss:0.48331\tvalid-mlogloss:0.92847\n",
      "[300]\ttrain-mlogloss:0.32229\tvalid-mlogloss:0.86553\n",
      "[368]\ttrain-mlogloss:0.26998\tvalid-mlogloss:0.86755\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:4.00561\tvalid-mlogloss:4.02618\n",
      "[100]\ttrain-mlogloss:0.92295\tvalid-mlogloss:1.31058\n",
      "[200]\ttrain-mlogloss:0.48256\tvalid-mlogloss:0.94970\n",
      "[300]\ttrain-mlogloss:0.31910\tvalid-mlogloss:0.88550\n",
      "[370]\ttrain-mlogloss:0.26619\tvalid-mlogloss:0.88832\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:4.00444\tvalid-mlogloss:4.02481\n",
      "[100]\ttrain-mlogloss:0.92244\tvalid-mlogloss:1.27430\n",
      "[200]\ttrain-mlogloss:0.48734\tvalid-mlogloss:0.92063\n",
      "[300]\ttrain-mlogloss:0.32262\tvalid-mlogloss:0.85540\n",
      "[373]\ttrain-mlogloss:0.26758\tvalid-mlogloss:0.85845\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:4.00347\tvalid-mlogloss:4.02409\n",
      "[100]\ttrain-mlogloss:0.93041\tvalid-mlogloss:1.25457\n",
      "[200]\ttrain-mlogloss:0.49139\tvalid-mlogloss:0.88504\n",
      "[300]\ttrain-mlogloss:0.32679\tvalid-mlogloss:0.81139\n",
      "[393]\ttrain-mlogloss:0.26190\tvalid-mlogloss:0.81225\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.99975\tvalid-mlogloss:4.02111\n",
      "[100]\ttrain-mlogloss:0.92290\tvalid-mlogloss:1.29852\n",
      "[200]\ttrain-mlogloss:0.48372\tvalid-mlogloss:0.93753\n",
      "[300]\ttrain-mlogloss:0.32186\tvalid-mlogloss:0.87483\n",
      "[368]\ttrain-mlogloss:0.26994\tvalid-mlogloss:0.87580\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "# 모델 2번: xgboost\n",
    "\n",
    "def build_xgboost(split_num, train, target, test, rnd):\n",
    "    \n",
    "    params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 12,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':61,\n",
    "                }\n",
    "    \n",
    "    # return train pred prob and test pred prob \n",
    "    train_pred, test_pred = np.zeros((train.shape[0], 61)), np.zeros((test.shape[0], 61))\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=233*rnd)\n",
    "    for train_idx, val_idx in skf.split(train, target):\n",
    "\n",
    "        # split train, validation set\n",
    "        X = train[train_idx]\n",
    "        y = target[train_idx]\n",
    "        valid_x = train[val_idx]\n",
    "        valid_y = target[val_idx]\n",
    "\n",
    "        d_train = xgb.DMatrix(X, y)\n",
    "        d_valid = xgb.DMatrix(valid_x, valid_y)\n",
    "        d_temp = xgb.DMatrix(valid_x)\n",
    "        d_test = xgb.DMatrix(test)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        \n",
    "        #run traning\n",
    "        model = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=100)\n",
    "\n",
    "        # save feat\n",
    "        train_pred[val_idx] = model.predict(d_temp)\n",
    "        test_pred += model.predict(d_test)/split_num\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "        \n",
    "    return train_pred, test_pred\n",
    "\n",
    "xgb_train1, xgb_test1 = build_xgboost(5, ml_train, ml_label, ml_test, 1)\n",
    "xgb_train2, xgb_test2 = build_xgboost(5, ml_train, ml_label, ml_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 모델 2번: catboost\n",
    "\n",
    "# def build_catboost(split_num, train, target, test, rnd):\n",
    "#     # return train pred prob and test pred prob \n",
    "#     train_pred, test_pred = np.zeros((train.shape[0], 61)), np.zeros((test.shape[0], 61))\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=233*rnd)\n",
    "#     for train_idx, val_idx in skf.split(train, target):\n",
    "\n",
    "#         # split train, validation set\n",
    "#         X = train.iloc[train_idx]\n",
    "#         y = target.iloc[train_idx]\n",
    "#         valid_x = train.iloc[val_idx]\n",
    "#         valid_y = target.iloc[val_idx]\n",
    "\n",
    "#         model = cb.CatBoostClassifier(iterations=1500,\n",
    "#                                       learning_rate=0.01,\n",
    "#                                       l2_leaf_reg=3.5,\n",
    "#                                       depth=8,\n",
    "#                                       rsm=0.98,\n",
    "#                                       loss_function= 'MultiClass',\n",
    "#                                       eval_metric='AUC',\n",
    "#                                       use_best_model=True,\n",
    "#                                       random_seed=42,\n",
    "#                                       verbose=50)\n",
    "\n",
    "#         model.fit(X, y,\n",
    "#                   eval_set=(valid_x, valid_y),\n",
    "#                   early_stopping_rounds=30)\n",
    "        \n",
    "#         # save feat\n",
    "#         train_pred[val_idx] = model.predict(valid_x)\n",
    "#         test_pred += model.predict(test)/split_num\n",
    "        \n",
    "#         # release\n",
    "#         del model\n",
    "#         gc.collect()\n",
    "#         print('------------------')\n",
    "        \n",
    "#     return train_pred, test_pred\n",
    "\n",
    "# catboost_train1, catboost_test1 = build_catboost(5, ml_train, ml_label, ml_test, 1)\n",
    "# catboost_train2, catboost_test2 = build_catboost(5, ml_train, ml_label, ml_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array(train.iloc[:,2:]).reshape(-1, 600, 6, 1)\n",
    "# y_train = tf.keras.utils.to_categorical(train_label['label'])\n",
    "# x_test = np.array(test.iloc[:,2:]).reshape(-1, 600, 6, 1)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "# print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 5번: transformer\n",
    "# # https://www.tensorflow.org/tutorials/text/transformer\n",
    "\n",
    "# class Transformer(tf.keras.Model):\n",
    "#     def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "#                target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "#         super(Transformer, self).__init__()\n",
    "\n",
    "#         self.tokenizer = Encoder(num_layers, d_model, num_heads, dff, \n",
    "#                                input_vocab_size, pe_input, rate)\n",
    "\n",
    "#         self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "#                                target_vocab_size, pe_target, rate)\n",
    "\n",
    "#         self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "#     def call(self, inp, tar, training, enc_padding_mask, \n",
    "#            look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "#         enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "#         # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "#         dec_output, attention_weights = self.decoder(\n",
    "#             tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "#         final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "#         return final_output, attention_weights\n",
    "\n",
    "\n",
    "# def build_transformer(split_num, train, target, test, rnd):\n",
    "#     # return train pred prob and test pred prob \n",
    "#     train_pred, test_pred = np.zeros((train.shape[0], 1)), np.zeros((test.shape[0], 1))\n",
    "\n",
    "#     skf = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=233*rnd)\n",
    "#     for train_idx, val_idx in skf.split(train, target):\n",
    "\n",
    "#         # split train, validation set\n",
    "#         X = train[train_idx]\n",
    "#         y = target[train_idx]\n",
    "#         valid_x = train[val_idx]\n",
    "#         valid_y = target[val_idx]\n",
    "\n",
    "#         #가벼운 모델 생성\n",
    "#         model = Sequential()\n",
    "#         model.add(LSTM(32, input_shape=(600,6)))\n",
    "#         model.add(Dense(128, activation='relu'))\n",
    "#         model.add(Dense(61, activation='softmax'))\n",
    "\n",
    "#         model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#         model.fit(X,y, epochs=30, batch_size=128, validation_split=0.2)\n",
    "        \n",
    "#         # save feat\n",
    "#         submission.iloc[:,1:]=model.predict(test_X)\n",
    "#         train_pred[val_idx] = model.predict(valid_x).reshape(-1,1)\n",
    "#         test_pred += model.predict(test).reshape(-1,1)/split_num\n",
    "        \n",
    "#         # release\n",
    "#         del model\n",
    "#         gc.collect()\n",
    "#         print('------------------')\n",
    "        \n",
    "#     return train_pred, test_pred\n",
    "\n",
    "# transformer_train1, transformer_test1 = build_transformer(5, train, train_y, test, 1)\n",
    "# transformer_train2, transformer_test2 = build_transformer(5, train, train_y, test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35265, 4)\n",
      "(782, 4)\n"
     ]
    }
   ],
   "source": [
    "xgb1_onehot = np.argmax(xgb_train1, axis=1).reshape(-1,1)\n",
    "xgb2_onehot = np.argmax(xgb_train2, axis=1).reshape(-1,1)\n",
    "# catboost1_onehot = np.argmax(catboost_train1, axis=1).reshape(-1,1)\n",
    "# catboost2_onehot = np.argmax(catboost_train2, axis=1).reshape(-1,1)\n",
    "cnn1_onehot = np.argmax(cnn_train1, axis=1).reshape(-1,1)\n",
    "cnn2_onehot = np.argmax(cnn_train2, axis=1).reshape(-1,1)\n",
    "\n",
    "xgb1_onehot_test = np.argmax(xgb_test1, axis=1).reshape(-1,1)\n",
    "xgb2_onehot_test = np.argmax(xgb_test2, axis=1).reshape(-1,1)\n",
    "# catboost1_onehot_test = np.argmax(catboost_test1, axis=1).reshape(-1,1)\n",
    "# catboost2_onehot_test = np.argmax(catboost_test2, axis=1).reshape(-1,1)\n",
    "cnn1_onehot_test = np.argmax(cnn_test1, axis=1).reshape(-1,1)\n",
    "cnn2_onehot_test = np.argmax(cnn_test2, axis=1).reshape(-1,1)\n",
    "\n",
    "train_final = np.hstack([xgb1_onehot, xgb2_onehot,\n",
    "#                          catboost1_onehot, catboost2_onehot,\n",
    "#                          lstm_train1, lstm_train2,\n",
    "                         cnn1_onehot, cnn2_onehot])\n",
    "\n",
    "test_final = np.hstack([xgb1_onehot_test, xgb2_onehot_test,\n",
    "#                         catboost1_onehot_test, catboost2_onehot_test,\n",
    "#                         lstm_test1, lstm_test2,\n",
    "                        cnn1_onehot_test, cnn2_onehot_test])\n",
    "\n",
    "print(train_final.shape)\n",
    "print(test_final.shape)\n",
    "\n",
    "# https://m.blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221343373342&proxyReferer=https:%2F%2Fwww.google.com%2F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:3.11181\tvalid-mlogloss:3.11558\n",
      "[100]\ttrain-mlogloss:0.06595\tvalid-mlogloss:0.07497\n",
      "[200]\ttrain-mlogloss:0.01428\tvalid-mlogloss:0.02389\n",
      "[300]\ttrain-mlogloss:0.00947\tvalid-mlogloss:0.01981\n",
      "[400]\ttrain-mlogloss:0.00808\tvalid-mlogloss:0.01932\n",
      "[443]\ttrain-mlogloss:0.00772\tvalid-mlogloss:0.01941\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.11146\tvalid-mlogloss:3.11385\n",
      "[100]\ttrain-mlogloss:0.06503\tvalid-mlogloss:0.07740\n",
      "[200]\ttrain-mlogloss:0.01321\tvalid-mlogloss:0.02952\n",
      "[300]\ttrain-mlogloss:0.00839\tvalid-mlogloss:0.02742\n",
      "[369]\ttrain-mlogloss:0.00734\tvalid-mlogloss:0.02750\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.11236\tvalid-mlogloss:3.11177\n",
      "[100]\ttrain-mlogloss:0.06559\tvalid-mlogloss:0.07726\n",
      "[200]\ttrain-mlogloss:0.01396\tvalid-mlogloss:0.02691\n",
      "[300]\ttrain-mlogloss:0.00917\tvalid-mlogloss:0.02361\n",
      "[400]\ttrain-mlogloss:0.00779\tvalid-mlogloss:0.02346\n",
      "[428]\ttrain-mlogloss:0.00756\tvalid-mlogloss:0.02348\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.11195\tvalid-mlogloss:3.11244\n",
      "[100]\ttrain-mlogloss:0.06538\tvalid-mlogloss:0.07631\n",
      "[200]\ttrain-mlogloss:0.01372\tvalid-mlogloss:0.02683\n",
      "[300]\ttrain-mlogloss:0.00894\tvalid-mlogloss:0.02385\n",
      "[400]\ttrain-mlogloss:0.00757\tvalid-mlogloss:0.02388\n",
      "[411]\ttrain-mlogloss:0.00747\tvalid-mlogloss:0.02389\n",
      "------------------\n",
      "[0]\ttrain-mlogloss:3.11175\tvalid-mlogloss:3.11092\n",
      "[100]\ttrain-mlogloss:0.06572\tvalid-mlogloss:0.07326\n",
      "[200]\ttrain-mlogloss:0.01389\tvalid-mlogloss:0.02479\n",
      "[300]\ttrain-mlogloss:0.00897\tvalid-mlogloss:0.02213\n",
      "[400]\ttrain-mlogloss:0.00754\tvalid-mlogloss:0.02212\n",
      "[408]\ttrain-mlogloss:0.00747\tvalid-mlogloss:0.02214\n",
      "------------------\n",
      "       id         0         1             2             3         4         5  \\\n",
      "0    3125  0.000102  0.000080  5.581095e-05  7.539787e-05  0.000096  0.000383   \n",
      "1    3126  0.000001  0.000001  7.670320e-07  9.991997e-07  0.000001  0.000002   \n",
      "2    3127  0.000001  0.000001  7.670320e-07  9.991997e-07  0.000001  0.000002   \n",
      "3    3128  0.000001  0.000001  7.670320e-07  9.991997e-07  0.000001  0.000002   \n",
      "4    3129  0.000001  0.000001  7.670320e-07  9.991997e-07  0.000001  0.000002   \n",
      "..    ...       ...       ...           ...           ...       ...       ...   \n",
      "777  3902  0.000032  0.000009  7.993224e-06  1.423789e-05  0.000013  0.000010   \n",
      "778  3903  0.000049  0.000035  3.087519e-05  3.315739e-05  0.000034  0.000096   \n",
      "779  3904  0.000005  0.000003  2.763532e-06  2.897406e-06  0.000005  0.000004   \n",
      "780  3905  0.000032  0.000009  7.993224e-06  1.423789e-05  0.000013  0.000010   \n",
      "781  3906  0.000200  0.000153  1.164117e-04  1.354405e-04  0.000170  0.000559   \n",
      "\n",
      "            6         7         8  ...        51        52            53  \\\n",
      "0    0.000082  0.000129  0.000084  ...  0.000126  0.000098  6.991751e-05   \n",
      "1    0.000001  0.000001  0.000002  ...  0.000002  0.000001  9.994918e-07   \n",
      "2    0.000001  0.000001  0.000002  ...  0.000002  0.000001  9.994918e-07   \n",
      "3    0.000001  0.000001  0.000002  ...  0.000002  0.000001  9.994918e-07   \n",
      "4    0.000001  0.000001  0.000002  ...  0.000002  0.000001  9.994918e-07   \n",
      "..        ...       ...       ...  ...       ...       ...           ...   \n",
      "777  0.000011  0.000009  0.000595  ...  0.000021  0.000014  9.602304e-06   \n",
      "778  0.000029  0.000036  0.000038  ...  0.000029  0.000047  3.328611e-05   \n",
      "779  0.000005  0.000003  0.000007  ...  0.000004  0.000004  3.104701e-06   \n",
      "780  0.000011  0.000009  0.000595  ...  0.000021  0.000014  9.602304e-06   \n",
      "781  0.000121  0.000148  0.000110  ...  0.000122  0.000191  1.440255e-04   \n",
      "\n",
      "           54        55        56        57        58        59        60  \n",
      "0    0.000091  0.000114  0.000065  0.000074  0.000105  0.000102  0.000114  \n",
      "1    0.000001  0.000004  0.000001  0.000001  0.000001  0.000001  0.000002  \n",
      "2    0.000001  0.000004  0.000001  0.000001  0.000001  0.000001  0.000002  \n",
      "3    0.000001  0.000004  0.000001  0.000001  0.000001  0.000001  0.000002  \n",
      "4    0.000001  0.000004  0.000001  0.000001  0.000001  0.000001  0.000002  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "777  0.000024  0.000022  0.000052  0.000049  0.000013  0.000018  0.000021  \n",
      "778  0.000045  0.000149  0.000043  0.000027  0.000043  0.000044  0.000050  \n",
      "779  0.000007  0.000012  0.000003  0.000003  0.000004  0.000004  0.000006  \n",
      "780  0.000024  0.000022  0.000052  0.000049  0.000013  0.000018  0.000021  \n",
      "781  0.000154  0.000387  0.000115  0.000113  0.000193  0.000168  0.000216  \n",
      "\n",
      "[782 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "# 최종 앙상블\n",
    "\n",
    "def ensemble_xgb(split_num, train, target, test):\n",
    "\n",
    "    test_pred = np.zeros((test.shape[0], 61))\n",
    "    \n",
    "    params = {\n",
    "                'colsample_bytree': 0.7,\n",
    "                'subsample': 0.8,\n",
    "                'eta': 0.04,\n",
    "                'max_depth': 12,\n",
    "                'eval_metric':'mlogloss',\n",
    "                'objective':'multi:softprob',\n",
    "                'num_class':61,\n",
    "                }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=2021)\n",
    "    for train_idx, val_idx in skf.split(train, target):\n",
    "\n",
    "        # split train, validation set\n",
    "        X = train[train_idx]\n",
    "        y = target[train_idx]\n",
    "        valid_x = train[val_idx]\n",
    "        valid_y = target[val_idx]\n",
    "        \n",
    "        d_train = xgb.DMatrix(X, y)\n",
    "        d_valid = xgb.DMatrix(valid_x, valid_y)\n",
    "        d_test = xgb.DMatrix(test)\n",
    "        \n",
    "        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "        \n",
    "        #run traning\n",
    "        model = xgb.train(params, d_train, 2000, watchlist, \n",
    "                        early_stopping_rounds=50,\n",
    "                        verbose_eval=100)\n",
    "\n",
    "        # save feat\n",
    "        test_pred += model.predict(d_test)/split_num\n",
    "        \n",
    "        # release\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('------------------')\n",
    "    \n",
    "    sample_submssion = pd.read_csv(path + 'sample_submission.csv')\n",
    "    sample_submssion.iloc[:,1:] = test_pred\n",
    "    sample_submssion.to_csv(\"ensemble.csv\", index = False)\n",
    "    \n",
    "    print(sample_submssion)\n",
    "    \n",
    "ensemble_xgb(5, train_final, ml_label, test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
