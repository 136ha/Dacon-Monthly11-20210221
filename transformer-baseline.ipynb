{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"papermill":{"duration":654.273766,"end_time":"2020-12-19T21:56:32.593054","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-12-19T21:45:38.319288","version":"2.1.0"},"colab":{"name":"transformer-baseline.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PFBBlwXwJCOr","executionInfo":{"status":"ok","timestamp":1613358068025,"user_tz":-540,"elapsed":3978,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"5b10de35-d293-412c-e26a-10df6c705286"},"source":["!pip install iterative-stratification"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting iterative-stratification\n","  Downloading https://files.pythonhosted.org/packages/9d/79/9ba64c8c07b07b8b45d80725b2ebd7b7884701c1da34f70d4749f7b45f9a/iterative_stratification-0.1.6-py3-none-any.whl\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (0.22.2.post1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from iterative-stratification) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->iterative-stratification) (1.0.0)\n","Installing collected packages: iterative-stratification\n","Successfully installed iterative-stratification-0.1.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_Pc0n7jwQSq","executionInfo":{"status":"ok","timestamp":1613358085211,"user_tz":-540,"elapsed":21159,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"fd4a6d55-3a38-4342-92a3-5b49822d205c"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7bJ70Ws1wSGD","executionInfo":{"status":"ok","timestamp":1613358085211,"user_tz":-540,"elapsed":21158,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["#경로 설정\r\n","import os\r\n","os.chdir('/content/drive/My Drive/Colab Notebooks/운동동작분류AI경진대회')"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2020-12-19T21:45:42.915933Z","iopub.status.busy":"2020-12-19T21:45:42.915215Z","iopub.status.idle":"2020-12-19T21:45:55.422401Z","shell.execute_reply":"2020-12-19T21:45:55.423089Z"},"papermill":{"duration":12.539859,"end_time":"2020-12-19T21:45:55.423292","exception":false,"start_time":"2020-12-19T21:45:42.883433","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"9bbPFoUlwP3L","executionInfo":{"status":"ok","timestamp":1613358087332,"user_tz":-540,"elapsed":23275,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"397e496d-2708-4807-d90e-9d3f29db05bb"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","import tensorflow.keras.backend as K\n","import tensorflow.keras.layers as layers\n","from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","\n","import os, gc, random, datetime\n","import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from joblib import dump, load\n","from time import time\n","import scipy as sp\n","import scipy.fftpack\n","\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","\n","print(\"Tensorflow version \" + tf.__version__)\n","AUTO = tf.data.experimental.AUTOTUNE"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Tensorflow version 2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.021446,"end_time":"2020-12-19T21:45:55.576052","exception":false,"start_time":"2020-12-19T21:45:55.554606","status":"completed"},"tags":[],"id":"dC9e5tXuwP3M"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"A77rJfaswP3M","executionInfo":{"status":"ok","timestamp":1613358097156,"user_tz":-540,"elapsed":33098,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# 데이터 불러오기\n","\n","path = './data/'\n","train = pd.read_csv(path + 'train_features.csv')\n","train_label = pd.read_csv(path + 'train_labels.csv')\n","test = pd.read_csv(path + 'test_features.csv')\n","submission = pd.read_csv(path + 'sample_submission.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4bg_yTDwP3M","executionInfo":{"status":"ok","timestamp":1613359084820,"user_tz":-540,"elapsed":369763,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# Pre-Processing Effect on the Accuracy of Event-Based Activity Segmentation and Classification through Inertial Sensors \n","# https://www.researchgate.net/publication/281836367_Pre-Processing_Effect_on_the_Accuracy_of_Event-Based_Activity_Segmentation_and_Classification_through_Inertial_Sensors\n","\n","train['acc_t']  = train.apply(lambda x : (x['acc_x']**2 + x['acc_y'] **2 +  x['acc_z'] **2)**(1/2), axis=1)\n","test['acc_t']  = test.apply(lambda x : (x['acc_x']**2 + x['acc_y'] **2 +  x['acc_z'] **2)**(1/2), axis=1)\n","train['gy_t']  = train.apply(lambda x : (x['gy_x']**2 + x['gy_y'] **2 +  x['gy_z'] **2)**(1/2), axis=1)\n","test['gy_t']  = test.apply(lambda x : (x['gy_x']**2 + x['gy_y'] **2 +  x['gy_z'] **2)**(1/2), axis=1)\n","\n","# SVM selected features\n","train['mean'] = train[['acc_x','acc_y']].mean(axis=1)\n","train['median'] = train[['acc_y', 'gy_z', 'gy_t']].median(axis=1)\n","train['standard_deviation'] = train[['acc_x', 'acc_y']].std(axis=1)\n","train['interquartile'] = train.quantile(.75, axis=1) - train.quantile(.25, axis=1)\n","train['FFT_acc_x'] = sp.fftpack.fft(np.array(train['acc_x']))\n","train['FFT_acc_y'] = sp.fftpack.fft(np.array(train['acc_y']))\n","train['FFT_acc_t'] = sp.fftpack.fft(np.array(train['acc_t']))\n","train['FFT_gy_z'] = sp.fftpack.fft(np.array(train['gy_z']))\n","\n","test['mean'] = test[['acc_x','acc_y']].mean(axis=1)\n","test['median'] = test[['acc_y', 'gy_z', 'gy_t']].median(axis=1)\n","test['standard_deviation'] = test[['acc_x', 'acc_y']].std(axis=1)\n","test['interquartile'] = test.quantile(.75, axis=1) - test.quantile(.25, axis=1)\n","test['FFT_acc_x'] = sp.fftpack.fft(np.array(test['acc_x']))\n","test['FFT_acc_y'] = sp.fftpack.fft(np.array(test['acc_y']))\n","test['FFT_acc_t'] = sp.fftpack.fft(np.array(test['acc_t']))\n","test['FFT_gy_z'] = sp.fftpack.fft(np.array(test['gy_z']))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHMRT0CZwP3N","executionInfo":{"status":"ok","timestamp":1613359142864,"user_tz":-540,"elapsed":1256,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["x = np.array(train.iloc[:,2:]).reshape(-1, 600, 16)\n","y = tf.keras.utils.to_categorical(train_label['label'])\n","test = np.array(test.iloc[:,2:]).reshape(-1, 600, 16)"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.034259,"end_time":"2020-12-19T21:47:05.668897","exception":false,"start_time":"2020-12-19T21:47:05.634638","status":"completed"},"tags":[],"id":"ruDx0aTTwP3N"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033446,"end_time":"2020-12-19T21:47:05.735558","exception":false,"start_time":"2020-12-19T21:47:05.702112","status":"completed"},"tags":[],"id":"jJCWdNkWwP3N"},"source":["Base Transformer structure from https://www.tensorflow.org/tutorials/text/transformer, modified with Swish activation function."]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2020-12-19T21:47:05.822914Z","iopub.status.busy":"2020-12-19T21:47:05.821803Z","iopub.status.idle":"2020-12-19T21:47:05.866016Z","shell.execute_reply":"2020-12-19T21:47:05.865400Z"},"papermill":{"duration":0.09191,"end_time":"2020-12-19T21:47:05.866129","exception":false,"start_time":"2020-12-19T21:47:05.774219","status":"completed"},"tags":[],"id":"b7k4RMZhwP3N","executionInfo":{"status":"ok","timestamp":1613359145566,"user_tz":-540,"elapsed":797,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Calculate the attention weights.\n","    q, k, v must have matching leading dimensions.\n","    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","    The mask has different shapes depending on its type(padding or look ahead) \n","    but it must be broadcastable for addition.\n","\n","    Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","    Returns:\n","    output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n","\n","    # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # add the mask to the scaled tensor.\n","    if mask is not None:\n","        \n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    # softmax is normalized on the last axis (seq_len_k) so that the scores\n","    # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    \n","    def __init__(self, d_model, num_heads):\n","        \n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm = [0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        \n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","        return output, attention_weights\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","    \n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation = 'relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","    ])\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n","        \n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output, training = training)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output, training = training)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n","\n","class TransformerEncoder(tf.keras.layers.Layer):\n","    \n","    def __init__(self, num_layers, d_model, num_heads, dff, \n","                 maximum_position_encoding, rate = 0.1):\n","        \n","        super(TransformerEncoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.num_heads = num_heads\n","        self.dff = dff\n","        self.maximum_position_encoding = maximum_position_encoding\n","        self.rate = rate\n","\n","#         self.pos_encoding = positional_encoding(self.maximum_position_encoding, \n","#                                                 self.d_model)\n","#         self.embedding = tf.keras.layers.Dense(self.d_model)\n","        self.pos_emb = tf.keras.layers.Embedding(input_dim = self.maximum_position_encoding, \n","                                                 output_dim = self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate) \n","                           for _ in range(self.num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(self.rate)\n","        \n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'num_layers': self.num_layers,\n","            'd_model': self.d_model,\n","            'num_heads': self.num_heads,\n","            'dff': self.dff,\n","            'maximum_position_encoding': self.maximum_position_encoding,\n","            'dropout': self.dropout,\n","        })\n","        return config\n","\n","    def call(self, x, training, mask = None):\n","\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","#         x += self.pos_encoding[:, :seq_len, :]\n","#         x = self.embedding(x)\n","        positions = tf.range(start = 0, limit = seq_len, delta = 1)\n","        x += self.pos_emb(positions)\n","\n","        x = self.dropout(x, training = training)\n","\n","        for i in range(self.num_layers):\n","\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x  # (batch_size, input_seq_len, d_model)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:47:05.924111Z","iopub.status.busy":"2020-12-19T21:47:05.923351Z","iopub.status.idle":"2020-12-19T21:47:05.927506Z","shell.execute_reply":"2020-12-19T21:47:05.927026Z"},"papermill":{"duration":0.038039,"end_time":"2020-12-19T21:47:05.927604","exception":false,"start_time":"2020-12-19T21:47:05.889565","status":"completed"},"tags":[],"id":"mOCptCFxwP3P","executionInfo":{"status":"ok","timestamp":1613359147116,"user_tz":-540,"elapsed":613,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["def create_transformer_model(num_columns, num_labels, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate):\n","    \n","    inp = tf.keras.layers.Input(shape = (window_size, num_columns))\n","    x = tf.keras.layers.BatchNormalization()(inp)\n","    x = tf.keras.layers.Dense(d_model)(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","    x = tf.keras.layers.SpatialDropout1D(dropout_rate)(x)\n","    x = TransformerEncoder(num_layers, d_model, num_heads, dff, window_size, dropout_rate)(x)\n","    out = tf.keras.layers.Dense(num_labels, activation = 'softmax')(x[:, -1, :])\n","    \n","    model = tf.keras.models.Model(inputs = inp, outputs = out)\n","    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['AUC'])\n","    \n","    return model"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:47:06.026885Z","iopub.status.busy":"2020-12-19T21:47:06.025060Z","iopub.status.idle":"2020-12-19T21:47:06.027550Z","shell.execute_reply":"2020-12-19T21:47:06.028046Z"},"papermill":{"duration":0.031196,"end_time":"2020-12-19T21:47:06.028159","exception":false,"start_time":"2020-12-19T21:47:05.996963","status":"completed"},"tags":[],"id":"M8HPyemgwP3R","executionInfo":{"status":"ok","timestamp":1613359148406,"user_tz":-540,"elapsed":601,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["batch_size = 64\n","num_layers = 1\n","d_model = 128\n","num_heads = 1\n","dff = 128\n","window_size = 600\n","dropout_rate = 0.15\n","weight_decay = 0\n","label_smoothing = 1e-2\n","learning_rate = 1e-3\n","verbose = 1"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.023272,"end_time":"2020-12-19T21:47:09.916436","exception":false,"start_time":"2020-12-19T21:47:09.893164","status":"completed"},"tags":[],"id":"CPgqEvivwP3R"},"source":["# Train-Test-Split Training\n","\n","Split the train set into three folds, i.e., training-1, training-2 and validation sets. First, train the more on training-1 set and validate it on the validation set. Then use the training-2 set to find the best number of finetuning epochs. Finally, finetune on both training-2 and validation sets and submit."]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:47:12.495194Z","iopub.status.busy":"2020-12-19T21:47:12.494436Z","iopub.status.idle":"2020-12-19T21:48:42.358496Z","shell.execute_reply":"2020-12-19T21:48:42.357889Z"},"papermill":{"duration":89.899584,"end_time":"2020-12-19T21:48:42.358608","exception":false,"start_time":"2020-12-19T21:47:12.459024","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"q5wj-CmawP3R","executionInfo":{"status":"ok","timestamp":1613359560384,"user_tz":-540,"elapsed":407939,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"75feffe9-ae47-4b00-8d0d-9f887a27c00a"},"source":["# 데이터 증강\n","def aug(data, shift):\n","    shift_data = np.roll(data, shift, axis=2)\n","    return shift_data\n","\n","# 모델 1번: Transformer\n","\n","def build_transformer(split_num, train, target, test, rnd):\n","    start_time_fold = time()\n","    # return train pred prob and test pred prob \n","    test_pred = np.zeros((test.shape[0], 61))\n","\n","    ckp_path = 'JSTransformer.hdf5'\n","\n","    rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, verbose = verbose, min_delta = 1e-4, mode = 'min')\n","    ckp = ModelCheckpoint(ckp_path, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n","    es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 4, mode = 'min', baseline = None, restore_best_weights = True, verbose = 0)\n","\n","    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    for train_idx, val_idx in mskf.split(train, target):\n","\n","        # split train, validation set\n","        X = train[train_idx]\n","        y = target[train_idx]\n","        valid_x = train[val_idx]\n","        valid_y = target[val_idx]\n","\n","        #가벼운 모델 생성\n","        model = create_transformer_model(train.shape[2], 61, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\n","\n","        model.fit(X, y, epochs = 100,\n","                  validation_data = (valid_x, valid_y),\n","                  batch_size = batch_size,\n","                  callbacks = [rlr, ckp, es],\n","                  verbose = verbose)\n","        \n","        # save feat\n","        model.load_weights(ckp_path)\n","        test_pred += model.predict(test)/split_num\n","        \n","        # release\n","        del model\n","        gc.collect()\n","        print('  ==============================================================================================  ')\n","\n","        \n","    return test_pred\n","\n","transformer_test = build_transformer(5, x, y, test, 1)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","40/40 [==============================] - 6s 69ms/step - loss: 3.3560 - auc: 0.7219 - val_loss: 2.4630 - val_auc: 0.8711\n","Epoch 2/100\n","40/40 [==============================] - 2s 55ms/step - loss: 2.3536 - auc: 0.8832 - val_loss: 2.0221 - val_auc: 0.9270\n","Epoch 3/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.9773 - auc: 0.9337 - val_loss: 1.8119 - val_auc: 0.9478\n","Epoch 4/100\n","40/40 [==============================] - 2s 55ms/step - loss: 1.7869 - auc: 0.9487 - val_loss: 1.6703 - val_auc: 0.9561\n","Epoch 5/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.7029 - auc: 0.9546 - val_loss: 1.5715 - val_auc: 0.9574\n","Epoch 6/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.6097 - auc: 0.9601 - val_loss: 1.4564 - val_auc: 0.9642\n","Epoch 7/100\n","40/40 [==============================] - 2s 55ms/step - loss: 1.5825 - auc: 0.9611 - val_loss: 1.4729 - val_auc: 0.9637\n","Epoch 8/100\n","40/40 [==============================] - 2s 55ms/step - loss: 1.4441 - auc: 0.9689 - val_loss: 1.3556 - val_auc: 0.9680\n","Epoch 9/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.4527 - auc: 0.9674 - val_loss: 1.3397 - val_auc: 0.9667\n","Epoch 10/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.4010 - auc: 0.9680 - val_loss: 1.2817 - val_auc: 0.9702\n","Epoch 11/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.3976 - auc: 0.9722 - val_loss: 1.2767 - val_auc: 0.9680\n","Epoch 12/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.3349 - auc: 0.9721 - val_loss: 1.2506 - val_auc: 0.9687\n","Epoch 13/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.2539 - auc: 0.9781 - val_loss: 1.2124 - val_auc: 0.9751\n","Epoch 14/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.2251 - auc: 0.9795 - val_loss: 1.1688 - val_auc: 0.9751\n","Epoch 15/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.2167 - auc: 0.9778 - val_loss: 1.1724 - val_auc: 0.9738\n","Epoch 16/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.1662 - auc: 0.9794 - val_loss: 1.1790 - val_auc: 0.9705\n","Epoch 17/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.1410 - auc: 0.9788 - val_loss: 1.1583 - val_auc: 0.9718\n","Epoch 18/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.0452 - auc: 0.9821 - val_loss: 1.1850 - val_auc: 0.9730\n","Epoch 19/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.0687 - auc: 0.9839 - val_loss: 1.1408 - val_auc: 0.9720\n","Epoch 20/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.0664 - auc: 0.9838 - val_loss: 1.1253 - val_auc: 0.9721\n","Epoch 21/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.0182 - auc: 0.9847 - val_loss: 1.1089 - val_auc: 0.9719\n","Epoch 22/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.9968 - auc: 0.9863 - val_loss: 1.1363 - val_auc: 0.9698\n","Epoch 23/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.9410 - auc: 0.9847 - val_loss: 1.0835 - val_auc: 0.9732\n","Epoch 24/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.9065 - auc: 0.9879 - val_loss: 1.1553 - val_auc: 0.9650\n","Epoch 25/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.9257 - auc: 0.9859 - val_loss: 1.0728 - val_auc: 0.9731\n","Epoch 26/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.8783 - auc: 0.9858 - val_loss: 1.1528 - val_auc: 0.9706\n","Epoch 27/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9166 - auc: 0.9853 - val_loss: 1.1127 - val_auc: 0.9698\n","Epoch 28/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9225 - auc: 0.9826 - val_loss: 1.1214 - val_auc: 0.9720\n","\n","Epoch 00028: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 29/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.7684 - auc: 0.9906 - val_loss: 1.0940 - val_auc: 0.9744\n","  ==============================================================================================  \n","Epoch 1/100\n","40/40 [==============================] - 4s 69ms/step - loss: 3.1594 - auc: 0.7728 - val_loss: 2.5437 - val_auc: 0.8555\n","Epoch 2/100\n","40/40 [==============================] - 2s 56ms/step - loss: 2.2746 - auc: 0.8978 - val_loss: 2.1441 - val_auc: 0.9111\n","Epoch 3/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.9287 - auc: 0.9363 - val_loss: 1.8851 - val_auc: 0.9408\n","Epoch 4/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.8068 - auc: 0.9528 - val_loss: 1.7635 - val_auc: 0.9484\n","Epoch 5/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.6144 - auc: 0.9606 - val_loss: 1.6468 - val_auc: 0.9534\n","Epoch 6/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.5764 - auc: 0.9640 - val_loss: 1.6392 - val_auc: 0.9502\n","Epoch 7/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.5014 - auc: 0.9658 - val_loss: 1.5052 - val_auc: 0.9625\n","Epoch 8/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.5451 - auc: 0.9663 - val_loss: 1.4690 - val_auc: 0.9621\n","Epoch 9/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.3632 - auc: 0.9710 - val_loss: 1.4438 - val_auc: 0.9606\n","Epoch 10/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.4147 - auc: 0.9713 - val_loss: 1.4276 - val_auc: 0.9593\n","Epoch 11/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.3615 - auc: 0.9719 - val_loss: 1.3689 - val_auc: 0.9635\n","Epoch 12/100\n","40/40 [==============================] - 2s 56ms/step - loss: 1.3267 - auc: 0.9730 - val_loss: 1.3867 - val_auc: 0.9621\n","Epoch 13/100\n","40/40 [==============================] - 2s 60ms/step - loss: 1.2509 - auc: 0.9766 - val_loss: 1.3181 - val_auc: 0.9667\n","Epoch 14/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2297 - auc: 0.9773 - val_loss: 1.2961 - val_auc: 0.9630\n","Epoch 15/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2286 - auc: 0.9758 - val_loss: 1.2987 - val_auc: 0.9642\n","Epoch 16/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1593 - auc: 0.9807 - val_loss: 1.3191 - val_auc: 0.9600\n","Epoch 17/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0948 - auc: 0.9833 - val_loss: 1.3169 - val_auc: 0.9612\n","\n","Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 18/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0738 - auc: 0.9837 - val_loss: 1.2540 - val_auc: 0.9650\n","Epoch 19/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9791 - auc: 0.9864 - val_loss: 1.2429 - val_auc: 0.9638\n","Epoch 20/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0115 - auc: 0.9864 - val_loss: 1.2323 - val_auc: 0.9660\n","Epoch 21/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0328 - auc: 0.9840 - val_loss: 1.2389 - val_auc: 0.9640\n","Epoch 22/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9661 - auc: 0.9854 - val_loss: 1.2262 - val_auc: 0.9648\n","Epoch 23/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9374 - auc: 0.9890 - val_loss: 1.2232 - val_auc: 0.9641\n","Epoch 24/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.9924 - auc: 0.9870 - val_loss: 1.2247 - val_auc: 0.9635\n","Epoch 25/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9276 - auc: 0.9896 - val_loss: 1.2193 - val_auc: 0.9628\n","Epoch 26/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9610 - auc: 0.9870 - val_loss: 1.2270 - val_auc: 0.9613\n","Epoch 27/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9980 - auc: 0.9879 - val_loss: 1.2277 - val_auc: 0.9615\n","Epoch 28/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9649 - auc: 0.9872 - val_loss: 1.2315 - val_auc: 0.9622\n","\n","Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","Epoch 29/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.9380 - auc: 0.9895 - val_loss: 1.2281 - val_auc: 0.9622\n","  ==============================================================================================  \n","Epoch 1/100\n","40/40 [==============================] - 5s 70ms/step - loss: 3.2616 - auc: 0.7522 - val_loss: 2.5598 - val_auc: 0.8561\n","Epoch 2/100\n","40/40 [==============================] - 2s 57ms/step - loss: 2.1956 - auc: 0.9093 - val_loss: 2.1378 - val_auc: 0.9147\n","Epoch 3/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.9774 - auc: 0.9298 - val_loss: 1.8803 - val_auc: 0.9360\n","Epoch 4/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.8373 - auc: 0.9457 - val_loss: 1.7386 - val_auc: 0.9438\n","Epoch 5/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.6359 - auc: 0.9606 - val_loss: 1.6416 - val_auc: 0.9486\n","Epoch 6/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.6233 - auc: 0.9582 - val_loss: 1.5939 - val_auc: 0.9515\n","Epoch 7/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.4767 - auc: 0.9662 - val_loss: 1.4797 - val_auc: 0.9569\n","Epoch 8/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.4263 - auc: 0.9716 - val_loss: 1.5012 - val_auc: 0.9539\n","Epoch 9/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2965 - auc: 0.9724 - val_loss: 1.3740 - val_auc: 0.9605\n","Epoch 10/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2899 - auc: 0.9760 - val_loss: 1.4027 - val_auc: 0.9617\n","Epoch 11/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.3337 - auc: 0.9736 - val_loss: 1.3333 - val_auc: 0.9632\n","Epoch 12/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2546 - auc: 0.9759 - val_loss: 1.2887 - val_auc: 0.9663\n","Epoch 13/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2538 - auc: 0.9764 - val_loss: 1.2968 - val_auc: 0.9625\n","Epoch 14/100\n","40/40 [==============================] - 2s 60ms/step - loss: 1.1882 - auc: 0.9803 - val_loss: 1.3162 - val_auc: 0.9616\n","Epoch 15/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1454 - auc: 0.9817 - val_loss: 1.2564 - val_auc: 0.9647\n","Epoch 16/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1265 - auc: 0.9812 - val_loss: 1.2131 - val_auc: 0.9673\n","Epoch 17/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.0575 - auc: 0.9829 - val_loss: 1.2382 - val_auc: 0.9629\n","Epoch 18/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.0604 - auc: 0.9839 - val_loss: 1.2506 - val_auc: 0.9640\n","Epoch 19/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0378 - auc: 0.9833 - val_loss: 1.1780 - val_auc: 0.9680\n","Epoch 20/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0036 - auc: 0.9847 - val_loss: 1.2322 - val_auc: 0.9629\n","Epoch 21/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9677 - auc: 0.9831 - val_loss: 1.1820 - val_auc: 0.9682\n","Epoch 22/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9433 - auc: 0.9880 - val_loss: 1.1571 - val_auc: 0.9657\n","Epoch 23/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.9737 - auc: 0.9838 - val_loss: 1.1713 - val_auc: 0.9642\n","Epoch 24/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0111 - auc: 0.9822 - val_loss: 1.1486 - val_auc: 0.9680\n","Epoch 25/100\n","40/40 [==============================] - 2s 56ms/step - loss: 0.8912 - auc: 0.9891 - val_loss: 1.1276 - val_auc: 0.9672\n","Epoch 26/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8834 - auc: 0.9881 - val_loss: 1.1406 - val_auc: 0.9685\n","Epoch 27/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8658 - auc: 0.9872 - val_loss: 1.1595 - val_auc: 0.9650\n","Epoch 28/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8974 - auc: 0.9859 - val_loss: 1.0820 - val_auc: 0.9704\n","Epoch 29/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.7700 - auc: 0.9910 - val_loss: 1.1014 - val_auc: 0.9691\n","Epoch 30/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.8532 - auc: 0.9902 - val_loss: 1.1173 - val_auc: 0.9662\n","Epoch 31/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8162 - auc: 0.9878 - val_loss: 1.1157 - val_auc: 0.9636\n","\n","Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 32/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.7630 - auc: 0.9917 - val_loss: 1.0885 - val_auc: 0.9683\n","  ==============================================================================================  \n","Epoch 1/100\n","40/40 [==============================] - 4s 69ms/step - loss: 3.0395 - auc: 0.7813 - val_loss: 2.4878 - val_auc: 0.8637\n","Epoch 2/100\n","40/40 [==============================] - 2s 57ms/step - loss: 2.2398 - auc: 0.8994 - val_loss: 2.2309 - val_auc: 0.9073\n","Epoch 3/100\n","40/40 [==============================] - 2s 58ms/step - loss: 2.0075 - auc: 0.9248 - val_loss: 1.8849 - val_auc: 0.9382\n","Epoch 4/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.8939 - auc: 0.9407 - val_loss: 1.7549 - val_auc: 0.9446\n","Epoch 5/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.7593 - auc: 0.9535 - val_loss: 1.6327 - val_auc: 0.9564\n","Epoch 6/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.6782 - auc: 0.9561 - val_loss: 1.6090 - val_auc: 0.9530\n","Epoch 7/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.5530 - auc: 0.9636 - val_loss: 1.5465 - val_auc: 0.9611\n","Epoch 8/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.5275 - auc: 0.9644 - val_loss: 1.4805 - val_auc: 0.9593\n","Epoch 9/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.3703 - auc: 0.9755 - val_loss: 1.4846 - val_auc: 0.9597\n","Epoch 10/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.3382 - auc: 0.9757 - val_loss: 1.3730 - val_auc: 0.9663\n","Epoch 11/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.4094 - auc: 0.9679 - val_loss: 1.3383 - val_auc: 0.9663\n","Epoch 12/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.3292 - auc: 0.9718 - val_loss: 1.3395 - val_auc: 0.9646\n","Epoch 13/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2672 - auc: 0.9786 - val_loss: 1.2577 - val_auc: 0.9685\n","Epoch 14/100\n","40/40 [==============================] - 2s 62ms/step - loss: 1.2255 - auc: 0.9802 - val_loss: 1.2452 - val_auc: 0.9716\n","Epoch 15/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.2776 - auc: 0.9767 - val_loss: 1.2504 - val_auc: 0.9704\n","Epoch 16/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1798 - auc: 0.9800 - val_loss: 1.1862 - val_auc: 0.9737\n","Epoch 17/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1607 - auc: 0.9801 - val_loss: 1.1955 - val_auc: 0.9689\n","Epoch 18/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.1241 - auc: 0.9812 - val_loss: 1.1543 - val_auc: 0.9748\n","Epoch 19/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.1430 - auc: 0.9786 - val_loss: 1.1457 - val_auc: 0.9728\n","Epoch 20/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.0318 - auc: 0.9852 - val_loss: 1.1474 - val_auc: 0.9710\n","Epoch 21/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0740 - auc: 0.9820 - val_loss: 1.1924 - val_auc: 0.9673\n","Epoch 22/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9898 - auc: 0.9834 - val_loss: 1.1716 - val_auc: 0.9702\n","\n","Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 23/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.9690 - auc: 0.9849 - val_loss: 1.1212 - val_auc: 0.9750\n","Epoch 24/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9151 - auc: 0.9884 - val_loss: 1.1114 - val_auc: 0.9737\n","Epoch 25/100\n","40/40 [==============================] - 2s 59ms/step - loss: 0.9184 - auc: 0.9881 - val_loss: 1.1063 - val_auc: 0.9752\n","Epoch 26/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.9457 - auc: 0.9872 - val_loss: 1.1015 - val_auc: 0.9767\n","Epoch 27/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8849 - auc: 0.9902 - val_loss: 1.0984 - val_auc: 0.9768\n","Epoch 28/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.8892 - auc: 0.9870 - val_loss: 1.0980 - val_auc: 0.9761\n","Epoch 29/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8915 - auc: 0.9889 - val_loss: 1.0958 - val_auc: 0.9762\n","Epoch 30/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8572 - auc: 0.9898 - val_loss: 1.0912 - val_auc: 0.9763\n","Epoch 31/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8563 - auc: 0.9897 - val_loss: 1.0958 - val_auc: 0.9777\n","Epoch 32/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9407 - auc: 0.9853 - val_loss: 1.0987 - val_auc: 0.9763\n","Epoch 33/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9027 - auc: 0.9882 - val_loss: 1.0839 - val_auc: 0.9751\n","Epoch 34/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.8815 - auc: 0.9882 - val_loss: 1.0827 - val_auc: 0.9758\n","Epoch 35/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9271 - auc: 0.9851 - val_loss: 1.0993 - val_auc: 0.9749\n","Epoch 36/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8414 - auc: 0.9907 - val_loss: 1.0926 - val_auc: 0.9764\n","Epoch 37/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9008 - auc: 0.9874 - val_loss: 1.0877 - val_auc: 0.9765\n","\n","Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","Epoch 38/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8709 - auc: 0.9899 - val_loss: 1.0867 - val_auc: 0.9757\n","  ==============================================================================================  \n","Epoch 1/100\n","40/40 [==============================] - 5s 70ms/step - loss: 3.0780 - auc: 0.7780 - val_loss: 2.5927 - val_auc: 0.8433\n","Epoch 2/100\n","40/40 [==============================] - 2s 57ms/step - loss: 2.1230 - auc: 0.9081 - val_loss: 2.1774 - val_auc: 0.9023\n","Epoch 3/100\n","40/40 [==============================] - 2s 57ms/step - loss: 2.0119 - auc: 0.9313 - val_loss: 1.9156 - val_auc: 0.9344\n","Epoch 4/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.7948 - auc: 0.9441 - val_loss: 1.8424 - val_auc: 0.9416\n","Epoch 5/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.6631 - auc: 0.9578 - val_loss: 1.6718 - val_auc: 0.9534\n","Epoch 6/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.6468 - auc: 0.9616 - val_loss: 1.5771 - val_auc: 0.9578\n","Epoch 7/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.4910 - auc: 0.9669 - val_loss: 1.5111 - val_auc: 0.9613\n","Epoch 8/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.4768 - auc: 0.9676 - val_loss: 1.4725 - val_auc: 0.9620\n","Epoch 9/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.3703 - auc: 0.9709 - val_loss: 1.4292 - val_auc: 0.9633\n","Epoch 10/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.3576 - auc: 0.9735 - val_loss: 1.3937 - val_auc: 0.9665\n","Epoch 11/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.3530 - auc: 0.9724 - val_loss: 1.3310 - val_auc: 0.9636\n","Epoch 12/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.2805 - auc: 0.9769 - val_loss: 1.3367 - val_auc: 0.9659\n","Epoch 13/100\n","40/40 [==============================] - 2s 62ms/step - loss: 1.2709 - auc: 0.9755 - val_loss: 1.3328 - val_auc: 0.9668\n","Epoch 14/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.2512 - auc: 0.9776 - val_loss: 1.2888 - val_auc: 0.9670\n","Epoch 15/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1582 - auc: 0.9810 - val_loss: 1.3267 - val_auc: 0.9673\n","Epoch 16/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.1364 - auc: 0.9785 - val_loss: 1.2585 - val_auc: 0.9653\n","Epoch 17/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1365 - auc: 0.9808 - val_loss: 1.2494 - val_auc: 0.9674\n","Epoch 18/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.0365 - auc: 0.9836 - val_loss: 1.2146 - val_auc: 0.9697\n","Epoch 19/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.0300 - auc: 0.9840 - val_loss: 1.2140 - val_auc: 0.9708\n","Epoch 20/100\n","40/40 [==============================] - 2s 57ms/step - loss: 1.1033 - auc: 0.9830 - val_loss: 1.2055 - val_auc: 0.9639\n","Epoch 21/100\n","40/40 [==============================] - 2s 58ms/step - loss: 1.0068 - auc: 0.9821 - val_loss: 1.2094 - val_auc: 0.9688\n","Epoch 22/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9941 - auc: 0.9861 - val_loss: 1.1801 - val_auc: 0.9700\n","Epoch 23/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9372 - auc: 0.9861 - val_loss: 1.1783 - val_auc: 0.9688\n","Epoch 24/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8553 - auc: 0.9917 - val_loss: 1.1375 - val_auc: 0.9702\n","Epoch 25/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.9008 - auc: 0.9889 - val_loss: 1.1442 - val_auc: 0.9702\n","Epoch 26/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9190 - auc: 0.9856 - val_loss: 1.1533 - val_auc: 0.9654\n","Epoch 27/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.9033 - auc: 0.9878 - val_loss: 1.1536 - val_auc: 0.9669\n","\n","Epoch 00027: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 28/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.8727 - auc: 0.9879 - val_loss: 1.1239 - val_auc: 0.9679\n","Epoch 29/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.7730 - auc: 0.9904 - val_loss: 1.1035 - val_auc: 0.9703\n","Epoch 30/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.8042 - auc: 0.9901 - val_loss: 1.1094 - val_auc: 0.9667\n","Epoch 31/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.7715 - auc: 0.9901 - val_loss: 1.1090 - val_auc: 0.9674\n","Epoch 32/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.7603 - auc: 0.9920 - val_loss: 1.1044 - val_auc: 0.9683\n","\n","Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","Epoch 33/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.7456 - auc: 0.9924 - val_loss: 1.1032 - val_auc: 0.9676\n","Epoch 34/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.7744 - auc: 0.9896 - val_loss: 1.1018 - val_auc: 0.9698\n","Epoch 35/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8120 - auc: 0.9901 - val_loss: 1.1011 - val_auc: 0.9691\n","Epoch 36/100\n","40/40 [==============================] - 2s 58ms/step - loss: 0.7716 - auc: 0.9927 - val_loss: 1.1027 - val_auc: 0.9698\n","Epoch 37/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.7798 - auc: 0.9901 - val_loss: 1.1025 - val_auc: 0.9683\n","Epoch 38/100\n","40/40 [==============================] - 2s 60ms/step - loss: 0.7905 - auc: 0.9921 - val_loss: 1.1031 - val_auc: 0.9690\n","\n","Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","Epoch 39/100\n","40/40 [==============================] - 2s 57ms/step - loss: 0.8019 - auc: 0.9883 - val_loss: 1.1028 - val_auc: 0.9690\n","  ==============================================================================================  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.773427,"end_time":"2020-12-19T21:50:37.314953","exception":false,"start_time":"2020-12-19T21:50:36.541526","status":"completed"},"tags":[],"id":"DmRggyuUwP3S"},"source":["# Submitting"]},{"cell_type":"code","metadata":{"id":"9xxRPhDGwP3S","colab":{"base_uri":"https://localhost:8080/","height":609},"executionInfo":{"status":"ok","timestamp":1613359561608,"user_tz":-540,"elapsed":1213,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"0afb75b2-e841-4878-b557-ca30cbb3614c"},"source":["sample_submssion = pd.read_csv(path + 'sample_submission.csv')\n","sample_submssion.iloc[:,1:] = transformer_test\n","sample_submssion.to_csv(\"transformer.csv\", index = False)\n","sample_submssion"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3125</td>\n","      <td>0.000004</td>\n","      <td>3.199803e-06</td>\n","      <td>5.679599e-06</td>\n","      <td>0.000540</td>\n","      <td>0.000351</td>\n","      <td>0.000002</td>\n","      <td>1.067405e-04</td>\n","      <td>0.000001</td>\n","      <td>0.000171</td>\n","      <td>0.029167</td>\n","      <td>1.819316e-02</td>\n","      <td>0.210785</td>\n","      <td>5.442244e-02</td>\n","      <td>1.000304e-01</td>\n","      <td>1.162669e-01</td>\n","      <td>0.000413</td>\n","      <td>5.373705e-07</td>\n","      <td>7.817568e-05</td>\n","      <td>3.283367e-05</td>\n","      <td>5.963559e-05</td>\n","      <td>1.570774e-05</td>\n","      <td>1.583519e-06</td>\n","      <td>0.000165</td>\n","      <td>0.000759</td>\n","      <td>2.887614e-01</td>\n","      <td>0.000029</td>\n","      <td>0.002046</td>\n","      <td>5.954775e-06</td>\n","      <td>0.000395</td>\n","      <td>6.126476e-04</td>\n","      <td>0.001438</td>\n","      <td>0.000657</td>\n","      <td>0.003723</td>\n","      <td>0.000020</td>\n","      <td>0.000262</td>\n","      <td>1.342923e-06</td>\n","      <td>0.000111</td>\n","      <td>5.264769e-05</td>\n","      <td>0.000924</td>\n","      <td>0.000389</td>\n","      <td>0.000024</td>\n","      <td>0.000002</td>\n","      <td>0.002101</td>\n","      <td>0.001833</td>\n","      <td>7.270281e-05</td>\n","      <td>1.954279e-04</td>\n","      <td>3.956714e-05</td>\n","      <td>0.000257</td>\n","      <td>0.017404</td>\n","      <td>0.000370</td>\n","      <td>0.000094</td>\n","      <td>0.010349</td>\n","      <td>6.601973e-02</td>\n","      <td>3.972859e-05</td>\n","      <td>8.774088e-06</td>\n","      <td>0.000007</td>\n","      <td>9.427548e-07</td>\n","      <td>2.515463e-04</td>\n","      <td>3.636388e-02</td>\n","      <td>1.086393e-05</td>\n","      <td>0.033550</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3126</td>\n","      <td>0.000457</td>\n","      <td>7.930650e-06</td>\n","      <td>7.342406e-06</td>\n","      <td>0.001670</td>\n","      <td>0.000012</td>\n","      <td>0.000050</td>\n","      <td>4.953320e-07</td>\n","      <td>0.003520</td>\n","      <td>0.000012</td>\n","      <td>0.000332</td>\n","      <td>5.847937e-07</td>\n","      <td>0.000067</td>\n","      <td>1.670109e-06</td>\n","      <td>1.897715e-05</td>\n","      <td>6.311990e-06</td>\n","      <td>0.000726</td>\n","      <td>1.085518e-03</td>\n","      <td>1.277019e-06</td>\n","      <td>7.844548e-05</td>\n","      <td>5.235327e-07</td>\n","      <td>1.519000e-05</td>\n","      <td>6.711196e-04</td>\n","      <td>0.000838</td>\n","      <td>0.000710</td>\n","      <td>4.792686e-06</td>\n","      <td>0.000011</td>\n","      <td>0.975872</td>\n","      <td>4.170228e-06</td>\n","      <td>0.000003</td>\n","      <td>6.544556e-06</td>\n","      <td>0.000253</td>\n","      <td>0.000049</td>\n","      <td>0.000080</td>\n","      <td>0.000233</td>\n","      <td>0.001613</td>\n","      <td>6.929004e-05</td>\n","      <td>0.000260</td>\n","      <td>6.151832e-07</td>\n","      <td>0.000005</td>\n","      <td>0.000006</td>\n","      <td>0.000654</td>\n","      <td>0.000026</td>\n","      <td>0.000392</td>\n","      <td>0.000052</td>\n","      <td>3.987473e-06</td>\n","      <td>7.886456e-05</td>\n","      <td>4.411476e-07</td>\n","      <td>0.000014</td>\n","      <td>0.003554</td>\n","      <td>0.000089</td>\n","      <td>0.000125</td>\n","      <td>0.000025</td>\n","      <td>5.362591e-06</td>\n","      <td>6.733638e-05</td>\n","      <td>3.943343e-04</td>\n","      <td>0.000036</td>\n","      <td>9.141508e-05</td>\n","      <td>6.366154e-05</td>\n","      <td>7.181816e-07</td>\n","      <td>5.131632e-06</td>\n","      <td>0.005565</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3127</td>\n","      <td>0.000951</td>\n","      <td>2.363748e-02</td>\n","      <td>1.436562e-06</td>\n","      <td>0.000013</td>\n","      <td>0.000002</td>\n","      <td>0.000056</td>\n","      <td>9.751473e-04</td>\n","      <td>0.014894</td>\n","      <td>0.014993</td>\n","      <td>0.000004</td>\n","      <td>7.243217e-07</td>\n","      <td>0.000011</td>\n","      <td>9.250743e-05</td>\n","      <td>7.579114e-07</td>\n","      <td>4.507687e-05</td>\n","      <td>0.000140</td>\n","      <td>3.351221e-04</td>\n","      <td>4.093389e-05</td>\n","      <td>6.890607e-06</td>\n","      <td>2.871860e-06</td>\n","      <td>1.890440e-07</td>\n","      <td>8.199413e-06</td>\n","      <td>0.000002</td>\n","      <td>0.000003</td>\n","      <td>6.015507e-06</td>\n","      <td>0.000007</td>\n","      <td>0.001512</td>\n","      <td>3.752296e-04</td>\n","      <td>0.000541</td>\n","      <td>9.923330e-05</td>\n","      <td>0.000003</td>\n","      <td>0.000008</td>\n","      <td>0.000385</td>\n","      <td>0.000661</td>\n","      <td>0.000125</td>\n","      <td>8.378948e-06</td>\n","      <td>0.000041</td>\n","      <td>1.218698e-03</td>\n","      <td>0.000528</td>\n","      <td>0.000181</td>\n","      <td>0.001109</td>\n","      <td>0.000014</td>\n","      <td>0.000089</td>\n","      <td>0.000208</td>\n","      <td>3.926054e-02</td>\n","      <td>8.652668e-01</td>\n","      <td>1.212111e-06</td>\n","      <td>0.001988</td>\n","      <td>0.014284</td>\n","      <td>0.003481</td>\n","      <td>0.000107</td>\n","      <td>0.000026</td>\n","      <td>1.384518e-04</td>\n","      <td>2.717580e-07</td>\n","      <td>1.062233e-02</td>\n","      <td>0.000001</td>\n","      <td>3.516010e-04</td>\n","      <td>1.940584e-07</td>\n","      <td>1.358634e-05</td>\n","      <td>2.245386e-04</td>\n","      <td>0.000897</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3128</td>\n","      <td>0.001685</td>\n","      <td>1.445622e-06</td>\n","      <td>5.516265e-05</td>\n","      <td>0.000043</td>\n","      <td>0.000024</td>\n","      <td>0.000018</td>\n","      <td>8.604532e-08</td>\n","      <td>0.000209</td>\n","      <td>0.000047</td>\n","      <td>0.000001</td>\n","      <td>6.892806e-07</td>\n","      <td>0.000002</td>\n","      <td>1.653091e-07</td>\n","      <td>1.670446e-08</td>\n","      <td>2.031162e-07</td>\n","      <td>0.000113</td>\n","      <td>3.537409e-05</td>\n","      <td>3.424891e-07</td>\n","      <td>1.761731e-05</td>\n","      <td>6.727868e-08</td>\n","      <td>6.628845e-08</td>\n","      <td>1.615631e-04</td>\n","      <td>0.000408</td>\n","      <td>0.000040</td>\n","      <td>8.882908e-07</td>\n","      <td>0.000011</td>\n","      <td>0.988166</td>\n","      <td>5.227134e-07</td>\n","      <td>0.000008</td>\n","      <td>9.381392e-06</td>\n","      <td>0.000086</td>\n","      <td>0.000014</td>\n","      <td>0.000530</td>\n","      <td>0.000152</td>\n","      <td>0.000093</td>\n","      <td>2.485258e-06</td>\n","      <td>0.000006</td>\n","      <td>1.258721e-07</td>\n","      <td>0.000002</td>\n","      <td>0.000002</td>\n","      <td>0.000002</td>\n","      <td>0.000043</td>\n","      <td>0.000057</td>\n","      <td>0.000028</td>\n","      <td>3.009510e-07</td>\n","      <td>4.714276e-07</td>\n","      <td>4.157913e-06</td>\n","      <td>0.000002</td>\n","      <td>0.000883</td>\n","      <td>0.001989</td>\n","      <td>0.004250</td>\n","      <td>0.000002</td>\n","      <td>9.195410e-07</td>\n","      <td>2.587433e-06</td>\n","      <td>3.836748e-06</td>\n","      <td>0.000005</td>\n","      <td>5.851774e-07</td>\n","      <td>1.239584e-04</td>\n","      <td>1.794896e-07</td>\n","      <td>9.008100e-07</td>\n","      <td>0.000655</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3129</td>\n","      <td>0.001282</td>\n","      <td>1.087125e-06</td>\n","      <td>7.811357e-07</td>\n","      <td>0.000006</td>\n","      <td>0.001859</td>\n","      <td>0.000459</td>\n","      <td>3.822749e-06</td>\n","      <td>0.000208</td>\n","      <td>0.000070</td>\n","      <td>0.000063</td>\n","      <td>9.578569e-06</td>\n","      <td>0.000007</td>\n","      <td>2.531105e-07</td>\n","      <td>8.582837e-07</td>\n","      <td>8.237320e-06</td>\n","      <td>0.000211</td>\n","      <td>5.440887e-06</td>\n","      <td>2.060923e-06</td>\n","      <td>1.816982e-03</td>\n","      <td>2.860328e-07</td>\n","      <td>2.958379e-06</td>\n","      <td>1.968860e-05</td>\n","      <td>0.011647</td>\n","      <td>0.000009</td>\n","      <td>6.877446e-06</td>\n","      <td>0.000235</td>\n","      <td>0.961295</td>\n","      <td>4.541154e-07</td>\n","      <td>0.000033</td>\n","      <td>4.615864e-06</td>\n","      <td>0.000286</td>\n","      <td>0.000022</td>\n","      <td>0.002564</td>\n","      <td>0.000011</td>\n","      <td>0.000199</td>\n","      <td>5.231528e-07</td>\n","      <td>0.000017</td>\n","      <td>1.496658e-06</td>\n","      <td>0.000005</td>\n","      <td>0.000023</td>\n","      <td>0.000010</td>\n","      <td>0.000030</td>\n","      <td>0.000002</td>\n","      <td>0.000002</td>\n","      <td>2.158898e-06</td>\n","      <td>1.778826e-06</td>\n","      <td>1.156243e-05</td>\n","      <td>0.000010</td>\n","      <td>0.001129</td>\n","      <td>0.001543</td>\n","      <td>0.014131</td>\n","      <td>0.000001</td>\n","      <td>4.921151e-07</td>\n","      <td>6.991006e-07</td>\n","      <td>1.641918e-07</td>\n","      <td>0.000005</td>\n","      <td>1.858679e-06</td>\n","      <td>9.214650e-06</td>\n","      <td>6.478103e-07</td>\n","      <td>5.906661e-05</td>\n","      <td>0.000653</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>777</th>\n","      <td>3902</td>\n","      <td>0.004295</td>\n","      <td>2.460296e-06</td>\n","      <td>6.039345e-06</td>\n","      <td>0.000006</td>\n","      <td>0.000659</td>\n","      <td>0.000039</td>\n","      <td>8.003377e-07</td>\n","      <td>0.000863</td>\n","      <td>0.000103</td>\n","      <td>0.000364</td>\n","      <td>5.211375e-05</td>\n","      <td>0.000037</td>\n","      <td>1.021327e-06</td>\n","      <td>5.597417e-07</td>\n","      <td>1.083226e-05</td>\n","      <td>0.002453</td>\n","      <td>7.915429e-06</td>\n","      <td>5.686478e-06</td>\n","      <td>3.163069e-03</td>\n","      <td>7.051581e-07</td>\n","      <td>2.759230e-07</td>\n","      <td>2.181601e-05</td>\n","      <td>0.002383</td>\n","      <td>0.000014</td>\n","      <td>3.272359e-06</td>\n","      <td>0.000143</td>\n","      <td>0.974144</td>\n","      <td>8.243088e-07</td>\n","      <td>0.000080</td>\n","      <td>1.253799e-05</td>\n","      <td>0.000248</td>\n","      <td>0.000036</td>\n","      <td>0.004681</td>\n","      <td>0.000012</td>\n","      <td>0.001504</td>\n","      <td>2.697787e-05</td>\n","      <td>0.000042</td>\n","      <td>5.827691e-07</td>\n","      <td>0.000002</td>\n","      <td>0.000027</td>\n","      <td>0.000011</td>\n","      <td>0.000011</td>\n","      <td>0.000004</td>\n","      <td>0.000003</td>\n","      <td>2.359547e-06</td>\n","      <td>1.236040e-06</td>\n","      <td>3.265007e-05</td>\n","      <td>0.000007</td>\n","      <td>0.000237</td>\n","      <td>0.001710</td>\n","      <td>0.001272</td>\n","      <td>0.000005</td>\n","      <td>1.161548e-06</td>\n","      <td>1.162374e-05</td>\n","      <td>1.826842e-07</td>\n","      <td>0.000006</td>\n","      <td>5.595795e-07</td>\n","      <td>1.481665e-05</td>\n","      <td>7.298780e-07</td>\n","      <td>4.678393e-05</td>\n","      <td>0.001169</td>\n","    </tr>\n","    <tr>\n","      <th>778</th>\n","      <td>3903</td>\n","      <td>0.022230</td>\n","      <td>9.198171e-06</td>\n","      <td>8.071564e-05</td>\n","      <td>0.000288</td>\n","      <td>0.002483</td>\n","      <td>0.000127</td>\n","      <td>7.835117e-06</td>\n","      <td>0.001747</td>\n","      <td>0.000094</td>\n","      <td>0.000090</td>\n","      <td>5.739951e-05</td>\n","      <td>0.000014</td>\n","      <td>2.365059e-06</td>\n","      <td>1.823477e-06</td>\n","      <td>8.734289e-06</td>\n","      <td>0.007329</td>\n","      <td>2.081825e-05</td>\n","      <td>2.520397e-05</td>\n","      <td>1.155536e-02</td>\n","      <td>3.864913e-06</td>\n","      <td>7.229333e-06</td>\n","      <td>2.609942e-04</td>\n","      <td>0.008220</td>\n","      <td>0.000048</td>\n","      <td>1.150680e-05</td>\n","      <td>0.000127</td>\n","      <td>0.939080</td>\n","      <td>4.192576e-06</td>\n","      <td>0.000039</td>\n","      <td>8.053523e-07</td>\n","      <td>0.000164</td>\n","      <td>0.000062</td>\n","      <td>0.001131</td>\n","      <td>0.000020</td>\n","      <td>0.000237</td>\n","      <td>1.299454e-04</td>\n","      <td>0.000074</td>\n","      <td>7.802985e-06</td>\n","      <td>0.000004</td>\n","      <td>0.000041</td>\n","      <td>0.000179</td>\n","      <td>0.000011</td>\n","      <td>0.000007</td>\n","      <td>0.000002</td>\n","      <td>1.245788e-06</td>\n","      <td>6.632108e-07</td>\n","      <td>5.206932e-05</td>\n","      <td>0.000010</td>\n","      <td>0.000329</td>\n","      <td>0.000960</td>\n","      <td>0.001821</td>\n","      <td>0.000021</td>\n","      <td>1.440705e-06</td>\n","      <td>1.188457e-05</td>\n","      <td>2.041688e-06</td>\n","      <td>0.000028</td>\n","      <td>1.784824e-05</td>\n","      <td>7.061465e-05</td>\n","      <td>3.157177e-06</td>\n","      <td>3.162059e-04</td>\n","      <td>0.000308</td>\n","    </tr>\n","    <tr>\n","      <th>779</th>\n","      <td>3904</td>\n","      <td>0.000322</td>\n","      <td>1.333728e-07</td>\n","      <td>9.506515e-07</td>\n","      <td>0.000013</td>\n","      <td>0.000204</td>\n","      <td>0.000043</td>\n","      <td>1.011551e-06</td>\n","      <td>0.000028</td>\n","      <td>0.000028</td>\n","      <td>0.000212</td>\n","      <td>3.254090e-06</td>\n","      <td>0.000023</td>\n","      <td>3.983599e-07</td>\n","      <td>9.722271e-07</td>\n","      <td>2.104642e-06</td>\n","      <td>0.000053</td>\n","      <td>1.037531e-06</td>\n","      <td>1.786107e-07</td>\n","      <td>7.756500e-05</td>\n","      <td>4.798668e-08</td>\n","      <td>9.165767e-07</td>\n","      <td>1.339309e-05</td>\n","      <td>0.007310</td>\n","      <td>0.000011</td>\n","      <td>7.914059e-06</td>\n","      <td>0.000018</td>\n","      <td>0.984760</td>\n","      <td>1.202762e-07</td>\n","      <td>0.000017</td>\n","      <td>5.162187e-06</td>\n","      <td>0.000750</td>\n","      <td>0.000023</td>\n","      <td>0.001841</td>\n","      <td>0.000005</td>\n","      <td>0.000260</td>\n","      <td>1.278571e-07</td>\n","      <td>0.000011</td>\n","      <td>7.008268e-07</td>\n","      <td>0.000002</td>\n","      <td>0.000013</td>\n","      <td>0.000003</td>\n","      <td>0.000002</td>\n","      <td>0.000004</td>\n","      <td>0.000003</td>\n","      <td>3.066692e-07</td>\n","      <td>1.009424e-06</td>\n","      <td>4.535526e-06</td>\n","      <td>0.000014</td>\n","      <td>0.001172</td>\n","      <td>0.000492</td>\n","      <td>0.001342</td>\n","      <td>0.000002</td>\n","      <td>9.056953e-07</td>\n","      <td>2.097317e-06</td>\n","      <td>3.356830e-07</td>\n","      <td>0.000005</td>\n","      <td>1.960888e-07</td>\n","      <td>7.883888e-06</td>\n","      <td>2.226534e-07</td>\n","      <td>5.031873e-06</td>\n","      <td>0.000879</td>\n","    </tr>\n","    <tr>\n","      <th>780</th>\n","      <td>3905</td>\n","      <td>0.000212</td>\n","      <td>1.477733e-02</td>\n","      <td>1.229445e-05</td>\n","      <td>0.000007</td>\n","      <td>0.000007</td>\n","      <td>0.000037</td>\n","      <td>3.718256e-02</td>\n","      <td>0.000495</td>\n","      <td>0.000134</td>\n","      <td>0.000049</td>\n","      <td>8.229397e-06</td>\n","      <td>0.000003</td>\n","      <td>1.187087e-05</td>\n","      <td>1.331099e-05</td>\n","      <td>7.726767e-07</td>\n","      <td>0.000011</td>\n","      <td>2.238695e-04</td>\n","      <td>6.130533e-04</td>\n","      <td>8.102581e-07</td>\n","      <td>2.506042e-05</td>\n","      <td>1.763265e-06</td>\n","      <td>3.454903e-07</td>\n","      <td>0.000005</td>\n","      <td>0.000002</td>\n","      <td>1.014279e-04</td>\n","      <td>0.000050</td>\n","      <td>0.002166</td>\n","      <td>2.586949e-03</td>\n","      <td>0.000007</td>\n","      <td>3.180867e-04</td>\n","      <td>0.000020</td>\n","      <td>0.000005</td>\n","      <td>0.000727</td>\n","      <td>0.000148</td>\n","      <td>0.000005</td>\n","      <td>9.580469e-06</td>\n","      <td>0.000001</td>\n","      <td>8.555789e-01</td>\n","      <td>0.000004</td>\n","      <td>0.000001</td>\n","      <td>0.001860</td>\n","      <td>0.000661</td>\n","      <td>0.000009</td>\n","      <td>0.000006</td>\n","      <td>4.602746e-04</td>\n","      <td>2.687839e-03</td>\n","      <td>1.804904e-05</td>\n","      <td>0.065880</td>\n","      <td>0.000422</td>\n","      <td>0.001031</td>\n","      <td>0.000004</td>\n","      <td>0.000009</td>\n","      <td>5.042593e-05</td>\n","      <td>1.017475e-05</td>\n","      <td>9.958289e-04</td>\n","      <td>0.000010</td>\n","      <td>1.770676e-03</td>\n","      <td>1.285139e-06</td>\n","      <td>1.494896e-05</td>\n","      <td>8.475245e-03</td>\n","      <td>0.000060</td>\n","    </tr>\n","    <tr>\n","      <th>781</th>\n","      <td>3906</td>\n","      <td>0.000081</td>\n","      <td>1.260919e-06</td>\n","      <td>1.610641e-04</td>\n","      <td>0.012172</td>\n","      <td>0.000405</td>\n","      <td>0.000290</td>\n","      <td>2.622624e-06</td>\n","      <td>0.000007</td>\n","      <td>0.000025</td>\n","      <td>0.000019</td>\n","      <td>6.226733e-06</td>\n","      <td>0.000707</td>\n","      <td>1.058181e-05</td>\n","      <td>9.731446e-05</td>\n","      <td>2.258265e-05</td>\n","      <td>0.001235</td>\n","      <td>1.237274e-04</td>\n","      <td>2.337877e-06</td>\n","      <td>1.821319e-04</td>\n","      <td>1.889724e-06</td>\n","      <td>1.244691e-05</td>\n","      <td>1.261405e-04</td>\n","      <td>0.003277</td>\n","      <td>0.001498</td>\n","      <td>6.042214e-05</td>\n","      <td>0.001373</td>\n","      <td>0.902969</td>\n","      <td>1.627393e-06</td>\n","      <td>0.000072</td>\n","      <td>8.769568e-04</td>\n","      <td>0.000205</td>\n","      <td>0.000043</td>\n","      <td>0.001400</td>\n","      <td>0.000808</td>\n","      <td>0.035338</td>\n","      <td>4.326082e-05</td>\n","      <td>0.000057</td>\n","      <td>4.926202e-06</td>\n","      <td>0.000008</td>\n","      <td>0.000089</td>\n","      <td>0.000012</td>\n","      <td>0.000067</td>\n","      <td>0.001343</td>\n","      <td>0.004431</td>\n","      <td>3.058024e-05</td>\n","      <td>9.139730e-05</td>\n","      <td>1.395408e-03</td>\n","      <td>0.000044</td>\n","      <td>0.005775</td>\n","      <td>0.000483</td>\n","      <td>0.018784</td>\n","      <td>0.000012</td>\n","      <td>2.271434e-05</td>\n","      <td>8.394180e-06</td>\n","      <td>5.337096e-06</td>\n","      <td>0.000058</td>\n","      <td>6.582557e-06</td>\n","      <td>9.944940e-04</td>\n","      <td>2.705160e-06</td>\n","      <td>1.468289e-05</td>\n","      <td>0.002606</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>782 rows × 62 columns</p>\n","</div>"],"text/plain":["       id         0             1  ...            58            59        60\n","0    3125  0.000004  3.199803e-06  ...  3.636388e-02  1.086393e-05  0.033550\n","1    3126  0.000457  7.930650e-06  ...  7.181816e-07  5.131632e-06  0.005565\n","2    3127  0.000951  2.363748e-02  ...  1.358634e-05  2.245386e-04  0.000897\n","3    3128  0.001685  1.445622e-06  ...  1.794896e-07  9.008100e-07  0.000655\n","4    3129  0.001282  1.087125e-06  ...  6.478103e-07  5.906661e-05  0.000653\n","..    ...       ...           ...  ...           ...           ...       ...\n","777  3902  0.004295  2.460296e-06  ...  7.298780e-07  4.678393e-05  0.001169\n","778  3903  0.022230  9.198171e-06  ...  3.157177e-06  3.162059e-04  0.000308\n","779  3904  0.000322  1.333728e-07  ...  2.226534e-07  5.031873e-06  0.000879\n","780  3905  0.000212  1.477733e-02  ...  1.494896e-05  8.475245e-03  0.000060\n","781  3906  0.000081  1.260919e-06  ...  2.705160e-06  1.468289e-05  0.002606\n","\n","[782 rows x 62 columns]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"A76Wi3ZtwP3S","executionInfo":{"status":"ok","timestamp":1613359561609,"user_tz":-540,"elapsed":1208,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# https://www.kaggle.com/gogo827jz/jane-street-ffill-transformer-baseline\r\n","# https://wikidocs.net/31379\r\n","# https://www.tensorflow.org/tutorials/text/transformer"],"execution_count":21,"outputs":[]}]}