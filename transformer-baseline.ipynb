{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"papermill":{"duration":654.273766,"end_time":"2020-12-19T21:56:32.593054","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2020-12-19T21:45:38.319288","version":"2.1.0"},"colab":{"name":"transformer-baseline.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7add8683f4dc47719f29c556afd5da87":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_227b95980050451d9c40820a5cc6286a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3c3839cf47544fdf8791dfec22b4484b","IPY_MODEL_7ca1d1111c2b4888aae372580fb01fb8"]}},"227b95980050451d9c40820a5cc6286a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3c3839cf47544fdf8791dfec22b4484b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c4cacb421bc84f6dadd15ac2ec4dabae","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":1875000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1875000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28980ba3dfec4c3d9e87abe8edd3d449"}},"7ca1d1111c2b4888aae372580fb01fb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4591f4ff03e8489faa78d0276753d25c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1875000/1875000 [1:12:02&lt;00:00, 433.79it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7859e58d634949acb02fe709b644d0d2"}},"c4cacb421bc84f6dadd15ac2ec4dabae":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"28980ba3dfec4c3d9e87abe8edd3d449":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4591f4ff03e8489faa78d0276753d25c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7859e58d634949acb02fe709b644d0d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f431773cfa484b019cfcb42ca3813701":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8c2ae04720504ceb8a44b02b7d3d3cef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4dc2ba1599cb4705b201d4b6d6cbe937","IPY_MODEL_b207064f4e62464bb65f09c277938977"]}},"8c2ae04720504ceb8a44b02b7d3d3cef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4dc2ba1599cb4705b201d4b6d6cbe937":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_dce0c63c607b40b2abca5f8e4653599e","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f6eb4ced23d846b69cdbcea91fdd33de"}},"b207064f4e62464bb65f09c277938977":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e9eeb897565b44f3b8cd2451c57723ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20/20 [00:40&lt;00:00,  2.04s/it]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a36af8add48c49beb8a7982b3cd7ad9e"}},"dce0c63c607b40b2abca5f8e4653599e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f6eb4ced23d846b69cdbcea91fdd33de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e9eeb897565b44f3b8cd2451c57723ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a36af8add48c49beb8a7982b3cd7ad9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_Pc0n7jwQSq","executionInfo":{"status":"ok","timestamp":1613304569098,"user_tz":-540,"elapsed":16290,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"74e93c45-6a80-4ea2-9447-6a94d9c6b38f"},"source":["from google.colab import drive\r\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7bJ70Ws1wSGD","executionInfo":{"status":"ok","timestamp":1613304569099,"user_tz":-540,"elapsed":16289,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["#경로 설정\r\n","import os\r\n","os.chdir('/content/drive/My Drive/Colab Notebooks/운동동작분류AI경진대회')"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"_kg_hide-output":false,"execution":{"iopub.execute_input":"2020-12-19T21:45:42.915933Z","iopub.status.busy":"2020-12-19T21:45:42.915215Z","iopub.status.idle":"2020-12-19T21:45:55.422401Z","shell.execute_reply":"2020-12-19T21:45:55.423089Z"},"papermill":{"duration":12.539859,"end_time":"2020-12-19T21:45:55.423292","exception":false,"start_time":"2020-12-19T21:45:42.883433","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"9bbPFoUlwP3L","executionInfo":{"status":"ok","timestamp":1613304571615,"user_tz":-540,"elapsed":18799,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"b9b72901-5da0-4493-e67f-8008ac3f2959"},"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","import tensorflow as tf\n","tf.random.set_seed(42)\n","import tensorflow.keras.backend as K\n","import tensorflow.keras.layers as layers\n","from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","\n","import os, gc, random, datetime\n","import pandas as pd\n","import numpy as np\n","import xgboost as xgb\n","from sklearn.metrics import roc_auc_score, roc_curve\n","import matplotlib.pyplot as plt\n","from tqdm.notebook import tqdm\n","from joblib import dump, load\n","from time import time\n","\n","print(\"Tensorflow version \" + tf.__version__)\n","AUTO = tf.data.experimental.AUTOTUNE"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Tensorflow version 2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.021446,"end_time":"2020-12-19T21:45:55.576052","exception":false,"start_time":"2020-12-19T21:45:55.554606","status":"completed"},"tags":[],"id":"dC9e5tXuwP3M"},"source":["# Preprocessing"]},{"cell_type":"code","metadata":{"id":"A77rJfaswP3M","executionInfo":{"status":"ok","timestamp":1613304577177,"user_tz":-540,"elapsed":24360,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# 데이터 불러오기\n","\n","path = './data/'\n","train = pd.read_csv(path + 'train_features.csv')\n","train_label = pd.read_csv(path + 'train_labels.csv')\n","test = pd.read_csv(path + 'test_features.csv')\n","submission = pd.read_csv(path + 'sample_submission.csv')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"I4bg_yTDwP3M","executionInfo":{"status":"ok","timestamp":1613304612399,"user_tz":-540,"elapsed":59581,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# Pre-Processing Effect on the Accuracy of Event-Based Activity Segmentation and Classification through Inertial Sensors \n","# https://www.researchgate.net/publication/281836367_Pre-Processing_Effect_on_the_Accuracy_of_Event-Based_Activity_Segmentation_and_Classification_through_Inertial_Sensors\n","\n","train['acc_t']  = train.apply(lambda x : (x['acc_x']**2 + x['acc_y'] **2 +  x['acc_z'] ** 2 )**(1/3), axis=1)\n","test['acc_t']  = test.apply(lambda x : (x['acc_x']**2 + x['acc_y'] **2 +  x['acc_z'] ** 2 )**(1/3), axis=1)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gHMRT0CZwP3N","executionInfo":{"status":"ok","timestamp":1613304612401,"user_tz":-540,"elapsed":59582,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["x = np.array(train.iloc[:,2:]).reshape(-1, 600, 7)\n","y = np.array(train_label['label'])\n","test = np.array(test.iloc[:,2:]).reshape(-1, 600, 7)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLcuiCj5bWxK","executionInfo":{"status":"ok","timestamp":1613304612402,"user_tz":-540,"elapsed":59582,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# 26번을 제외한 id 리스트\r\n","feature = list(train_label[train_label['label'] != 26]['id'])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["7add8683f4dc47719f29c556afd5da87","227b95980050451d9c40820a5cc6286a","3c3839cf47544fdf8791dfec22b4484b","7ca1d1111c2b4888aae372580fb01fb8","c4cacb421bc84f6dadd15ac2ec4dabae","28980ba3dfec4c3d9e87abe8edd3d449","4591f4ff03e8489faa78d0276753d25c","7859e58d634949acb02fe709b644d0d2"]},"id":"FW6BmtbybWpD","executionInfo":{"status":"ok","timestamp":1613305098277,"user_tz":-540,"elapsed":545456,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"4b3d4dba-5f66-49e2-f9b6-39963a3d19e4"},"source":["# train 데이터에서 26번을 삭제시킨다.\r\n","temp = []\r\n","for n in tqdm(range(train.shape[0])):\r\n","    if train['id'][n] in feature:\r\n","        temp.append(train.iloc[n])"],"execution_count":8,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7add8683f4dc47719f29c556afd5da87","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=1875000.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ln2RjNRHbWla","executionInfo":{"status":"ok","timestamp":1613305117165,"user_tz":-540,"elapsed":564343,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"bbe04256-5d81-4fca-ca6c-703f5d44ce32"},"source":["# 26번을 삭제시킨 데이터프레임\r\n","without = pd.DataFrame(data=np.array(temp), columns=train.columns)\r\n","without = without.astype({'id':int, 'time':int})\r\n","without = np.array(without.iloc[:,2:]).reshape(-1, 600, 7)\r\n","without_label = train_label[train_label['label'] != 26]['label']\r\n","without.shape, without_label.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1607, 600, 7), (1607,))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":67,"referenced_widgets":["f431773cfa484b019cfcb42ca3813701","8c2ae04720504ceb8a44b02b7d3d3cef","4dc2ba1599cb4705b201d4b6d6cbe937","b207064f4e62464bb65f09c277938977","dce0c63c607b40b2abca5f8e4653599e","f6eb4ced23d846b69cdbcea91fdd33de","e9eeb897565b44f3b8cd2451c57723ce","a36af8add48c49beb8a7982b3cd7ad9e"]},"id":"Unh0t6zKbWiK","executionInfo":{"status":"ok","timestamp":1613305292787,"user_tz":-540,"elapsed":1002,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"0552ae0c-0d2f-4152-a7d3-4661cadfdac4"},"source":["# 데이터 증강\r\n","def aug(data, shift):\r\n","    shift_data = np.roll(data, shift, axis=2)\r\n","    return shift_data\r\n","\r\n","shift_data = []\r\n","shift_label = []\r\n","for n in tqdm(range(20)):\r\n","    shifted = aug(without, n*30)\r\n","    shift_data.append(shifted)\r\n","    shift_label.append(without_label)\r\n","\r\n","shift_data = np.array(shift_data).reshape(-1,600,7)\r\n","shift_label = np.array(shift_label).reshape(1,-1)\r\n","shift_label = [element for array in shift_label for element in array]"],"execution_count":19,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f431773cfa484b019cfcb42ca3813701","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_wUyO0fUb0z9","executionInfo":{"status":"ok","timestamp":1613305297825,"user_tz":-540,"elapsed":639,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"a1328026-c80c-406b-f42f-f05a7359afc2"},"source":["# 원본 데이터와 증강 데이터 합치기\r\n","concat_train = np.concatenate((x, shift_data), axis=0)\r\n","concat_label = np.concatenate((y, shift_label), axis=0)\r\n","print(concat_train.shape)\r\n","print(concat_label.shape)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["(35265, 600, 7)\n","(35265,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.034259,"end_time":"2020-12-19T21:47:05.668897","exception":false,"start_time":"2020-12-19T21:47:05.634638","status":"completed"},"tags":[],"id":"ruDx0aTTwP3N"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.033446,"end_time":"2020-12-19T21:47:05.735558","exception":false,"start_time":"2020-12-19T21:47:05.702112","status":"completed"},"tags":[],"id":"jJCWdNkWwP3N"},"source":["Base Transformer structure from https://www.tensorflow.org/tutorials/text/transformer, modified with Swish activation function."]},{"cell_type":"code","metadata":{"_kg_hide-input":false,"execution":{"iopub.execute_input":"2020-12-19T21:47:05.822914Z","iopub.status.busy":"2020-12-19T21:47:05.821803Z","iopub.status.idle":"2020-12-19T21:47:05.866016Z","shell.execute_reply":"2020-12-19T21:47:05.865400Z"},"papermill":{"duration":0.09191,"end_time":"2020-12-19T21:47:05.866129","exception":false,"start_time":"2020-12-19T21:47:05.774219","status":"completed"},"tags":[],"id":"b7k4RMZhwP3N","executionInfo":{"status":"ok","timestamp":1613305320739,"user_tz":-540,"elapsed":646,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","    \"\"\"Calculate the attention weights.\n","    q, k, v must have matching leading dimensions.\n","    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","    The mask has different shapes depending on its type(padding or look ahead) \n","    but it must be broadcastable for addition.\n","\n","    Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","\n","    Returns:\n","    output, attention_weights\n","    \"\"\"\n","\n","    matmul_qk = tf.matmul(q, k, transpose_b = True)  # (..., seq_len_q, seq_len_k)\n","\n","    # scale matmul_qk\n","    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","    # add the mask to the scaled tensor.\n","    if mask is not None:\n","        \n","        scaled_attention_logits += (mask * -1e9)  \n","\n","    # softmax is normalized on the last axis (seq_len_k) so that the scores\n","    # add up to 1.\n","    attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)  # (..., seq_len_q, seq_len_k)\n","\n","    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","    return output, attention_weights\n","\n","class MultiHeadAttention(tf.keras.layers.Layer):\n","    \n","    def __init__(self, d_model, num_heads):\n","        \n","        super(MultiHeadAttention, self).__init__()\n","        self.num_heads = num_heads\n","        self.d_model = d_model\n","\n","        assert d_model % self.num_heads == 0\n","\n","        self.depth = d_model // self.num_heads\n","\n","        self.wq = tf.keras.layers.Dense(d_model)\n","        self.wk = tf.keras.layers.Dense(d_model)\n","        self.wv = tf.keras.layers.Dense(d_model)\n","\n","        self.dense = tf.keras.layers.Dense(d_model)\n","        \n","    def split_heads(self, x, batch_size):\n","        \"\"\"Split the last dimension into (num_heads, depth).\n","        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","        \"\"\"\n","        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","        return tf.transpose(x, perm = [0, 2, 1, 3])\n","    \n","    def call(self, v, k, q, mask):\n","        \n","        batch_size = tf.shape(q)[0]\n","\n","        q = self.wq(q)  # (batch_size, seq_len, d_model)\n","        k = self.wk(k)  # (batch_size, seq_len, d_model)\n","        v = self.wv(v)  # (batch_size, seq_len, d_model)\n","\n","        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","\n","        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","        scaled_attention, attention_weights = scaled_dot_product_attention(\n","            q, k, v, mask)\n","\n","        scaled_attention = tf.transpose(scaled_attention, perm = [0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","        concat_attention = tf.reshape(scaled_attention, \n","                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","        return output, attention_weights\n","\n","def point_wise_feed_forward_network(d_model, dff):\n","    \n","    return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation = 'relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","    ])\n","\n","class EncoderLayer(tf.keras.layers.Layer):\n","    \n","    def __init__(self, d_model, num_heads, dff, rate = 0.1):\n","        \n","        super(EncoderLayer, self).__init__()\n","\n","        self.mha = MultiHeadAttention(d_model, num_heads)\n","        self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n","        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon = 1e-6)\n","\n","        self.dropout1 = tf.keras.layers.Dropout(rate)\n","        self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","    def call(self, x, training, mask):\n","\n","        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","        attn_output = self.dropout1(attn_output, training = training)\n","        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","        ffn_output = self.dropout2(ffn_output, training = training)\n","        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","\n","        return out2\n","\n","class TransformerEncoder(tf.keras.layers.Layer):\n","    \n","    def __init__(self, num_layers, d_model, num_heads, dff, \n","                 maximum_position_encoding, rate = 0.1):\n","        \n","        super(TransformerEncoder, self).__init__()\n","\n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.num_heads = num_heads\n","        self.dff = dff\n","        self.maximum_position_encoding = maximum_position_encoding\n","        self.rate = rate\n","\n","#         self.pos_encoding = positional_encoding(self.maximum_position_encoding, \n","#                                                 self.d_model)\n","#         self.embedding = tf.keras.layers.Dense(self.d_model)\n","        self.pos_emb = tf.keras.layers.Embedding(input_dim = self.maximum_position_encoding, \n","                                                 output_dim = self.d_model)\n","\n","        self.enc_layers = [EncoderLayer(self.d_model, self.num_heads, self.dff, self.rate) \n","                           for _ in range(self.num_layers)]\n","\n","        self.dropout = tf.keras.layers.Dropout(self.rate)\n","        \n","    def get_config(self):\n","\n","        config = super().get_config().copy()\n","        config.update({\n","            'num_layers': self.num_layers,\n","            'd_model': self.d_model,\n","            'num_heads': self.num_heads,\n","            'dff': self.dff,\n","            'maximum_position_encoding': self.maximum_position_encoding,\n","            'dropout': self.dropout,\n","        })\n","        return config\n","\n","    def call(self, x, training, mask = None):\n","\n","        seq_len = tf.shape(x)[1]\n","\n","        # adding embedding and position encoding.\n","#         x += self.pos_encoding[:, :seq_len, :]\n","#         x = self.embedding(x)\n","        positions = tf.range(start = 0, limit = seq_len, delta = 1)\n","        x += self.pos_emb(positions)\n","\n","        x = self.dropout(x, training = training)\n","\n","        for i in range(self.num_layers):\n","\n","            x = self.enc_layers[i](x, training, mask)\n","\n","        return x  # (batch_size, input_seq_len, d_model)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:47:05.924111Z","iopub.status.busy":"2020-12-19T21:47:05.923351Z","iopub.status.idle":"2020-12-19T21:47:05.927506Z","shell.execute_reply":"2020-12-19T21:47:05.927026Z"},"papermill":{"duration":0.038039,"end_time":"2020-12-19T21:47:05.927604","exception":false,"start_time":"2020-12-19T21:47:05.889565","status":"completed"},"tags":[],"id":"mOCptCFxwP3P","executionInfo":{"status":"ok","timestamp":1613305322100,"user_tz":-540,"elapsed":618,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["def create_transformer_model(num_columns, num_labels, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate):\n","    \n","    inp = tf.keras.layers.Input(shape = (window_size, num_columns))\n","    x = tf.keras.layers.BatchNormalization()(inp)\n","    x = tf.keras.layers.Dense(d_model)(x)\n","    x = tf.keras.layers.BatchNormalization()(x)\n","    x = tf.keras.layers.Activation('relu')(x)\n","    x = tf.keras.layers.SpatialDropout1D(dropout_rate)(x)\n","    x = TransformerEncoder(num_layers, d_model, num_heads, dff, window_size, dropout_rate)(x)\n","    out = tf.keras.layers.Dense(num_labels, activation = 'softmax')(x[:, -1, :])\n","    \n","    model = tf.keras.models.Model(inputs = inp, outputs = out)\n","    model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['AUC'])\n","    \n","    return model"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:47:06.026885Z","iopub.status.busy":"2020-12-19T21:47:06.025060Z","iopub.status.idle":"2020-12-19T21:47:06.027550Z","shell.execute_reply":"2020-12-19T21:47:06.028046Z"},"papermill":{"duration":0.031196,"end_time":"2020-12-19T21:47:06.028159","exception":false,"start_time":"2020-12-19T21:47:05.996963","status":"completed"},"tags":[],"id":"M8HPyemgwP3R","executionInfo":{"status":"ok","timestamp":1613305323586,"user_tz":-540,"elapsed":641,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["batch_size = 64\n","num_layers = 1\n","d_model = 256\n","num_heads = 1\n","dff = 2048\n","window_size = 600\n","dropout_rate = 0.15\n","weight_decay = 0\n","label_smoothing = 1e-2\n","learning_rate = 1e-3\n","verbose = 1"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.023272,"end_time":"2020-12-19T21:47:09.916436","exception":false,"start_time":"2020-12-19T21:47:09.893164","status":"completed"},"tags":[],"id":"CPgqEvivwP3R"},"source":["# Train-Test-Split Training\n","\n","Split the train set into three folds, i.e., training-1, training-2 and validation sets. First, train the more on training-1 set and validate it on the validation set. Then use the training-2 set to find the best number of finetuning epochs. Finally, finetune on both training-2 and validation sets and submit."]},{"cell_type":"code","metadata":{"id":"Y9UiZHC-wP3R","executionInfo":{"status":"ok","timestamp":1613305326375,"user_tz":-540,"elapsed":711,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val = train_test_split(concat_train, concat_label, test_size=0.2, random_state=42)\n","y_train = tf.keras.utils.to_categorical(y_train)\n","y_val = tf.keras.utils.to_categorical(y_val)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:47:12.495194Z","iopub.status.busy":"2020-12-19T21:47:12.494436Z","iopub.status.idle":"2020-12-19T21:48:42.358496Z","shell.execute_reply":"2020-12-19T21:48:42.357889Z"},"papermill":{"duration":89.899584,"end_time":"2020-12-19T21:48:42.358608","exception":false,"start_time":"2020-12-19T21:47:12.459024","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"q5wj-CmawP3R","executionInfo":{"status":"ok","timestamp":1613308934975,"user_tz":-540,"elapsed":3607789,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"f8f8239c-ba9a-415a-c1ff-e4219f732056"},"source":["start_time_fold = time()\n","\n","ckp_path = 'JSTransformer.hdf5'\n","model = create_transformer_model(x.shape[2], 61, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\n","model.summary()\n","\n","rlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 3, verbose = verbose, min_delta = 1e-4, mode = 'min')\n","ckp = ModelCheckpoint(ckp_path, monitor = 'val_loss', verbose = 0, save_best_only = True, save_weights_only = True, mode = 'min')\n","es = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 5, mode = 'min', baseline = None, restore_best_weights = True, verbose = 0)\n","\n","history = model.fit(X_train, y_train,\n","                    validation_data = (X_val, y_val),\n","                    batch_size = batch_size,\n","                    epochs = 1000,\n","                    callbacks = [rlr, ckp, es],\n","                    verbose = verbose)\n","\n","hist = pd.DataFrame(history.history)\n","print(f'[{str(datetime.timedelta(seconds = time() - start_time_fold))[0:7]}] ROC loss:\\t', hist['val_loss'].min())\n","\n","del model\n","rubbish = gc.collect()"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 600, 7)]          0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 600, 7)            28        \n","_________________________________________________________________\n","dense (Dense)                (None, 600, 256)          2048      \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 600, 256)          1024      \n","_________________________________________________________________\n","activation (Activation)      (None, 600, 256)          0         \n","_________________________________________________________________\n","spatial_dropout1d (SpatialDr (None, 600, 256)          0         \n","_________________________________________________________________\n","transformer_encoder (Transfo (None, 600, 256)          1468672   \n","_________________________________________________________________\n","tf.__operators__.getitem (Sl (None, 256)               0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 61)                15677     \n","=================================================================\n","Total params: 1,487,449\n","Trainable params: 1,486,923\n","Non-trainable params: 526\n","_________________________________________________________________\n","Epoch 1/1000\n","441/441 [==============================] - 70s 151ms/step - loss: 3.6098 - auc: 0.7566 - val_loss: 2.9050 - val_auc: 0.8811\n","Epoch 2/1000\n","441/441 [==============================] - 69s 157ms/step - loss: 3.0302 - auc: 0.8649 - val_loss: 2.7079 - val_auc: 0.9025\n","Epoch 3/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 2.7318 - auc: 0.8995 - val_loss: 2.2540 - val_auc: 0.9413\n","Epoch 4/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 2.4970 - auc: 0.9203 - val_loss: 2.0391 - val_auc: 0.9539\n","Epoch 5/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 2.2960 - auc: 0.9362 - val_loss: 1.8827 - val_auc: 0.9623\n","Epoch 6/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 2.1114 - auc: 0.9480 - val_loss: 1.7660 - val_auc: 0.9659\n","Epoch 7/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 1.9877 - auc: 0.9542 - val_loss: 1.6855 - val_auc: 0.9679\n","Epoch 8/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 1.7994 - auc: 0.9636 - val_loss: 1.2878 - val_auc: 0.9844\n","Epoch 9/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 1.5772 - auc: 0.9717 - val_loss: 1.2357 - val_auc: 0.9816\n","Epoch 10/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 1.4239 - auc: 0.9768 - val_loss: 0.8945 - val_auc: 0.9921\n","Epoch 11/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 1.2121 - auc: 0.9827 - val_loss: 0.7429 - val_auc: 0.9935\n","Epoch 12/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 1.0207 - auc: 0.9869 - val_loss: 0.5984 - val_auc: 0.9955\n","Epoch 13/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.8708 - auc: 0.9900 - val_loss: 0.4256 - val_auc: 0.9983\n","Epoch 14/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.7365 - auc: 0.9916 - val_loss: 0.3438 - val_auc: 0.9988\n","Epoch 15/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.7028 - auc: 0.9914 - val_loss: 0.3749 - val_auc: 0.9965\n","Epoch 16/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.6047 - auc: 0.9932 - val_loss: 0.2572 - val_auc: 0.9980\n","Epoch 17/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 0.4742 - auc: 0.9948 - val_loss: 0.2131 - val_auc: 0.9995\n","Epoch 18/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.4049 - auc: 0.9964 - val_loss: 0.2054 - val_auc: 0.9985\n","Epoch 19/1000\n","441/441 [==============================] - 71s 160ms/step - loss: 0.4028 - auc: 0.9960 - val_loss: 0.1626 - val_auc: 0.9990\n","Epoch 20/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.3324 - auc: 0.9967 - val_loss: 0.1852 - val_auc: 0.9986\n","Epoch 21/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.3155 - auc: 0.9969 - val_loss: 0.1660 - val_auc: 0.9989\n","Epoch 22/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.2930 - auc: 0.9973 - val_loss: 0.1683 - val_auc: 0.9991\n","\n","Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n","Epoch 23/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.2062 - auc: 0.9983 - val_loss: 0.0746 - val_auc: 0.9995\n","Epoch 24/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1486 - auc: 0.9990 - val_loss: 0.0651 - val_auc: 0.9998\n","Epoch 25/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1276 - auc: 0.9994 - val_loss: 0.0606 - val_auc: 0.9998\n","Epoch 26/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1189 - auc: 0.9994 - val_loss: 0.0603 - val_auc: 0.9997\n","Epoch 27/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1169 - auc: 0.9993 - val_loss: 0.0546 - val_auc: 0.9997\n","Epoch 28/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1223 - auc: 0.9991 - val_loss: 0.0799 - val_auc: 0.9994\n","Epoch 29/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1111 - auc: 0.9995 - val_loss: 0.0544 - val_auc: 0.9998\n","Epoch 30/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1059 - auc: 0.9994 - val_loss: 0.0581 - val_auc: 0.9998\n","Epoch 31/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.1030 - auc: 0.9995 - val_loss: 0.0483 - val_auc: 0.9998\n","Epoch 32/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0971 - auc: 0.9995 - val_loss: 0.0472 - val_auc: 0.9998\n","Epoch 33/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0921 - auc: 0.9996 - val_loss: 0.0506 - val_auc: 0.9998\n","Epoch 34/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0911 - auc: 0.9995 - val_loss: 0.0406 - val_auc: 0.9998\n","Epoch 35/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0880 - auc: 0.9996 - val_loss: 0.0465 - val_auc: 0.9998\n","Epoch 36/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0905 - auc: 0.9996 - val_loss: 0.0435 - val_auc: 0.9998\n","Epoch 37/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0887 - auc: 0.9995 - val_loss: 0.0432 - val_auc: 0.9998\n","\n","Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n","Epoch 38/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0869 - auc: 0.9993 - val_loss: 0.0380 - val_auc: 0.9998\n","Epoch 39/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0755 - auc: 0.9998 - val_loss: 0.0371 - val_auc: 0.9998\n","Epoch 40/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0767 - auc: 0.9996 - val_loss: 0.0371 - val_auc: 0.9998\n","Epoch 41/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0799 - auc: 0.9995 - val_loss: 0.0364 - val_auc: 0.9998\n","Epoch 42/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0873 - auc: 0.9995 - val_loss: 0.0371 - val_auc: 0.9998\n","Epoch 43/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0742 - auc: 0.9996 - val_loss: 0.0375 - val_auc: 0.9998\n","Epoch 44/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0765 - auc: 0.9996 - val_loss: 0.0359 - val_auc: 0.9998\n","Epoch 45/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0789 - auc: 0.9997 - val_loss: 0.0365 - val_auc: 0.9998\n","Epoch 46/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0698 - auc: 0.9997 - val_loss: 0.0362 - val_auc: 0.9998\n","Epoch 47/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0759 - auc: 0.9998 - val_loss: 0.0372 - val_auc: 0.9998\n","\n","Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n","Epoch 48/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0725 - auc: 0.9997 - val_loss: 0.0366 - val_auc: 0.9998\n","Epoch 49/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0768 - auc: 0.9996 - val_loss: 0.0360 - val_auc: 0.9998\n","Epoch 50/1000\n","441/441 [==============================] - 70s 159ms/step - loss: 0.0710 - auc: 0.9997 - val_loss: 0.0360 - val_auc: 0.9998\n","\n","Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n","Epoch 51/1000\n","441/441 [==============================] - 70s 160ms/step - loss: 0.0724 - auc: 0.9997 - val_loss: 0.0364 - val_auc: 0.9998\n","[1:00:06] ROC loss:\t 0.03588555380702019\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.764766,"end_time":"2020-12-19T21:50:33.601357","exception":false,"start_time":"2020-12-19T21:50:32.836591","status":"completed"},"tags":[],"id":"VdNMUBSuwP3S"},"source":["# Load Model"]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2020-12-19T21:50:35.393691Z","iopub.status.busy":"2020-12-19T21:50:35.392744Z","iopub.status.idle":"2020-12-19T21:50:35.745739Z","shell.execute_reply":"2020-12-19T21:50:35.744628Z"},"papermill":{"duration":1.143613,"end_time":"2020-12-19T21:50:35.745866","exception":false,"start_time":"2020-12-19T21:50:34.602253","status":"completed"},"tags":[],"id":"gf5Or56xwP3S","executionInfo":{"status":"ok","timestamp":1613308937677,"user_tz":-540,"elapsed":1568,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["model = create_transformer_model(x.shape[2], 61, num_layers, d_model, num_heads, dff, window_size, dropout_rate, weight_decay, label_smoothing, learning_rate)\n","model.load_weights(ckp_path)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.773427,"end_time":"2020-12-19T21:50:37.314953","exception":false,"start_time":"2020-12-19T21:50:36.541526","status":"completed"},"tags":[],"id":"DmRggyuUwP3S"},"source":["# Submitting"]},{"cell_type":"code","metadata":{"id":"akq9ctMxwP3S","executionInfo":{"status":"ok","timestamp":1613308938691,"user_tz":-540,"elapsed":2575,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["test_pred = model.predict(test)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xxRPhDGwP3S","colab":{"base_uri":"https://localhost:8080/","height":609},"executionInfo":{"status":"ok","timestamp":1613308939467,"user_tz":-540,"elapsed":3348,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}},"outputId":"6d45f5ae-bbf9-4c4a-aca5-1f887f9effec"},"source":["sample_submssion = pd.read_csv(path + 'sample_submission.csv')\n","sample_submssion.iloc[:,1:] = test_pred\n","sample_submssion.to_csv(\"transformer.csv\", index = False)\n","sample_submssion"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3125</td>\n","      <td>2.126891e-04</td>\n","      <td>1.065281e-06</td>\n","      <td>1.000387e-05</td>\n","      <td>2.395879e-09</td>\n","      <td>6.525254e-06</td>\n","      <td>1.018646e-09</td>\n","      <td>7.055928e-02</td>\n","      <td>1.363940e-04</td>\n","      <td>3.264240e-08</td>\n","      <td>1.843692e-07</td>\n","      <td>8.089150e-06</td>\n","      <td>4.310654e-05</td>\n","      <td>8.022334e-06</td>\n","      <td>7.745991e-04</td>\n","      <td>8.911251e-01</td>\n","      <td>1.159552e-06</td>\n","      <td>2.438852e-07</td>\n","      <td>8.252284e-09</td>\n","      <td>9.231488e-11</td>\n","      <td>3.380202e-08</td>\n","      <td>6.840420e-07</td>\n","      <td>1.133943e-07</td>\n","      <td>2.105044e-09</td>\n","      <td>4.710884e-07</td>\n","      <td>1.641274e-02</td>\n","      <td>2.477653e-07</td>\n","      <td>0.010784</td>\n","      <td>9.029970e-06</td>\n","      <td>6.166005e-07</td>\n","      <td>2.936537e-07</td>\n","      <td>6.259574e-06</td>\n","      <td>2.328473e-08</td>\n","      <td>2.851499e-10</td>\n","      <td>1.991623e-11</td>\n","      <td>3.856387e-09</td>\n","      <td>2.351703e-06</td>\n","      <td>1.543913e-08</td>\n","      <td>8.042113e-06</td>\n","      <td>1.321023e-06</td>\n","      <td>2.678637e-07</td>\n","      <td>1.905799e-08</td>\n","      <td>1.574370e-07</td>\n","      <td>1.413292e-05</td>\n","      <td>2.150609e-08</td>\n","      <td>2.838985e-04</td>\n","      <td>3.034849e-06</td>\n","      <td>5.654870e-08</td>\n","      <td>1.426984e-09</td>\n","      <td>8.285498e-06</td>\n","      <td>5.321518e-08</td>\n","      <td>8.868165e-03</td>\n","      <td>5.980089e-04</td>\n","      <td>3.581215e-05</td>\n","      <td>8.205200e-09</td>\n","      <td>5.585874e-07</td>\n","      <td>1.167901e-07</td>\n","      <td>5.905737e-05</td>\n","      <td>3.947421e-11</td>\n","      <td>8.094799e-06</td>\n","      <td>1.944105e-06</td>\n","      <td>5.213298e-06</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>3126</td>\n","      <td>7.885522e-10</td>\n","      <td>1.055551e-06</td>\n","      <td>8.721771e-06</td>\n","      <td>2.028633e-07</td>\n","      <td>4.641043e-08</td>\n","      <td>3.149164e-02</td>\n","      <td>2.521717e-07</td>\n","      <td>9.036010e-01</td>\n","      <td>8.885154e-06</td>\n","      <td>2.172957e-05</td>\n","      <td>1.391279e-08</td>\n","      <td>2.447960e-07</td>\n","      <td>8.706434e-09</td>\n","      <td>4.744099e-04</td>\n","      <td>1.658126e-08</td>\n","      <td>5.000627e-06</td>\n","      <td>2.783955e-08</td>\n","      <td>2.782661e-07</td>\n","      <td>3.833583e-08</td>\n","      <td>1.575984e-08</td>\n","      <td>8.344176e-05</td>\n","      <td>1.230902e-08</td>\n","      <td>1.879319e-04</td>\n","      <td>1.152085e-04</td>\n","      <td>1.517773e-06</td>\n","      <td>2.719485e-07</td>\n","      <td>0.059510</td>\n","      <td>5.134079e-04</td>\n","      <td>1.068652e-09</td>\n","      <td>3.832934e-11</td>\n","      <td>1.575524e-07</td>\n","      <td>1.463507e-07</td>\n","      <td>8.900003e-10</td>\n","      <td>3.692954e-07</td>\n","      <td>3.407800e-06</td>\n","      <td>1.810020e-09</td>\n","      <td>7.665891e-06</td>\n","      <td>1.601894e-07</td>\n","      <td>5.802873e-07</td>\n","      <td>8.461827e-08</td>\n","      <td>2.720129e-04</td>\n","      <td>5.039849e-07</td>\n","      <td>1.471541e-07</td>\n","      <td>8.273526e-07</td>\n","      <td>3.596776e-03</td>\n","      <td>5.990785e-08</td>\n","      <td>1.998441e-07</td>\n","      <td>2.489497e-09</td>\n","      <td>1.374358e-06</td>\n","      <td>3.841076e-09</td>\n","      <td>9.763748e-06</td>\n","      <td>2.602903e-08</td>\n","      <td>8.476406e-10</td>\n","      <td>3.646425e-06</td>\n","      <td>5.696763e-10</td>\n","      <td>1.653541e-05</td>\n","      <td>1.710061e-06</td>\n","      <td>1.951390e-05</td>\n","      <td>1.145579e-09</td>\n","      <td>3.831389e-05</td>\n","      <td>1.567380e-07</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3127</td>\n","      <td>3.082519e-05</td>\n","      <td>6.902708e-05</td>\n","      <td>1.479054e-07</td>\n","      <td>4.582374e-07</td>\n","      <td>2.840427e-07</td>\n","      <td>5.588576e-07</td>\n","      <td>1.546478e-04</td>\n","      <td>3.937004e-01</td>\n","      <td>2.397854e-04</td>\n","      <td>6.683425e-06</td>\n","      <td>3.487549e-07</td>\n","      <td>2.012567e-02</td>\n","      <td>3.326028e-05</td>\n","      <td>2.444995e-07</td>\n","      <td>9.304542e-04</td>\n","      <td>6.264757e-06</td>\n","      <td>3.767976e-08</td>\n","      <td>6.417699e-07</td>\n","      <td>2.762596e-07</td>\n","      <td>3.221906e-08</td>\n","      <td>2.263820e-06</td>\n","      <td>8.483208e-05</td>\n","      <td>2.549096e-09</td>\n","      <td>1.031511e-03</td>\n","      <td>5.168565e-08</td>\n","      <td>1.383908e-07</td>\n","      <td>0.451802</td>\n","      <td>8.009289e-07</td>\n","      <td>1.108412e-04</td>\n","      <td>1.124595e-06</td>\n","      <td>8.920994e-04</td>\n","      <td>1.460240e-05</td>\n","      <td>9.930548e-05</td>\n","      <td>2.391724e-09</td>\n","      <td>6.792374e-05</td>\n","      <td>2.131650e-05</td>\n","      <td>3.076509e-06</td>\n","      <td>2.755387e-02</td>\n","      <td>4.037925e-03</td>\n","      <td>1.403809e-05</td>\n","      <td>5.882907e-11</td>\n","      <td>2.527814e-08</td>\n","      <td>7.101571e-07</td>\n","      <td>2.941807e-06</td>\n","      <td>7.885009e-06</td>\n","      <td>4.370196e-03</td>\n","      <td>1.627367e-08</td>\n","      <td>4.349165e-12</td>\n","      <td>5.932013e-05</td>\n","      <td>1.208317e-07</td>\n","      <td>8.479905e-02</td>\n","      <td>1.058335e-04</td>\n","      <td>1.162084e-04</td>\n","      <td>2.516901e-05</td>\n","      <td>3.392106e-11</td>\n","      <td>5.982601e-08</td>\n","      <td>4.948650e-03</td>\n","      <td>5.000131e-09</td>\n","      <td>2.969836e-08</td>\n","      <td>1.409325e-06</td>\n","      <td>4.524831e-03</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3128</td>\n","      <td>7.997050e-04</td>\n","      <td>1.133802e-07</td>\n","      <td>1.253678e-09</td>\n","      <td>5.898038e-09</td>\n","      <td>2.131822e-08</td>\n","      <td>2.575670e-05</td>\n","      <td>1.678392e-04</td>\n","      <td>7.351980e-06</td>\n","      <td>7.777659e-06</td>\n","      <td>5.361857e-07</td>\n","      <td>1.259219e-05</td>\n","      <td>1.061837e-07</td>\n","      <td>1.155872e-10</td>\n","      <td>3.860556e-08</td>\n","      <td>9.083570e-07</td>\n","      <td>1.905802e-09</td>\n","      <td>1.583825e-08</td>\n","      <td>2.422280e-10</td>\n","      <td>1.735683e-07</td>\n","      <td>2.309622e-08</td>\n","      <td>1.301249e-12</td>\n","      <td>1.152238e-08</td>\n","      <td>2.486306e-06</td>\n","      <td>3.734811e-07</td>\n","      <td>6.113001e-03</td>\n","      <td>5.055030e-09</td>\n","      <td>0.032107</td>\n","      <td>1.455435e-05</td>\n","      <td>5.509997e-08</td>\n","      <td>5.737109e-09</td>\n","      <td>2.860268e-07</td>\n","      <td>1.330627e-07</td>\n","      <td>7.110240e-09</td>\n","      <td>8.084776e-05</td>\n","      <td>4.595432e-08</td>\n","      <td>6.047186e-08</td>\n","      <td>8.307565e-08</td>\n","      <td>1.164432e-08</td>\n","      <td>8.836111e-11</td>\n","      <td>1.295581e-08</td>\n","      <td>2.116248e-06</td>\n","      <td>1.024002e-07</td>\n","      <td>6.529297e-11</td>\n","      <td>4.179801e-08</td>\n","      <td>9.586948e-01</td>\n","      <td>5.458782e-08</td>\n","      <td>1.814689e-10</td>\n","      <td>2.350910e-06</td>\n","      <td>8.845674e-04</td>\n","      <td>8.989075e-04</td>\n","      <td>3.580881e-05</td>\n","      <td>6.892421e-07</td>\n","      <td>1.058100e-07</td>\n","      <td>7.515327e-09</td>\n","      <td>2.228728e-06</td>\n","      <td>2.152861e-09</td>\n","      <td>9.324058e-07</td>\n","      <td>1.977398e-08</td>\n","      <td>3.737633e-05</td>\n","      <td>9.742452e-05</td>\n","      <td>3.219506e-08</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>3129</td>\n","      <td>1.415563e-03</td>\n","      <td>3.333942e-09</td>\n","      <td>4.709638e-08</td>\n","      <td>6.530272e-06</td>\n","      <td>3.057835e-08</td>\n","      <td>3.686935e-07</td>\n","      <td>4.515118e-07</td>\n","      <td>1.025697e-11</td>\n","      <td>8.947000e-10</td>\n","      <td>5.657124e-10</td>\n","      <td>8.588579e-08</td>\n","      <td>2.661241e-09</td>\n","      <td>2.605229e-11</td>\n","      <td>1.153348e-07</td>\n","      <td>7.783218e-09</td>\n","      <td>5.124185e-11</td>\n","      <td>7.425396e-09</td>\n","      <td>4.167602e-07</td>\n","      <td>9.821284e-09</td>\n","      <td>1.363485e-12</td>\n","      <td>3.994247e-10</td>\n","      <td>1.442488e-09</td>\n","      <td>5.476716e-07</td>\n","      <td>3.087005e-07</td>\n","      <td>7.251893e-07</td>\n","      <td>9.286513e-10</td>\n","      <td>0.998116</td>\n","      <td>3.433246e-08</td>\n","      <td>1.493086e-08</td>\n","      <td>1.951436e-10</td>\n","      <td>6.517365e-07</td>\n","      <td>2.481416e-05</td>\n","      <td>1.189355e-07</td>\n","      <td>3.000812e-09</td>\n","      <td>3.892325e-10</td>\n","      <td>2.011029e-06</td>\n","      <td>1.101837e-04</td>\n","      <td>1.632064e-05</td>\n","      <td>6.483557e-08</td>\n","      <td>1.041651e-11</td>\n","      <td>2.040545e-09</td>\n","      <td>1.388297e-09</td>\n","      <td>2.442349e-12</td>\n","      <td>4.090329e-08</td>\n","      <td>2.971467e-10</td>\n","      <td>1.486130e-10</td>\n","      <td>2.860934e-08</td>\n","      <td>8.129356e-10</td>\n","      <td>3.129102e-08</td>\n","      <td>2.956286e-04</td>\n","      <td>3.018969e-06</td>\n","      <td>6.372161e-09</td>\n","      <td>1.334381e-09</td>\n","      <td>1.975161e-11</td>\n","      <td>1.881568e-08</td>\n","      <td>2.791591e-07</td>\n","      <td>2.316927e-07</td>\n","      <td>3.766061e-08</td>\n","      <td>3.949417e-09</td>\n","      <td>4.705754e-06</td>\n","      <td>2.390782e-09</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>777</th>\n","      <td>3902</td>\n","      <td>2.645198e-06</td>\n","      <td>1.491880e-02</td>\n","      <td>3.951535e-03</td>\n","      <td>1.144515e-08</td>\n","      <td>3.290720e-02</td>\n","      <td>7.957361e-10</td>\n","      <td>1.181593e-06</td>\n","      <td>5.971185e-06</td>\n","      <td>1.615224e-05</td>\n","      <td>1.125257e-04</td>\n","      <td>6.890712e-05</td>\n","      <td>6.266847e-03</td>\n","      <td>8.234084e-06</td>\n","      <td>2.145155e-05</td>\n","      <td>1.141516e-06</td>\n","      <td>4.154022e-06</td>\n","      <td>1.204708e-08</td>\n","      <td>2.749767e-05</td>\n","      <td>8.591838e-05</td>\n","      <td>3.955629e-07</td>\n","      <td>9.744840e-10</td>\n","      <td>2.042198e-07</td>\n","      <td>2.279693e-03</td>\n","      <td>5.228193e-01</td>\n","      <td>4.868780e-03</td>\n","      <td>1.304350e-05</td>\n","      <td>0.281311</td>\n","      <td>9.311557e-08</td>\n","      <td>2.140865e-05</td>\n","      <td>2.633618e-03</td>\n","      <td>2.112642e-06</td>\n","      <td>8.130202e-08</td>\n","      <td>1.393083e-09</td>\n","      <td>9.658860e-08</td>\n","      <td>9.678048e-08</td>\n","      <td>4.708879e-08</td>\n","      <td>3.840161e-08</td>\n","      <td>2.316173e-02</td>\n","      <td>7.232383e-05</td>\n","      <td>9.995877e-06</td>\n","      <td>6.853754e-07</td>\n","      <td>2.301059e-04</td>\n","      <td>9.616008e-05</td>\n","      <td>7.289503e-07</td>\n","      <td>1.205236e-04</td>\n","      <td>2.158560e-05</td>\n","      <td>3.278505e-03</td>\n","      <td>6.725710e-07</td>\n","      <td>4.883858e-06</td>\n","      <td>2.147391e-08</td>\n","      <td>1.303154e-05</td>\n","      <td>6.256751e-06</td>\n","      <td>2.971182e-04</td>\n","      <td>2.168681e-06</td>\n","      <td>5.241154e-08</td>\n","      <td>1.670553e-05</td>\n","      <td>1.695011e-07</td>\n","      <td>3.329912e-02</td>\n","      <td>3.157727e-02</td>\n","      <td>2.651319e-02</td>\n","      <td>8.926505e-03</td>\n","    </tr>\n","    <tr>\n","      <th>778</th>\n","      <td>3903</td>\n","      <td>3.211467e-04</td>\n","      <td>1.631296e-01</td>\n","      <td>2.790250e-07</td>\n","      <td>2.169322e-07</td>\n","      <td>1.033806e-04</td>\n","      <td>2.174156e-07</td>\n","      <td>8.423044e-09</td>\n","      <td>9.776104e-07</td>\n","      <td>3.108332e-05</td>\n","      <td>9.453482e-09</td>\n","      <td>6.256610e-09</td>\n","      <td>4.924730e-05</td>\n","      <td>7.343302e-10</td>\n","      <td>3.309971e-08</td>\n","      <td>2.149082e-09</td>\n","      <td>1.927298e-07</td>\n","      <td>4.099048e-05</td>\n","      <td>5.048131e-07</td>\n","      <td>9.287733e-05</td>\n","      <td>7.581265e-09</td>\n","      <td>4.076622e-10</td>\n","      <td>5.721755e-07</td>\n","      <td>6.365858e-10</td>\n","      <td>4.936562e-06</td>\n","      <td>5.826053e-04</td>\n","      <td>1.297076e-07</td>\n","      <td>0.626907</td>\n","      <td>2.491700e-07</td>\n","      <td>4.629826e-06</td>\n","      <td>3.001813e-08</td>\n","      <td>1.075062e-04</td>\n","      <td>4.386474e-07</td>\n","      <td>1.042043e-08</td>\n","      <td>9.154542e-07</td>\n","      <td>3.558041e-09</td>\n","      <td>1.978439e-08</td>\n","      <td>4.055906e-06</td>\n","      <td>1.418075e-04</td>\n","      <td>9.786910e-08</td>\n","      <td>1.009546e-08</td>\n","      <td>4.782006e-10</td>\n","      <td>1.714200e-11</td>\n","      <td>6.831797e-10</td>\n","      <td>2.573849e-07</td>\n","      <td>6.775953e-07</td>\n","      <td>1.028707e-09</td>\n","      <td>6.877654e-07</td>\n","      <td>5.078310e-07</td>\n","      <td>4.647050e-07</td>\n","      <td>3.333837e-09</td>\n","      <td>5.041821e-06</td>\n","      <td>1.238298e-06</td>\n","      <td>1.052725e-07</td>\n","      <td>1.585246e-07</td>\n","      <td>6.641799e-12</td>\n","      <td>8.608171e-08</td>\n","      <td>4.691814e-06</td>\n","      <td>2.347545e-05</td>\n","      <td>9.609820e-07</td>\n","      <td>2.084343e-01</td>\n","      <td>1.363378e-06</td>\n","    </tr>\n","    <tr>\n","      <th>779</th>\n","      <td>3904</td>\n","      <td>4.779145e-07</td>\n","      <td>1.249035e-08</td>\n","      <td>2.293448e-09</td>\n","      <td>2.388420e-08</td>\n","      <td>3.772523e-10</td>\n","      <td>6.402012e-09</td>\n","      <td>6.295384e-08</td>\n","      <td>4.299762e-06</td>\n","      <td>5.643910e-08</td>\n","      <td>3.135410e-10</td>\n","      <td>1.921254e-05</td>\n","      <td>1.477010e-08</td>\n","      <td>1.213234e-09</td>\n","      <td>1.734427e-09</td>\n","      <td>1.024439e-08</td>\n","      <td>2.455216e-10</td>\n","      <td>4.383014e-09</td>\n","      <td>9.717986e-07</td>\n","      <td>4.937782e-11</td>\n","      <td>4.411008e-09</td>\n","      <td>7.168077e-10</td>\n","      <td>3.312780e-08</td>\n","      <td>1.765967e-08</td>\n","      <td>6.676192e-08</td>\n","      <td>1.433280e-09</td>\n","      <td>2.108323e-08</td>\n","      <td>0.998875</td>\n","      <td>2.163433e-05</td>\n","      <td>5.207016e-08</td>\n","      <td>3.093100e-10</td>\n","      <td>3.977840e-08</td>\n","      <td>1.617360e-10</td>\n","      <td>7.979766e-09</td>\n","      <td>4.043355e-09</td>\n","      <td>1.245414e-09</td>\n","      <td>6.020039e-07</td>\n","      <td>7.036133e-05</td>\n","      <td>1.227192e-06</td>\n","      <td>4.174959e-04</td>\n","      <td>7.325600e-11</td>\n","      <td>1.119124e-08</td>\n","      <td>5.801034e-10</td>\n","      <td>5.424137e-11</td>\n","      <td>1.452691e-07</td>\n","      <td>2.339909e-09</td>\n","      <td>4.007452e-10</td>\n","      <td>6.646391e-10</td>\n","      <td>4.314648e-10</td>\n","      <td>6.927020e-09</td>\n","      <td>2.971474e-05</td>\n","      <td>2.905728e-07</td>\n","      <td>5.212570e-08</td>\n","      <td>1.439179e-08</td>\n","      <td>8.066630e-12</td>\n","      <td>1.948675e-05</td>\n","      <td>1.106705e-08</td>\n","      <td>1.323282e-08</td>\n","      <td>5.159681e-04</td>\n","      <td>4.786531e-10</td>\n","      <td>1.367019e-05</td>\n","      <td>8.699005e-06</td>\n","    </tr>\n","    <tr>\n","      <th>780</th>\n","      <td>3905</td>\n","      <td>7.280117e-05</td>\n","      <td>5.802110e-06</td>\n","      <td>7.991491e-05</td>\n","      <td>1.132583e-06</td>\n","      <td>6.107889e-05</td>\n","      <td>1.032036e-06</td>\n","      <td>6.044957e-07</td>\n","      <td>1.532528e-07</td>\n","      <td>8.008286e-08</td>\n","      <td>7.247918e-07</td>\n","      <td>6.334944e-04</td>\n","      <td>2.729880e-05</td>\n","      <td>2.209158e-07</td>\n","      <td>2.702101e-07</td>\n","      <td>3.703205e-07</td>\n","      <td>4.180835e-07</td>\n","      <td>4.063803e-04</td>\n","      <td>2.860059e-06</td>\n","      <td>2.037325e-07</td>\n","      <td>1.378971e-10</td>\n","      <td>8.366890e-09</td>\n","      <td>2.202371e-06</td>\n","      <td>2.535994e-08</td>\n","      <td>9.839263e-08</td>\n","      <td>1.501304e-02</td>\n","      <td>1.465687e-05</td>\n","      <td>0.644974</td>\n","      <td>1.256819e-08</td>\n","      <td>2.230774e-07</td>\n","      <td>1.079156e-05</td>\n","      <td>9.325752e-02</td>\n","      <td>3.783287e-02</td>\n","      <td>4.961118e-08</td>\n","      <td>1.783219e-01</td>\n","      <td>2.496603e-05</td>\n","      <td>1.905306e-06</td>\n","      <td>9.020455e-07</td>\n","      <td>8.226671e-05</td>\n","      <td>1.267798e-08</td>\n","      <td>6.242498e-05</td>\n","      <td>5.095342e-07</td>\n","      <td>4.388401e-06</td>\n","      <td>1.812730e-03</td>\n","      <td>2.138370e-05</td>\n","      <td>2.472414e-06</td>\n","      <td>5.096350e-06</td>\n","      <td>2.451563e-02</td>\n","      <td>2.713196e-03</td>\n","      <td>5.806927e-09</td>\n","      <td>2.537102e-08</td>\n","      <td>2.891091e-08</td>\n","      <td>9.262920e-07</td>\n","      <td>1.699291e-07</td>\n","      <td>2.885146e-09</td>\n","      <td>9.737131e-06</td>\n","      <td>7.492901e-08</td>\n","      <td>4.285176e-07</td>\n","      <td>6.211857e-08</td>\n","      <td>1.783122e-05</td>\n","      <td>3.449905e-07</td>\n","      <td>4.465059e-08</td>\n","    </tr>\n","    <tr>\n","      <th>781</th>\n","      <td>3906</td>\n","      <td>1.454541e-05</td>\n","      <td>2.464521e-06</td>\n","      <td>3.848173e-09</td>\n","      <td>4.637130e-08</td>\n","      <td>1.375921e-10</td>\n","      <td>1.977710e-05</td>\n","      <td>2.986191e-06</td>\n","      <td>4.832549e-05</td>\n","      <td>2.831185e-04</td>\n","      <td>1.451537e-05</td>\n","      <td>3.873453e-07</td>\n","      <td>2.980494e-05</td>\n","      <td>4.682554e-07</td>\n","      <td>7.832728e-07</td>\n","      <td>1.457043e-07</td>\n","      <td>7.129178e-10</td>\n","      <td>8.607410e-09</td>\n","      <td>1.171726e-06</td>\n","      <td>2.836611e-03</td>\n","      <td>1.387590e-04</td>\n","      <td>1.227752e-06</td>\n","      <td>4.146987e-08</td>\n","      <td>3.243392e-03</td>\n","      <td>9.127594e-07</td>\n","      <td>9.216614e-07</td>\n","      <td>1.191323e-05</td>\n","      <td>0.953550</td>\n","      <td>5.862368e-06</td>\n","      <td>1.250432e-03</td>\n","      <td>7.539347e-08</td>\n","      <td>1.818986e-06</td>\n","      <td>1.679120e-07</td>\n","      <td>8.488718e-09</td>\n","      <td>3.191231e-05</td>\n","      <td>2.351530e-08</td>\n","      <td>2.308622e-08</td>\n","      <td>1.098272e-03</td>\n","      <td>7.991565e-07</td>\n","      <td>5.676675e-07</td>\n","      <td>1.050369e-07</td>\n","      <td>9.836011e-05</td>\n","      <td>2.308925e-05</td>\n","      <td>7.525627e-08</td>\n","      <td>4.773405e-04</td>\n","      <td>2.658384e-06</td>\n","      <td>5.543229e-07</td>\n","      <td>2.783331e-09</td>\n","      <td>7.081868e-03</td>\n","      <td>9.793323e-10</td>\n","      <td>2.298063e-02</td>\n","      <td>5.477301e-07</td>\n","      <td>2.755124e-07</td>\n","      <td>1.465507e-07</td>\n","      <td>1.047302e-07</td>\n","      <td>2.122025e-06</td>\n","      <td>4.629133e-04</td>\n","      <td>1.723214e-03</td>\n","      <td>4.299283e-03</td>\n","      <td>4.146620e-06</td>\n","      <td>3.886976e-08</td>\n","      <td>2.503330e-04</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>782 rows × 62 columns</p>\n","</div>"],"text/plain":["       id             0             1  ...            58            59            60\n","0    3125  2.126891e-04  1.065281e-06  ...  8.094799e-06  1.944105e-06  5.213298e-06\n","1    3126  7.885522e-10  1.055551e-06  ...  1.145579e-09  3.831389e-05  1.567380e-07\n","2    3127  3.082519e-05  6.902708e-05  ...  2.969836e-08  1.409325e-06  4.524831e-03\n","3    3128  7.997050e-04  1.133802e-07  ...  3.737633e-05  9.742452e-05  3.219506e-08\n","4    3129  1.415563e-03  3.333942e-09  ...  3.949417e-09  4.705754e-06  2.390782e-09\n","..    ...           ...           ...  ...           ...           ...           ...\n","777  3902  2.645198e-06  1.491880e-02  ...  3.157727e-02  2.651319e-02  8.926505e-03\n","778  3903  3.211467e-04  1.631296e-01  ...  9.609820e-07  2.084343e-01  1.363378e-06\n","779  3904  4.779145e-07  1.249035e-08  ...  4.786531e-10  1.367019e-05  8.699005e-06\n","780  3905  7.280117e-05  5.802110e-06  ...  1.783122e-05  3.449905e-07  4.465059e-08\n","781  3906  1.454541e-05  2.464521e-06  ...  4.146620e-06  3.886976e-08  2.503330e-04\n","\n","[782 rows x 62 columns]"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"A76Wi3ZtwP3S","executionInfo":{"status":"ok","timestamp":1613308939468,"user_tz":-540,"elapsed":3346,"user":{"displayName":"기석윤","photoUrl":"","userId":"09664814525927757322"}}},"source":["# https://www.kaggle.com/gogo827jz/jane-street-ffill-transformer-baseline\r\n","# https://wikidocs.net/31379\r\n","# https://www.tensorflow.org/tutorials/text/transformer"],"execution_count":29,"outputs":[]}]}